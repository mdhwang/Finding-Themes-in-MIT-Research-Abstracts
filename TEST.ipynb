{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('EECS_Abstracts.csv','r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    lib = list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_lib = []\n",
    "for abstract in lib[0]:\n",
    "    word_tokens = tokenizer.tokenize(abstract)\n",
    "    filtered_abstract = [w for w in word_tokens if not w in stop_words]\n",
    "    stemmed = [porter.stem(word).lower() for word in filtered_abstract]\n",
    "    filtered_lib.append(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for each in filtered_lib:\n",
    "    each = set(each)\n",
    "    for word in each:\n",
    "        corpus.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nmf_helpers import build_text_vectorizer, hand_label_topics, analyze_article\n",
    "from my_nmf import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Purpose: To improve the image quality of highly accelerated multi-channel MRI data by learning a joint variational network that reconstructs multiple clinical contrasts jointly.\\n  Methods: Data from our multi-contrast acquisition was embedded into the variational network architecture where shared anatomical information is exchanged by mixing the input contrasts. Complementary k-space sampling across imaging contrasts and Bunch-Phase/Wave-Encoding were used for data acquisition to improve the reconstruction at high accelerations. At 3T, our joint variational network approach across T1w, T2w and T2-FLAIR-weighted brain scans was tested for retrospective under-sampling at R=6 (2D) and R=4x4 (3D) acceleration. Prospective acceleration was also performed for 3D data where the combined acquisition time for whole brain coverage at 1 mm isotropic resolution across three contrasts was less than three minutes.\\n  Results: Across all test datasets, our joint multi-contrast network better preserved fine anatomical details with reduced image-blurring when compared to the corresponding single-contrast reconstructions. Improvement in image quality was also obtained through complementary k-space sampling and Bunch-Phase/Wave-Encoding where the synergistic combination yielded the overall best performance as evidenced by exemplarily slices and quantitative error metrics.\\n  Conclusion: By leveraging shared anatomical structures across the jointly reconstructed scans, our joint multi-contrast approach learnt more efficient regularizers which helped to retain natural image appearance and avoid over-smoothing. When synergistically combined with advanced encoding techniques, the performance was further improved, enabling up to R=16-fold acceleration with good image quality. This should help pave the way to very rapid high-resolution brain exams.',\n",
       " 'We propose Nonlinear Dipole Inversion (NDI) for high-quality Quantitative Susceptibility Mapping (QSM) without regularization tuning, while matching the image quality of state-of-the-art reconstruction techniques. In addition to avoiding over-smoothing that these techniques often suffer from, we also obviate the need for parameter selection. NDI is flexible enough to allow for reconstruction from an arbitrary number of head orientations, and outperforms COSMOS even when using as few as 1-direction data. This is made possible by a nonlinear forward-model that uses the magnitude as an effective prior, for which we derived a simple gradient descent update rule. We synergistically combine this physics-model with a Variational Network (VN) to leverage the power of deep learning in the VaNDI algorithm. This technique adopts the simple gradient descent rule from NDI and learns the network parameters during training, hence requires no additional parameter tuning. Further, we evaluate NDI at 7T using highly accelerated Wave-CAIPI acquisitions at 0.5 mm isotropic resolution and demonstrate high-quality QSM from as few as 2-direction data.',\n",
       " 'The performance and diagnostic utility of magnetic resonance imaging (MRI) in pregnancy is fundamentally constrained by fetal motion. Motion of the fetus, which is unpredictable and rapid on the scale of conventional imaging times, limits the set of viable acquisition techniques to single-shot imaging with severe compromises in signal-to-noise ratio and diagnostic contrast, and frequently results in unacceptable image quality. Surprisingly little is known about the characteristics of fetal motion during MRI and here we propose and demonstrate methods that exploit a growing repository of MRI observations of the gravid abdomen that are acquired at low spatial resolution but relatively high temporal resolution and over long durations (10-30 minutes). We estimate fetal pose per frame in MRI volumes of the pregnant abdomen via deep learning algorithms that detect key fetal landmarks. Evaluation of the proposed method shows that our framework achieves quantitatively an average error of 4.47 mm and 96.4\\\\% accuracy (with error less than 10 mm). Fetal pose estimation in MRI time series yields novel means of quantifying fetal movements in health and disease, and enables the learning of kinematic models that may enhance prospective mitigation of fetal motion artifacts during MRI acquisition.',\n",
       " 'We present a robust method to correct for motion in volumetric in-utero MRI time series. Time-course analysis for in-utero volumetric MRI time series often suffers from substantial and unpredictable fetal motion. Registration provides voxel correspondences between images and is commonly employed for motion correction. Current registration methods often fail when aligning images that are substantially different from a template (reference image). To achieve accurate and robust alignment, we make a Markov assumption on the nature of motion and take advantage of the temporal smoothness in the image data. Forward message passing in the corresponding hidden Markov model (HMM) yields an estimation algorithm that only has to account for relatively small motion between consecutive frames. We evaluate the utility of the temporal model in the context of in-utero MRI time series alignment by examining the accuracy of propagated segmentation label maps. Our results suggest that the proposed model captures accurately the temporal dynamics of transformations in in-utero MRI time series.',\n",
       " 'We present a robust method to correct for motion and deformations for in-utero volumetric MRI time series. Spatio-temporal analysis of dynamic MRI requires robust alignment across time in the presence of substantial and unpredictable motion. We make a Markov assumption on the nature of deformations to take advantage of the temporal structure in the image data. Forward message passing in the corresponding hidden Markov model (HMM) yields an estimation algorithm that only has to account for relatively small motion between consecutive frames. We demonstrate the utility of the temporal model by showing that its use improves the accuracy of the segmentation propagation through temporal registration. Our results suggest that the proposed model captures accurately the temporal dynamics of deformations in in-utero MRI time series.',\n",
       " '  A linear inverse problem is proposed that requires the determination of multiple unknown signal vectors. Each unknown vector passes through a different system matrix and the results are added to yield a single observation vector. Given the matrices and lone observation, the objective is to find a simultaneously sparse set of unknown vectors that solves the system. We will refer to this as the multiple-system single-output (MSSO) simultaneous sparsity problem. This manuscript contrasts the MSSO problem with other simultaneous sparsity problems and conducts a thorough initial exploration of algorithms with which to solve it. Seven algorithms are formulated that approximately solve this NP-Hard problem. Three greedy techniques are developed (matching pursuit, orthogonal matching pursuit, and least squares matching pursuit) along with four methods based on a convex relaxation (iteratively reweighted least squares, two forms of iterative shrinkage, and formulation as a second-order cone program). The algorithms are evaluated across three experiments: the first and second involve sparsity profile recovery in noiseless and noisy scenarios, respectively, while the third deals with magnetic resonance imaging radio-frequency excitation pulse design.',\n",
       " 'Heart Failure is a major component of healthcare expenditure and a leading cause of mortality worldwide. Despite higher inter-rater variability, endomyocardial biopsy (EMB) is still regarded as the standard technique, used to identify the cause (e.g. ischemic or non-ischemic cardiomyopathy, coronary artery disease, myocardial infarction etc.) of unexplained heart failure. In this paper, we focus on identifying cardiomyopathy as ischemic or non-ischemic. For this, we propose and implement a new unified architecture comprising CNN (inception-V3 model) and bidirectional LSTM (BiLSTM) with self-attention mechanism to predict the ischemic or non-ischemic to classify cardiomyopathy using histopathological images. The proposed model is based on self-attention that implicitly focuses on the information outputted from the hidden layers of BiLSTM. Through our results we demonstrate that this framework carries a high learning capacity and is able to improve the classification performance.',\n",
       " 'Gliomas are the most common primary brain malignancies, with different degrees of aggressiveness, variable prognosis and various heterogeneous histologic sub-regions, i.e., peritumoral edematous/invaded tissue, necrotic core, active and non-enhancing core. This intrinsic heterogeneity is also portrayed in their radio-phenotype, as their sub-regions are depicted by varying intensity profiles disseminated across multi-parametric magnetic resonance imaging (mpMRI) scans, reflecting varying biological properties. Their heterogeneous shape, extent, and location are some of the factors that make these tumors difficult to resect, and in some cases inoperable. The amount of resected tumor is a factor also considered in longitudinal scans, when evaluating the apparent tumor for potential diagnosis of progression. Furthermore, there is mounting evidence that accurate segmentation of the various tumor sub-regions can offer the basis for quantitative image analysis towards prediction of patient overall survival. This study assesses the state-of-the-art machine learning (ML) methods used for brain tumor image analysis in mpMRI scans, during the last seven instances of the International Brain Tumor Segmentation (BraTS) challenge, i.e., 2012-2018. Specifically, we focus on i) evaluating segmentations of the various glioma sub-regions in pre-operative mpMRI scans, ii) assessing potential tumor progression by virtue of longitudinal growth of tumor sub-regions, beyond use of the RECIST/RANO criteria, and iii) predicting the overall survival from pre-operative mpMRI scans of patients that underwent gross total resection. Finally, we investigate the challenge of identifying the best ML algorithms for each of these tasks, considering that apart from being diverse on each instance of the challenge, the multi-institutional mpMRI BraTS dataset has also been a continuously evolving/growing dataset.',\n",
       " 'Theorems from Part 1 of this paper are generalized to Ïˆ-mixing sources in this paper. Application to Markoff chains and order m Markoff chains is presented. The main result is the generalization of Theorem 1 in Part 1.',\n",
       " 'In this paper, the problem of communication over an essentially unknown channel, which is known to be able to communicate a source to a destination to within a certain distortion level, is considered from a behavioral, interconnection view-point. Rates of reliable communication are derived and source-channel separation for communication with fidelity criteria is proved. The results are then generalized to the multi-user setting under certain assumptions. Other applications of this problem problem which follow from this perspective are discussed.',\n",
       " 'This is a three part paper.\\n  Optimality of source-channel separation for communication with a fidelity criterion when the channel is compound as defined by Csiszar and Korner in their book and general as defined by Verdu and Han, is proved in Part I. It is assumed that random codes are permitted. The word \"universal\" in the title of this paper refers to the fact that the channel model is compound. The proof uses a layered black-box or a layered input-output view-point. In particular, only the end-to-end description of the channel as being capable of communicating a source to within a certain distortion level is used when proving separation. This implies that the channel model does not play any role for separation to hold as long as there is a source model. Further implications of the layered black-box view-point are discussed.\\n  Optimality of source-medium separation for multi-user communication with fidelity criteria over a general, compound medium in the unicast setting is proved in Part II, thus generalizing Part I to the unicast, multi-user setting.\\n  Part III gets to an understanding of the question, \"Why is a channel which is capable of communicating a source to within a certain distortion level, also capable of communicating bits at any rate less than the infimum of the rates needed to code the source to within the distortion level\": this lies at the heart of why optimality of separation for communication with a fidelity criterion holds. The perspective taken to get to this understanding is a randomized covering-packing perspective, and the proof is operational.',\n",
       " \"  An operational perspective is used to understand the relationship between source and channel coding. This is based on a direct reduction of one problem to another that uses random coding (and hence common randomness) but unlike all prior work, does not involve any functional computations, in particular, no mutual-information computations. This result is then used to prove a universal source-channel separation theorem in the rate-distortion context where universality is in the sense of a compound ``general channel.''\",\n",
       " \"  Shannon proved that if we can transmit bits reliably at rates larger than the rate distortion function $R(D)$, then we can transmit this source to within a distortion $D$. We answer the converse question ``If we can transmit a source to within a distortion $D$, can we transmit bits reliably at rates less than the rate distortion function?'' in the affirmative. This can be viewed as a direct converse of the rate distortion theorem.\",\n",
       " 'Learning robotic manipulation tasks using reinforcement learning with sparse rewards is currently impractical due to the outrageous data requirements. Many practical tasks require manipulation of multiple objects, and the complexity of such tasks increases with the number of objects. Learning from a curriculum of increasingly complex tasks appears to be a natural solution, but unfortunately, does not work for many scenarios. We hypothesize that the inability of the state-of-the-art algorithms to effectively utilize a task curriculum stems from the absence of inductive biases for transferring knowledge from simpler to complex tasks. We show that graph-based relational architectures overcome this limitation and enable learning of complex tasks when provided with a simple curriculum of tasks with increasing numbers of objects. We demonstrate the utility of our framework on a simulated block stacking task. Starting from scratch, our agent learns to stack six blocks into a tower. Despite using step-wise sparse rewards, our method is orders of magnitude more data-efficient and outperforms the existing state-of-the-art method that utilizes human demonstrations. Furthermore, the learned policy exhibits zero-shot generalization, successfully stacking blocks into taller towers and previously unseen configurations such as pyramids, without any further training.',\n",
       " 'In this work, we consider a multi-wheeled payload transport system. Each of the wheels can be selectively actuated. When they are not actuated, wheels are free moving and do not consume battery power. The payload transport system is modeled as an actuated multi-agent system, with each wheel-motor pair as an agent. Kinematic and dynamic models are developed to ensure that the payload transport system moves as desired. We design optimization formulations to decide on the number of wheels to be active and which of the wheels to be active so that the battery is conserved and the wear on the motors is reduced. Our multi-level control framework over the agents ensures that near-optimal number of agents is active for the payload transport system to function. Through simulation studies we show that our solution ensures energy efficient operation and increases the distance traveled by the payload transport system, for the same battery power. We have built the payload transport system and provide results for preliminary experimental validation.',\n",
       " 'We present a method for storing multiple models within a single set of parameters. Models can coexist in superposition and still be retrieved individually. In experiments with neural networks, we show that a surprisingly large number of models can be effectively stored within a single parameter instance. Furthermore, each of these models can undergo thousands of training steps without significantly interfering with other models within the superposition. This approach may be viewed as the online complement of compression: rather than reducing the size of a network after training, we make use of the unrealized capacity of a network during training.',\n",
       " \"We present an approach for building an active agent that learns to segment its visual observations into individual objects by interacting with its environment in a completely self-supervised manner. The agent uses its current segmentation model to infer pixels that constitute objects and refines the segmentation model by interacting with these pixels. The model learned from over 50K interactions generalizes to novel objects and backgrounds. To deal with noisy training signal for segmenting objects obtained by self-supervised interactions, we propose robust set loss. A dataset of robot's interactions along-with a few human labeled examples is provided as a benchmark for future research. We test the utility of the learned segmentation model by providing results on a downstream vision-based control task of rearranging multiple objects into target configurations from visual inputs alone. Videos, code, and robotic interaction dataset are available at https://pathak22.github.io/seg-by-interaction/\",\n",
       " \"The current dominant paradigm for imitation learning relies on strong supervision of expert actions to learn both 'what' and 'how' to imitate. We pursue an alternative paradigm wherein an agent first explores the world without any expert supervision and then distills its experience into a goal-conditioned skill policy with a novel forward consistency loss. In our framework, the role of the expert is only to communicate the goals (i.e., what to imitate) during inference. The learned policy is then employed to mimic the expert (i.e., how to imitate) after seeing just a sequence of images demonstrating the desired task. Our method is 'zero-shot' in the sense that the agent never has access to expert actions during training or for the task demonstration at inference. We evaluate our zero-shot imitator in two real-world settings: complex rope manipulation with a Baxter robot and navigation in previously unseen office environments with a TurtleBot. Through further experiments in VizDoom simulation, we provide evidence that better mechanisms for exploration lead to learning a more capable policy which in turn improves end task performance. Videos, models, and more details are available at https://pathak22.github.io/zeroshot-imitation/\",\n",
       " 'What makes humans so good at solving seemingly complex video games? Unlike computers, humans bring in a great deal of prior knowledge about the world, enabling efficient decision making. This paper investigates the role of human priors for solving video games. Given a sample game, we conduct a series of ablation studies to quantify the importance of various priors on human performance. We do this by modifying the video game environment to systematically mask different types of visual information that could be used by humans as priors. We find that removal of some prior knowledge causes a drastic degradation in the speed with which human players solve the game, e.g. from 2 minutes to over 20 minutes. Furthermore, our results indicate that general priors, such as the importance of objects and visual consistency, are critical for efficient game-play. Videos and the game manipulations are available at https://rach0012.github.io/humanRL_website/',\n",
       " 'Automated cardiac image interpretation has the potential to transform clinical practice in multiple ways including enabling low-cost serial assessment of cardiac function in the primary care and rural setting. We hypothesized that advances in computer vision could enable building a fully automated, scalable analysis pipeline for echocardiogram (echo) interpretation. Our approach entailed: 1) preprocessing; 2) convolutional neural networks (CNN) for view identification, image segmentation, and phasing of the cardiac cycle; 3) quantification of chamber volumes and left ventricular mass; 4) particle tracking to compute longitudinal strain; and 5) targeted disease detection. CNNs accurately identified views (e.g. 99% for apical 4-chamber) and segmented individual cardiac chambers. Cardiac structure measurements agreed with study report values (e.g. mean absolute deviations (MAD) of 7.7 mL/kg/m2 for left ventricular diastolic volume index, 2918 studies). We computed automated ejection fraction and longitudinal strain measurements (within 2 cohorts), which agreed with commercial software-derived values [for ejection fraction, MAD=5.3%, N=3101 studies; for strain, MAD=1.5% (n=197) and 1.6% (n=110)], and demonstrated applicability to serial monitoring of breast cancer patients for trastuzumab cardiotoxicity. Overall, we found that, compared to manual measurements, automated measurements had superior performance across seven internal consistency metrics with an average increase in the Spearman correlation coefficient of 0.05 (p=0.02). Finally, we developed disease detection algorithms for hypertrophic cardiomyopathy and cardiac amyloidosis, with C-statistics of 0.93 and 0.84, respectively. Our pipeline lays the groundwork for using automated interpretation to support point-of-care handheld cardiac ultrasound and large-scale analysis of the millions of echos archived within healthcare systems.',\n",
       " \"In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at https://pathak22.github.io/noreward-rl/\",\n",
       " 'Manipulation of deformable objects, such as ropes and cloth, is an important but challenging problem in robotics. We present a learning-based system where a robot takes as input a sequence of images of a human manipulating a rope from an initial to goal configuration, and outputs a sequence of actions that can reproduce the human demonstration, using only monocular images as input. To perform this task, the robot learns a pixel-level inverse dynamics model of rope manipulation directly from images in a self-supervised manner, using about 60K interactions with the rope collected autonomously by the robot. The human demonstration provides a high-level plan of what to do and the low-level inverse model is used to execute the plan. We show that by combining the high and low-level plans, the robot can successfully manipulate a rope into a variety of target shapes using only a sequence of human-provided images for direction.',\n",
       " 'When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that state of art deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.',\n",
       " 'The tremendous success of ImageNet-trained deep features on a wide range of transfer tasks begs the question: what are the properties of the ImageNet dataset that are critical for learning good, general-purpose features? This work provides an empirical investigation of various facets of this question: Is more pre-training data always better? How does feature quality depend on the number of training examples per class? Does adding more object classes improve performance? For the same data budget, how should the data be split into classes? Is fine-grained recognition necessary for learning good features? Given the same number of training classes, is it better to have coarse classes or fine-grained classes? Which is better: more classes or more examples per class? To answer these and related questions, we pre-trained CNN features on various subsets of the ImageNet dataset and evaluated transfer performance on PASCAL detection, PASCAL action classification, and SUN scene classification tasks. Our overall findings suggest that most changes in the choice of pre-training data long thought to be critical do not significantly affect transfer performance.? Given the same number of training classes, is it better to have coarse classes or fine-grained classes? Which is better: more classes or more examples per class?',\n",
       " \"We investigate an experiential learning paradigm for acquiring an internal model of intuitive physics. Our model is evaluated on a real-world robotic manipulation task that requires displacing objects to target locations by poking. The robot gathered over 400 hours of experience by executing more than 100K pokes on different objects. We propose a novel approach based on deep neural networks for modeling the dynamics of robot's interactions directly from images, by jointly estimating forward and inverse models of dynamics. The inverse model objective provides supervision to construct informative visual features, which the forward model can then predict and in turn regularize the feature space for the inverse model. The interplay between these two objectives creates useful, accurate models that can then be used for multi-step decision making. This formulation has the additional benefit that it is possible to learn forward models in an abstract feature space and thus alleviate the need of predicting pixels. Our experiments show that this joint modeling approach outperforms alternative methods.\",\n",
       " 'The ability to plan and execute goal specific actions in varied, unexpected settings is a central requirement of intelligent agents. In this paper, we explore how an agent can be equipped with an internal model of the dynamics of the external world, and how it can use this model to plan novel actions by running multiple internal simulations (\"visual imagination\"). Our models directly process raw visual input, and use a novel object-centric prediction formulation based on visual glimpses centered on objects (fixations) to enforce translational invariance of the learned physical laws. The agent gathers training data through random interaction with a collection of different environments, and the resulting model can then be used to plan goal-directed actions in novel environments that the agent has not seen before. We demonstrate that our agent can accurately plan actions for playing a simulated billiards game, which requires pushing a ball into a target position or into collision with another ball.',\n",
       " 'Hierarchical feature extractors such as Convolutional Networks (ConvNets) have achieved impressive performance on a variety of classification tasks using purely feedforward processing. Feedforward architectures can learn rich representations of the input space but do not explicitly model dependencies in the output spaces, that are quite structured for tasks such as articulated human pose estimation or object segmentation. Here we propose a framework that expands the expressive power of hierarchical feature extractors to encompass both input and output spaces, by introducing top-down feedback. Instead of directly predicting the outputs in one go, we use a self-correcting model that progressively changes an initial solution by feeding back error predictions, in a process we call Iterative Error Feedback (IEF). IEF shows excellent performance on the task of articulated pose estimation in the challenging MPII and LSP benchmarks, matching the state-of-the-art without requiring ground truth scale annotation.',\n",
       " 'The dominant paradigm for feature learning in computer vision relies on training neural networks for the task of object recognition using millions of hand labelled images. Is it possible to learn useful features for a diverse set of visual tasks using any other form of supervision? In biology, living organisms developed the ability of visual perception for the purpose of moving and acting in the world. Drawing inspiration from this observation, in this work we investigate if the awareness of egomotion can be used as a supervisory signal for feature learning. As opposed to the knowledge of class labels, information about egomotion is freely available to mobile agents. We show that given the same number of training images, features learnt using egomotion as supervision compare favourably to features learnt using class-label as supervision on visual tasks of scene recognition, object recognition, visual odometry and keypoint matching.',\n",
       " 'The human brain is adept at solving difficult high-level visual processing problems such as image interpretation and object recognition in natural scenes. Over the past few years neuroscientists have made remarkable progress in understanding how the human brain represents categories of objects and actions in natural scenes. However, all current models of high-level human vision operate on hand annotated images in which the objects and actions have been assigned semantic tags by a human operator. No current models can account for high-level visual function directly in terms of low-level visual input (i.e., pixels). To overcome this fundamental limitation we sought to develop a new class of models that can predict human brain activity directly from low-level visual input (i.e., pixels). We explored two classes of models drawn from computer vision and machine learning. The first class of models was based on Fisher Vectors (FV) and the second was based on Convolutional Neural Networks (ConvNets). We find that both classes of models accurately predict brain activity in high-level visual areas, directly from pixels and without the need for any semantic tags or hand annotation of images. This is the first time that such a mapping has been obtained. The fit models provide a new platform for exploring the functional principles of human vision, and they show that modern methods of computer vision and machine learning provide important tools for characterizing brain function.',\n",
       " 'In the last two years, convolutional neural networks (CNNs) have achieved an impressive suite of results on standard recognition datasets and tasks. CNN-based features seem poised to quickly replace engineered representations, such as SIFT and HOG. However, compared to SIFT and HOG, we understand much less about the nature of the features learned by large CNNs. In this paper, we experimentally probe several aspects of CNN feature learning in an attempt to help practitioners gain useful, evidence-backed intuitions about how to apply CNNs to computer vision problems.',\n",
       " 'We present a Machine Learning-based method for tomographic reconstruction of dense layered objects, with range of projection angles limited to $\\\\pm $10$^\\\\circ$. Whereas previous approaches to phase tomography generally require two steps, first to retrieve phase projections from intensity projections and then perform tomographic reconstruction on the retrieved phase projections, in our work a physics-informed pre-processor followed by a Deep Neural Network (DNN) conduct the three-dimensional reconstruction directly from the intensity projections. We demonstrate this single-step method experimentally in the visible optical domain on a scaled up integrated circuit phantom. We show that even under conditions of highly attenuated photon fluxes a DNN trained only on synthetic data can be used to successfully reconstruct physical samples disjoint from the synthetic training set. Thus, the need of producing a large number of physical examples for training is ameliorated. The method is generally applicable to tomography with electromagnetic or other types of radiation at all bands.',\n",
       " 'Several programming languages use garbage collectors (GCs) to automatically manage memory for the programmer. Such collectors must decide when to look for unreachable objects to free, which can have a large performance impact on some applications. In this preliminary work, we propose a design for a learned garbage collector that autonomously learns over time when to perform collections. By using reinforcement learning, our design can incorporate user-defined reward functions, allowing an autonomous garbage collector to learn to optimize the exact metric the user desires (e.g., request latency or queries per second). We conduct an initial experimental study on a prototype, demonstrating that an approach based on tabular Q learning may be promising.',\n",
       " 'Query optimization remains one of the most challenging problems in data management systems. Recent efforts to apply machine learning techniques to query optimization challenges have been promising, but have shown few practical gains due to substantive training overhead, inability to adapt to changes, and poor tail performance. Motivated by these difficulties and drawing upon a long history of research in multi-armed bandits, we introduce Bao (the BAndit Optimizer). Bao takes advantage of the wisdom built into existing query optimizers by providing per-query optimization hints. Bao combines modern tree convolutional neural networks with Thompson sampling, a decades-old and well-studied reinforcement learning algorithm. As a result, Bao automatically learns from its mistakes and adapts to changes in query workloads, data, and schema. Experimentally, we demonstrate that Bao can quickly (an order of magnitude faster than previous approaches) learn strategies that improve end-to-end query execution performance, including tail latency. In cloud environments, we show that Bao can offer both reduced costs and better performance compared with a sophisticated commercial system.',\n",
       " 'Inferring road attributes such as lane count and road type from satellite imagery is challenging. Often, due to the occlusion in satellite imagery and the spatial correlation of road attributes, a road attribute at one position on a road may only be apparent when considering far-away segments of the road. Thus, to robustly infer road attributes, the model must integrate scattered information and capture the spatial correlation of features along roads. Existing solutions that rely on image classifiers fail to capture this correlation, resulting in poor accuracy. We find this failure is caused by a fundamental limitation -- the limited effective receptive field of image classifiers. To overcome this limitation, we propose RoadTagger, an end-to-end architecture which combines both Convolutional Neural Networks (CNNs) and Graph Neural Networks (GNNs) to infer road attributes. The usage of graph neural networks allows information propagation on the road network graph and eliminates the receptive field limitation of image classifiers. We evaluate RoadTagger on both a large real-world dataset covering 688 km^2 area in 20 U.S. cities and a synthesized micro-dataset. In the evaluation, RoadTagger improves inference accuracy over the CNN image classifier based approaches. RoadTagger also demonstrates strong robustness against different disruptions in the satellite imagery and the ability to learn complicated inductive rules for aggregating scattered information along the road network.',\n",
       " 'Scanning and filtering over multi-dimensional tables are key operations in modern analytical database engines. To optimize the performance of these operations, databases often create clustered indexes over a single dimension or multi-dimensional indexes such as R-trees, or use complex sort orders (e.g., Z-ordering). However, these schemes are often hard to tune and their performance is inconsistent across different datasets and queries. In this paper, we introduce Flood, a multi-dimensional in-memory index that automatically adapts itself to a particular dataset and workload by jointly optimizing the index structure and data storage. Flood achieves up to three orders of magnitude faster performance for range scans with predicates than state-of-the-art multi-dimensional indexes or sort orders on real-world datasets and workloads. Our work serves as a building block towards an end-to-end learned database system.',\n",
       " 'Street maps are a crucial data source that help to inform a wide range of decisions, from navigating a city to disaster relief and urban planning. However, in many parts of the world, street maps are incomplete or lag behind new construction. Editing maps today involves a tedious process of manually tracing and annotating roads, buildings, and other map features.\\n  Over the past decade, many automatic map inference systems have been proposed to automatically extract street map data from satellite imagery, aerial imagery, and GPS trajectory datasets. However, automatic map inference has failed to gain traction in practice due to two key limitations: high error rates (low precision), which manifest in noisy inference outputs, and a lack of end-to-end system design to leverage inferred data to update existing street maps.\\n  At MIT and QCRI, we have developed a number of algorithms and approaches to address these challenges, which we combined into a new system we call Mapster. Mapster is a human-in-the-loop street map editing system that incorporates three components to robustly accelerate the mapping process over traditional tools and workflows: high-precision automatic map inference, data refinement, and machine-assisted map editing.\\n  Through an evaluation on a large-scale dataset including satellite imagery, GPS trajectories, and ground-truth map data in forty cities, we show that Mapster makes automation practical for map editing, and enables the curation of map datasets that are more complete and up-to-date at less cost.',\n",
       " \"Bitcoin is the first fully decentralized permissionless blockchain protocol and achieves a high level of security: the ledger it maintains has guaranteed liveness and consistency properties as long as the adversary has less compute power than the honest nodes. However, its throughput is only 7 transactions per second and the confirmation latency can be up to hours. Prism is a new blockchain protocol which is designed to achieve a natural scaling of Bitcoin's performance while maintaining its full security guarantees. We present an implementation of Prism which achieves a throughput of 70,000 transactions per second and confirmation latencies of tens of seconds.\",\n",
       " 'Effective congestion control for data center networks is becoming increasingly challenging with rapidly increasing workload demand, ever faster links, small average transfer sizes, extremely bursty traffic, limited switch buffer capacity, and latency-sensitive applications. Widely deployed algorithms, such as DCTCP and DCQCN, are still far from optimal in many plausible scenarios, particularly for tail latency. Many operators compensate by running their networks at low average utilization, dramatically increasing costs.\\n  In this paper, we argue that we have reached the practical limits of end-to-end congestion control. Instead, we propose a new clean slate design based on hop-by-hop per-flow flow control. We show that our approach achieves near optimal tail latency behavior even under challenging conditions such as high link utilization and in-cast cross traffic. By contrast with prior hop-by-hop schemes, we show that per-flow flow control requires only limited metadata and packet buffering.',\n",
       " 'We present Placeto, a reinforcement learning (RL) approach to efficiently find device placements for distributed neural network training. Unlike prior approaches that only find a device placement for a specific computation graph, Placeto can learn generalizable device placement policies that can be applied to any graph. We propose two key ideas in our approach: (1) we represent the policy as performing iterative placement improvements, rather than outputting a placement in one shot; (2) we use graph embeddings to capture relevant information about the structure of the computation graph, without relying on node labels for indexing. These ideas allow Placeto to train efficiently and generalize to unseen graphs. Our experiments show that Placeto requires up to 6.1x fewer training steps to find placements that are on par with or better than the best placements found by prior approaches. Moreover, Placeto is able to learn a generalizable placement policy for any given family of graphs, which can then be used without any retraining to predict optimized placements for unseen graphs from the same family. This eliminates the large overhead incurred by prior RL approaches whose lack of generalizability necessitates re-training from scratch every time a new graph is to be placed.',\n",
       " 'Mapping road networks today is labor-intensive. As a result, road maps have poor coverage outside urban centers in many countries. Systems to automatically infer road network graphs from aerial imagery and GPS trajectories have been proposed to improve coverage of road maps. However, because of high error rates, these systems have not been adopted by mapping communities. We propose machine-assisted map editing, where automatic map inference is integrated into existing, human-centric map editing workflows. To realize this, we build Machine-Assisted iD (MAiD), where we extend the web-based OpenStreetMap editor, iD, with machine-assistance functionality. We complement MAiD with a novel approach for inferring road topology from aerial imagery that combines the speed of prior segmentation approaches with the accuracy of prior iterative graph construction methods. We design MAiD to tackle the addition of major, arterial roads in regions where existing maps have poor coverage, and the incremental improvement of coverage in regions where major roads are already mapped. We conduct two user studies and find that, when participants are given a fixed time to map roads, they are able to add as much as 3.5x more roads with MAiD.',\n",
       " \"Symbol detection for Massive Multiple-Input Multiple-Output (MIMO) is a challenging problem for which traditional algorithms are either impractical or suffer from performance limitations. Several recently proposed learning-based approaches achieve promising results on simple channel models (e.g., i.i.d. Gaussian). However, their performance degrades significantly on real-world channels with spatial correlation. We propose MMNet, a deep learning MIMO detection scheme that significantly outperforms existing approaches on realistic channels with the same or lower computational complexity. MMNet's design builds on the theory of iterative soft-thresholding algorithms and uses a novel training algorithm that leverages temporal and spectral correlation to accelerate training. Together, these innovations allow MMNet to train online for every realization of the channel. On i.i.d. Gaussian channels, MMNet requires two orders of magnitude fewer operations than existing deep learning schemes but achieves near-optimal performance. On spatially-correlated channels, it achieves the same error rate as the next-best learning scheme (OAMPNet) at 2.5dB lower SNR and with at least 10x less computational complexity. MMNet is also 4--8dB better overall than a classic linear scheme like the minimum mean square error (MMSE) detector.\",\n",
       " 'In this paper, we show that extending the butterfly operations from the FFT algorithm to a general Butterfly Transform (BFT) can be beneficial in building an efficient block structure for CNN designs. Pointwise convolutions, which we refer to as channel fusions, are the main computational bottleneck in the state-of-the-art efficient CNNs (e.g. MobileNets ). We introduce a set of criteria for channel fusion and prove that BFT yields an asymptotically optimal FLOP count with respect to these criteria. By replacing pointwise convolutions with BFT, we reduce the computational complexity of these layers from O(n^2) to O(n\\\\log n) with respect to the number of channels. Our experimental evaluations show that our method results in significant accuracy gains across a wide range of network architectures, especially at low FLOP ranges. For example, BFT results in up to a 6.75% absolute Top-1 improvement for MobileNetV1, 4.4 \\\\% for ShuffleNet V2 and 5.4% for MobileNetV3 on ImageNet under a similar number of FLOPS. Notably, ShuffleNet-V2+BFT outperforms state-of-the-art architecture search methods MNasNet, FBNet and MobilenetV3 in the low FLOP regime.',\n",
       " 'We propose Accel-Brake Control (ABC), a simple and deployable explicit congestion control protocol for network paths with time-varying wireless links. ABC routers mark each packet with an \"accelerate\" or \"brake\", which causes senders to slightly increase or decrease their congestion windows. Routers use this feedback to quickly guide senders towards a desired target rate. ABC requires no changes to header formats or user devices, but achieves better performance than XCP. ABC is also incrementally deployable; it operates correctly when the bottleneck is a non-ABC router, and can coexist with non-ABC traffic sharing the same bottleneck link. We evaluate ABC using a Wi-Fi implementation and trace-driven emulation of cellular links. ABC achieves 30-40% higher throughput than Cubic+Codel for similar delays, and 2.2X lower delays than BBR on a Wi-Fi path. On cellular network paths, ABC achieves 50% higher throughput than Cubic+Codel.',\n",
       " 'Query optimization is one of the most challenging problems in database systems. Despite the progress made over the past decades, query optimizers remain extremely complex components that require a great deal of hand-tuning for specific workloads and datasets. Motivated by this shortcoming and inspired by recent advances in applying machine learning to data management challenges, we introduce Neo (Neural Optimizer), a novel learning-based query optimizer that relies on deep neural networks to generate query executions plans. Neo bootstraps its query optimization model from existing optimizers and continues to learn from incoming queries, building upon its successes and learning from its failures. Furthermore, Neo naturally adapts to underlying data patterns and is robust to estimation errors. Experimental results demonstrate that Neo, even when bootstrapped from a simple optimizer like PostgreSQL, can learn a model that offers similar performance to state-of-the-art commercial optimizers, and in some cases even surpass them.',\n",
       " 'In this paper, we are eager to construct a new class of (n+1)-dimensional static magnetic brane solutions in quasi-topological gravity coupled to nonlinear electrodynamics such as exponential and logarithmic forms. The solutions of this magnetic brane are horizonless and have no curvature. For Ïnear r_{+}, the solution f(Ï) is dependent to the values of parameters q and n and for larger Ï, it depends on the coefficients of Love-Lock and quasi-topological gravities Î», Î¼and c. The obtained solutions also have a conic singularity at r=0 with a deficit angle that is only dependent to the parameters q, n and Î². We should remind that the two forms of nonlinear electrodynamics theory have similar behaviors on the obtained solutions. At last, by using the counterterm method, we obtain conserved quantities such as mass and electric charge. The value of the electric charge for this static magnetic brane is obtained zero.',\n",
       " 'Metallic layers are known to be used for the suppression of wave transmission when their thickness is sufficiently higher than the skin depth of metal. If in addition to blocking the transmission, metallic layers have the feature of blocking the reflection, too, they would make perfect absorbers. In this work, we propose an experimental approach of using a single thin layer of Manganese (Mn) as both the transmission suppresser and the reflection suppresser. This approach leads to obtaining lithography-free ultra-broadband perfect absorption in an ultra-wide spectrum ranging from Ultraviolet (UV) to Far Infrared (FIR). The measured average absorption is approximately 99%. Such a promising result can be achieved by only coating a single Mn layer on high-roughness substrates that include random nano-pyramids on it. In other words, we do not need a stack of different materials and combinations of geometrical features. The high roughness is realized on a commercial Silicon wafer substrate by inductively coupled plasma (ICP) etching. The key to this ultra-wideband absorption is electromagnetic field tapering which exists due to the graded-index feature of the structure (known as moth-eye effect), along with the ideal optical properties of Mn which makes it an excellent metal for broadband absorption applications. A full experimental characterization of the fabricated samples is presented along with the physical analysis of the phenomena. The findings of this paper can be used for the realization of lithography-free, cost-effective and high-throughput mass production of broadband absorbers.',\n",
       " \"Efficiently scheduling data processing jobs on distributed compute clusters requires complex algorithms. Current systems, however, use simple generalized heuristics and ignore workload characteristics, since developing and tuning a scheduling policy for each workload is infeasible. In this paper, we show that modern machine learning techniques can generate highly-efficient policies automatically. Decima uses reinforcement learning (RL) and neural networks to learn workload-specific scheduling algorithms without any human instruction beyond a high-level objective such as minimizing average job completion time. Off-the-shelf RL techniques, however, cannot handle the complexity and scale of the scheduling problem. To build Decima, we had to develop new representations for jobs' dependency graphs, design scalable RL models, and invent RL training methods for dealing with continuous stochastic job arrivals. Our prototype integration with Spark on a 25-node cluster shows that Decima improves the average job completion time over hand-tuned scheduling heuristics by at least 21%, achieving up to 2x improvement during periods of high cluster load.\",\n",
       " 'Despite growing adoption of cryptocurrencies, making fast payments at scale remains a challenge. Payment channel networks (PCNs) such as the Lightning Network have emerged as a viable scaling solution. However, completing payments on PCNs is challenging: payments must be routed on paths with sufficient funds. As payments flow over a single channel (link) in the same direction, the channel eventually becomes depleted and cannot support further payments in that direction; hence, naive routing schemes like shortest-path routing can deplete key payment channels and paralyze the system. Today\\'s PCNs also route payments atomically, worsening the problem. In this paper, we present Spider, a routing solution that \"packetizes\" transactions and uses a multi-path transport protocol to achieve high-throughput routing in PCNs. Packetization allows Spider to complete even large transactions on low-capacity payment channels over time, while the multi-path congestion control protocol ensures balanced utilization of channels and fairness across flows. Extensive simulations comparing Spider with state-of-the-art approaches shows that Spider requires less than 25% of the funds to successfully route over 95% of transactions on balanced traffic demands, and offloads 4x more transactions onto the PCN on imbalanced demands.',\n",
       " 'Prior research has proposed technical solutions to use peer-to-peer (P2P) content delivery to serve Internet video, showing that it can reduce costs to content providers. Yet, such methods have not become widespread except for a few niche instances. An important challenge is incentivization: what tangible benefits does P2P content delivery offer users who bring resources to the table? In this paper, we ask whether monetary incentives can help attract peers in P2P content delivery systems. We commissioned a professional survey of people around theUnited States to answer several relevant questions. We found that 51% of the 876 respondents--substantially larger than our expectations--answered \"yes\" to whether they would participate for suitable financial incentives. Encouraged by the results of the survey, we propose Gringotts, a system to structure incentives and securely incorporate P2P delivery into content delivery systems. Gringotts provides a novel Proof of Delivery mechanism that allows content providers to verify correct delivery of their files, and shows how to use cryptocurrency to pay peers while guarding against liars and Sybil attacks.',\n",
       " 'We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with disturbances, and object tracking. Since the state dynamics and rewards depend on the input process, the state alone provides limited information for the expected future returns. Therefore, policy gradient methods with standard state-dependent baselines suffer high variance during training. We derive a bias-free, input-dependent baseline to reduce this variance, and analytically show its benefits over state-dependent baselines. We then propose a meta-learning approach to overcome the complexity of learning a baseline that depends on a long sequence of inputs. Our experimental results show that across environments from queuing systems, computer networks, and MuJoCo robotic locomotion, input-dependent baselines consistently improve training stability and result in better eventual policies.',\n",
       " \"Homa is a new transport protocol for datacenter networks. It provides exceptionally low latency, especially for workloads with a high volume of very short messages, and it also supports large messages and high network utilization. Homa uses in-network priority queues to ensure low latency for short messages; priority allocation is managed dynamically by each receiver and integrated with a receiver-driven flow control mechanism. Homa also uses controlled overcommitment of receiver downlinks to ensure efficient bandwidth utilization at high load. Our implementation of Homa delivers 99th percentile round-trip times less than 15Î¼s for short messages on a 10 Gbps network running at 80% load. These latencies are almost 100x lower than the best published measurements of an implementation. In simulations, Homa's latency is roughly equal to pFabric and significantly better than pHost, PIAS, and NDP for almost all message sizes and workloads. Homa can also sustain higher network loads than pFabric, pHost, or PIAS.\",\n",
       " 'This paper introduces Nimbus, a robust technique to detect whether the cross traffic competing with a flow is \"elastic\", and shows that this elasticity detector improves congestion control. If cross traffic is inelastic, then a sender can control queueing delays while achieving high throughput, but in the presence of elastic traffic, it may lose throughput if it attempts to control packet delay. To estimate elasticity, Nimbus modulates the flow\\'s sending rate with sinusoidal pulses that create small traffic fluctuations at the bottleneck link, and measures the frequency response of the rate of the cross traffic. Our results on emulated and real-world paths show that congestion control using elasticity detection achieves throughput comparable to Cubic, but with delays that are 50-70 ms lower when cross traffic is inelastic. Nimbus detects the nature of the cross traffic more accurately than Copa, and is usable as a building block by other end-to-end algorithms.',\n",
       " 'Neural networks have been shown to be an effective tool for learning algorithms over graph-structured data. However, graph representation techniques---that convert graphs to real-valued vectors for use with neural networks---are still in their infancy. Recent works have proposed several approaches (e.g., graph convolutional networks), but these methods have difficulty scaling and generalizing to graphs with different sizes and shapes. We present Graph2Seq, a new technique that represents vertices of graphs as infinite time-series. By not limiting the representation to a fixed dimension, Graph2Seq scales naturally to graphs of arbitrary sizes and shapes. Graph2Seq is also reversible, allowing full recovery of the graph structure from the sequences. By analyzing a formal computational model for graph representation, we show that an unbounded sequence is necessary for scalability. Our experimental results with Graph2Seq show strong generalization and new state-of-the-art performance on a variety of graph combinatorial optimization problems.',\n",
       " 'Mapping road networks is currently both expensive and labor-intensive. High-resolution aerial imagery provides a promising avenue to automatically infer a road network. Prior work uses convolutional neural networks (CNNs) to detect which pixels belong to a road (segmentation), and then uses complex post-processing heuristics to infer graph connectivity. We show that these segmentation methods have high error rates because noisy CNN outputs are difficult to correct. We propose RoadTracer, a new method to automatically construct accurate road network maps from aerial images. RoadTracer uses an iterative search process guided by a CNN-based decision function to derive the road network graph directly from the output of the CNN. We compare our approach with a segmentation method on fifteen cities, and find that at a 5% error rate, RoadTracer correctly captures 45% more junctions across these cities.',\n",
       " 'Let $\\\\mathcal{A}$ and $\\\\mathcal{B}$\\\\ are $C^{\\\\huge \\\\ast}% $-algebras\\\\textbf{.} A\\\\textbf{ }linear map $Ï†:\\\\mathcal{A\\\\rightarrow B}$ is $C^{\\\\ast}$-Jordan homomorphism if it is a Jordan homomorphism which preserves the adjoint operation. In this note we show that $C^{\\\\ast}$-Jordan homomorphisms -- under mild assumptions -- preserving covariance set and covariance coset in $C^{\\\\ast}$-algebras.',\n",
       " 'As its price per bit drops, SSD is increasingly becoming the default storage medium for cloud application databases. However, it has not become the preferred storage medium for key-value caches, even though SSD offers more than 10x lower price per bit and sufficient performance compared to DRAM. This is because key-value caches need to frequently insert, update and evict small objects. This causes excessive writes and erasures on flash storage, since flash only supports writes and erasures of large chunks of data. These excessive writes and erasures significantly shorten the lifetime of flash, rendering it impractical to use for key-value caches. We present Flashield, a hybrid key-value cache that uses DRAM as a \"filter\" to minimize writes to SSD. Flashield performs light-weight machine learning profiling to predict which objects are likely to be read frequently before getting updated; these objects, which are prime candidates to be stored on SSD, are written to SSD in large chunks sequentially. In order to efficiently utilize the cache\\'s available memory, we design a novel in-memory index for the variable-sized objects stored on flash that requires only 4 bytes per object in DRAM. We describe Flashield\\'s design and implementation and, we evaluate it on a real-world cache trace. Compared to state-of-the-art systems that suffer a write amplification of 2.5x or more, Flashield maintains a median write amplification of 0.5x without any loss of hit rate or throughput.',\n",
       " 'Switches today provide a small set of scheduling algorithms. While we can tweak scheduling parameters, we cannot modify algorithmic logic, or add a completely new algorithm, after the switch has been designed. This paper presents a design for a programmable packet scheduler, which allows scheduling algorithms---potentially algorithms that are unknown today---to be programmed into a switch without requiring hardware redesign.\\n  Our design builds on the observation that scheduling algorithms make two decisions: in what order to schedule packets and when to schedule them. Further, in many scheduling algorithms these decisions can be made when packets are enqueued. We leverage this observation to build a programmable scheduler using a single abstraction: the push-in first-out queue (PIFO), a priority queue that maintains the scheduling order and time for such algorithms.\\n  We show that a programmable scheduler using PIFOs lets us program a wide variety of scheduling algorithms. We present a detailed hardware design for this scheduler for a 64-port 10 Gbit/s shared-memory switch with <4% chip area overhead on a 16-nm standard-cell library. Our design lets us program many sophisticated algorithms, such as a 5-level hierarchical scheduler with programmable scheduling algorithms at each level.',\n",
       " \"Many algorithms for congestion control, scheduling, network measurement, active queue management, security, and load balancing require custom processing of packets as they traverse the data plane of a network switch. To run at line rate, these data-plane algorithms must be in hardware. With today's switch hardware, algorithms cannot be changed, nor new algorithms installed, after a switch has been built.\\n  This paper shows how to program data-plane algorithms in a high-level language and compile those programs into low-level microcode that can run on emerging programmable line-rate switching chipsets. The key challenge is that these algorithms create and modify algorithmic state. The key idea to achieve line-rate programmability for stateful algorithms is the notion of a packet transaction : a sequential code block that is atomic and isolated from other such code blocks. We have developed this idea in Domino, a C-like imperative language to express data-plane algorithms. We show with many examples that Domino provides a convenient and natural way to express sophisticated data-plane algorithms, and show that these algorithms can be run at line rate with modest estimated die-area overhead.\",\n",
       " \"Hybrid switching - in which a high bandwidth circuit switch (optical or wireless) is used in conjunction with a low bandwidth packet switch - is a promising alternative to interconnect servers in today's large scale data-centers. Circuit switches offer a very high link rate, but incur a non-trivial reconfiguration delay which makes their scheduling challenging. In this paper, we demonstrate a lightweight, simple and nearly-optimal scheduling algorithm that trades-off configuration costs with the benefits of reconfiguration that match the traffic demands. The algorithm has strong connections to submodular optimization, has performance at least half that of the optimal schedule and strictly outperforms state of the art in a variety of traffic demand settings. These ideas naturally generalize: we see that indirect routing leads to exponential connectivity; this is another phenomenon of the power of multi hop routing, distinct from the well-known load balancing effects.\",\n",
       " 'Let $Ïƒ(A)$, $Ï(A)$ and $r(A)$ denote the spectrum, spectral radius and numerical radius of a bounded linear operator $A$ on a Hilbert space $H$, respectively. We show that a linear operator $A$ satisfying $$Ï(AB)\\\\le r(A)r(B) \\\\quad\\\\text{ for all bounded linear operators } B$$ if and only if there is a unique $Î¼\\\\in Ïƒ(A)$ satisfying $|Î¼| = Ï(A)$ and $A = \\\\frac{Î¼(I + L)}{2}$ for a contraction $L$ with $1\\\\inÏƒ(L)$. One can get the same conclusion on $A$ if $Ï(AB) \\\\le r(A)r(B)$ for all rank one operators $B$. If $H$ is of finite dimension, we can further decompose $L$ as a direct sum of $C \\\\oplus 0$ under a suitable choice of orthonormal basis so that $Re(C^{-1}x,x) \\\\ge 1$ for all unit vector $x$.',\n",
       " 'This paper presents a practical approach to rapidly introduce new dataplane functionality into networks: End-hosts embed tiny programs into packets to actively query and manipulate a network\\'s internal state. We show how this \"tiny packet program\" (TPP) interface gives end-hosts unprecedented visibility into network behavior, enabling them to work with the network to achieve a common goal. Our design leverages what each component does best: (a) switches forward and execute tiny packet programs (at most 5 instructions) at line rate, and (b) end-hosts perform arbitrary computation on network state, which are easy to evolve. Using a hardware prototype on a NetFPGA, we show our design is feasible, at a reasonable cost. By implementing three different research proposals, we show that TPPs are also useful. And finally, we present an architecture in which they can be made secure.',\n",
       " \"This paper shows how to generate code that efficiently converts sparse tensors between disparate storage formats (data layouts) such as CSR, DIA, ELL, and many others. We decompose sparse tensor conversion into three logical phases: coordinate remapping, analysis, and assembly. We then develop a language that precisely describes how different formats group together and order a tensor's nonzeros in memory. This lets a compiler emit code that performs complex remappings of nonzeros when converting between formats. We also develop a query language that can extract statistics about sparse tensors, and we show how to emit efficient analysis code that computes such queries. Finally, we define an abstract interface that captures how data structures for storing a tensor can be efficiently assembled given specific statistics about the tensor. Disparate formats can implement this common interface, thus letting a compiler emit optimized sparse tensor conversion code for arbitrary combinations of many formats without hard-coding for any specific combination.\\n  Our evaluation shows that the technique generates sparse tensor conversion routines with performance between 1.00 and 2.01$\\\\times$ that of hand-optimized versions in SPARSKIT and Intel MKL, two popular sparse linear algebra libraries. And by emitting code that avoids materializing temporaries, which both libraries need for many combinations of source and target formats, our technique outperforms those libraries by 1.78 to 4.01$\\\\times$ for CSC/COO to DIA/ELL conversion.\",\n",
       " 'We address the problem of optimizing mixed sparse and dense tensor algebra in a compiler. We show that standard loop transformations, such as strip-mining, tiling, collapsing, parallelization and vectorization, can be applied to irregular loops over sparse iteration spaces. We also show how these transformations can be applied to the contiguous value arrays of sparse tensor data structures, which we call their position space, to unlock load-balanced tiling and parallelism.\\n  We have prototyped these concepts in the open-source TACO system, where they are exposed as a scheduling API similar to the Halide domain-specific language for dense computations. Using this scheduling API, we show how to optimize mixed sparse/dense tensor algebra expressions, how to generate load-balanced code by scheduling sparse tensor algebra in position space, and how to generate sparse tensor algebra GPU code. Our evaluation shows that our transformations let us generate good code that is competitive with many hand-optimized implementations from the literature.',\n",
       " 'Many graph problems can be solved using ordered parallel graph algorithms that achieve significant speedup over their unordered counterparts by reducing redundant work. This paper introduces a new priority-based extension to GraphIt, a domain-specific language for writing graph applications, to simplify writing high-performance parallel ordered graph algorithms. The extension enables vertices to be processed in a dynamic order while hiding low-level implementation details from the user. We extend the compiler with new program analyses, transformations, and code generation to produce fast implementations of ordered parallel graph algorithms. We also introduce bucket fusion, a new performance optimization that fuses together different rounds of ordered algorithms to reduce synchronization overhead, resulting in $1.2\\\\times$--3$\\\\times$ speedup over the fastest existing ordered algorithm implementations on road networks with large diameters. With the extension, GraphIt achieves up to 3$\\\\times$ speedup on six ordered graph algorithms over state-of-the-art frameworks and hand-optimized implementations (Julienne, Galois, and GAPBS) that support ordered algorithms.',\n",
       " \"Modern microprocessors are equipped with Single Instruction Multiple Data (SIMD) or vector instructions which expose data level parallelism at a fine granularity. Programmers exploit this parallelism by using low-level vector intrinsics in their code. However, once programs are written using vector intrinsics of a specific instruction set, the code becomes non-portable. Modern compilers are unable to analyze and retarget the code to newer vector instruction sets. Hence, programmers have to manually rewrite the same code using vector intrinsics of a newer generation to exploit higher data widths and capabilities of new instruction sets. This process is tedious, error-prone and requires maintaining multiple code bases. We propose Revec, a compiler optimization pass which revectorizes already vectorized code, by retargeting it to use vector instructions of newer generations. The transformation is transparent, happening at the compiler intermediate representation level, and enables performance portability of hand-vectorized code.\\n  Revec can achieve performance improvements in real-world performance critical kernels. In particular, Revec achieves geometric mean speedups of 1.160$\\\\times$ and 1.430$\\\\times$ on fast integer unpacking kernels, and speedups of 1.145$\\\\times$ and 1.195$\\\\times$ on hand-vectorized x265 media codec kernels when retargeting their SSE-series implementations to use AVX2 and AVX-512 vector instructions respectively. We also extensively test Revec's impact on 216 intrinsic-rich implementations of image processing and stencil kernels relative to hand-retargeting.\",\n",
       " \"Predicting the number of clock cycles a processor takes to execute a block of assembly instructions in steady state (the throughput) is important for both compiler designers and performance engineers. Building an analytical model to do so is especially complicated in modern x86-64 Complex Instruction Set Computer (CISC) machines with sophisticated processor microarchitectures in that it is tedious, error prone, and must be performed from scratch for each processor generation. In this paper we present Ithemal, the first tool which learns to predict the throughput of a set of instructions. Ithemal uses a hierarchical LSTM--based approach to predict throughput based on the opcodes and operands of instructions in a basic block. We show that Ithemal is more accurate than state-of-the-art hand-written tools currently used in compiler backends and static machine code analyzers. In particular, our model has less than half the error of state-of-the-art analytical models (LLVM's llvm-mca and Intel's IACA). Ithemal is also able to predict these throughput values just as fast as the aforementioned tools, and is easily ported across a variety of processor microarchitectures with minimal developer effort.\",\n",
       " 'Modern out-of-order processors have increased capacity to exploit instruction level parallelism (ILP) and memory level parallelism (MLP), e.g., by using wide superscalar pipelines and vector execution units, as well as deep buffers for in-flight memory requests. These resources, however, often exhibit poor utilization rates on workloads with large working sets, e.g., in-memory databases, key-value stores, and graph analytics, as compilers and hardware struggle to expose ILP and MLP from the instruction stream automatically.\\n  In this paper, we introduce the IMLP (Instruction and Memory Level Parallelism) task programming model. IMLP tasks execute as coroutines that yield execution at annotated long-latency operations, e.g., memory accesses, divisions, or unpredictable branches. IMLP tasks are interleaved on a single thread, and integrate well with thread parallelism and vectorization. Our DSL embedded in C++, Cimple, allows exploration of task scheduling and transformations, such as buffering, vectorization, pipelining, and prefetching.\\n  We demonstrate state-of-the-art performance on core algorithms used in in-memory databases that operate on arrays, hash tables, trees, and skip lists. Cimple applications reach 2.5x throughput gains over hardware multithreading on a multi-core, and 6.4x single thread speedup.',\n",
       " 'The performance bottlenecks of graph applications depend not only on the algorithm and the underlying hardware, but also on the size and structure of the input graph. Programmers must try different combinations of a large set of techniques to develop the best implementation for a specific algorithm and type of graph. Existing graph frameworks lack flexibility, supporting only a limited set of optimizations.\\n  This paper introduces GraphIt, a new DSL for graph computations that generates fast implementations for algorithms with different performance characteristics running on graphs with different sizes and structures. GraphIt separates what is computed (algorithm) from how it is computed (schedule). Programmers specify the algorithm using an algorithm language, and performance optimizations are specified using a scheduling language. The algorithm language simplifies expressing the algorithms. We formulate graph optimizations, including edge traversal direction, data layout, parallelization, cache, NUMA, and kernel fusion optimizations, as tradeoffs among locality, parallelism, and work-efficiency. The scheduling language enables programmers to easily search through this complicated tradeoff space by composing together optimizations. We also built an autotuner to automatically find high-performance schedules. The compiler uses a new scheduling representation, the graph iteration space, to model, compose, and ensure the validity of the large number of optimizations. GraphIt outperforms the next fastest of six state-of-the-art shared-memory frameworks (Ligra, Green-Marl, GraphMat, Galois, Gemini, and Grazelle) on 24 out of 32 experiments by up to 4.8$\\\\times$, and is never more than 43% slower than the fastest framework on the other experiments. GraphIt also reduces the lines of code by up to an order of magnitude compared to the next fastest framework.',\n",
       " 'This paper introduces Tiramisu, a polyhedral framework designed to generate high performance code for multiple platforms including multicores, GPUs, and distributed machines. Tiramisu introduces a scheduling language with novel extensions to explicitly manage the complexities that arise when targeting these systems. The framework is designed for the areas of image processing, stencils, linear algebra and deep learning. Tiramisu has two main features: it relies on a flexible representation based on the polyhedral model and it has a rich scheduling language allowing fine-grained control of optimizations. Tiramisu uses a four-level intermediate representation that allows full separation between the algorithms, loop transformations, data layouts, and communication. This separation simplifies targeting multiple hardware architectures with the same algorithm. We evaluate Tiramisu by writing a set of image processing, deep learning, and linear algebra benchmarks and compare them with state-of-the-art compilers and hand-tuned libraries. We show that Tiramisu matches or outperforms existing compilers and libraries on different hardware architectures, including multicore CPUs, GPUs, and distributed machines.',\n",
       " 'This paper shows how to build a sparse tensor algebra compiler that is agnostic to tensor formats (data layouts). We develop an interface that describes formats in terms of their capabilities and properties, and show how to build a modular code generator where new formats can be added as plugins. We then describe six implementations of the interface that compose to form the dense, CSR/CSF, COO, DIA, ELL, and HASH tensor formats and countless variants thereof. With these implementations at hand, our code generator can generate code to compute any tensor algebra expression on any combination of the aforementioned formats.\\n  To demonstrate our technique, we have implemented it in the taco tensor algebra compiler. Our modular code generator design makes it simple to add support for new tensor formats, and the performance of the generated code is competitive with hand-optimized implementations. Furthermore, by extending taco to support a wider range of formats specialized for different application and data characteristics, we can improve end-user application performance. For example, if input data is provided in the COO format, our technique allows computing a single matrix-vector multiplication directly with the data in COO, which is up to 3.6$\\\\times$ faster than by first converting the data to CSR.',\n",
       " \"Modern microprocessors are equipped with single instruction multiple data (SIMD) or vector instruction sets which allow compilers to exploit superword level parallelism (SLP), a type of fine-grained parallelism. Current SLP auto-vectorization techniques use heuristics to discover vectorization opportunities in high-level language code. These heuristics are fragile, local and typically only present one vectorization strategy that is either accepted or rejected by a cost model. We present goSLP, a novel SLP auto-vectorization framework which solves the statement packing problem in a pairwise optimal manner. Using an integer linear programming (ILP) solver, goSLP searches the entire space of statement packing opportunities for a whole function at a time, while limiting total compilation time to a few minutes. Furthermore, goSLP optimally solves the vector permutation selection problem using dynamic programming. We implemented goSLP in the LLVM compiler infrastructure, achieving a geometric mean speedup of 7.58% on SPEC2017fp, 2.42% on SPEC2006fp and 4.07% on NAS benchmarks compared to LLVM's existing SLP auto-vectorizer.\",\n",
       " 'In this position paper, we describe our vision of the future of machine programming through a categorical examination of three pillars of research. Those pillars are: (i) intention, (ii) invention, and(iii) adaptation. Intention emphasizes advancements in the human-to-computer and computer-to-machine-learning interfaces. Invention emphasizes the creation or refinement of algorithms or core hardware and software building blocks through machine learning (ML). Adaptation emphasizes advances in the use of ML-based constructs to autonomously evolve software.',\n",
       " 'High-performance DSL developers work hard to take advantage of modern hardware. The DSL compilers have to build their own complex middle-ends before they can target a common back-end such as LLVM, which only handles single instruction streams with SIMD instructions. We introduce Tiramisu, a common middle-end that can generate efficient code for modern processors and accelerators such as multicores, GPUs, FPGAs and distributed clusters. Tiramisu introduces a novel three-level IR that separates the algorithm, how that algorithm is executed, and where intermediate data are stored. This separation simplifies optimization and makes targeting multiple hardware architectures from the same algorithm easier. As a result, DSL compilers can be made considerably less complex with no loss of performance while immediately targeting multiple hardware or hardware combinations such as distributed nodes with both CPUs and GPUs. We evaluated Tiramisu by creating a new middle-end for the Halide and Julia compilers. We show that Tiramisu extends Halide and Julia with many new capabilities including the ability to: express new algorithms (such as recurrent filters and non-rectangular iteration spaces), perform new complex loop nest transformations (such as wavefront parallelization, loop shifting and loop fusion) and generate efficient code for more architectures (such as combinations of distributed clusters, multicores, GPUs and FPGAs). Finally, we demonstrate that Tiramisu can generate very efficient code that matches the highly optimized Intel MKL gemm (generalized matrix multiplication) implementation, we also show speedups reaching 4X in Halide and 16X in Julia due to optimizations enabled by Tiramisu.',\n",
       " 'This paper shows how to optimize sparse tensor algebraic expressions by introducing temporary tensors, called workspaces, into the resulting loop nests. We develop a new intermediate language for tensor operations called concrete index notation that extends tensor index notation. Concrete index notation expresses when and where sub-computations occur and what tensor they are stored into. We then describe the workspace optimization in this language, and how to compile it to sparse code by building on prior work in the literature.\\n  We demonstrate the importance of the optimization on several important sparse tensor kernels, including sparse matrix-matrix multiplication (SpMM), sparse tensor addition (SpAdd), and the matricized tensor times Khatri-Rao product (MTTKRP) used to factorize tensors. Our results show improvements over prior work on tensor algebra compilation and brings the performance of these kernels on par with state-of-the-art hand-optimized implementations. For example, SpMM was not supported by prior tensor algebra compilers, the performance of MTTKRP on the nell-2 data set improves by 35%, and MTTKRP can for the first time have sparse results.',\n",
       " 'Data analytics applications combine multiple functions from different libraries and frameworks. Even when each function is optimized in isolation, the performance of the combined application can be an order of magnitude below hardware limits due to extensive data movement across these functions. To address this problem, we propose Weld, a new interface between data-intensive libraries that can optimize across disjoint libraries and functions. Weld exposes a lazily-evaluated API where diverse functions can submit their computations in a simple but general intermediate representation that captures their data-parallel structure. It then optimizes data movement across these functions and emits efficient code for diverse hardware. Weld can be integrated into existing frameworks such as Spark, TensorFlow, Pandas and NumPy without changing their user-facing APIs. We demonstrate that Weld can speed up applications using these frameworks by up to 29x.',\n",
       " 'Modern hardware systems are heavily underutilized when running large-scale graph applications. While many in-memory graph frameworks have made substantial progress in optimizing these applications, we show that it is still possible to achieve up to 4 $\\\\times$ speedups over the fastest frameworks by greatly improving cache utilization. Previous systems have applied out-of-core processing techniques from the memory/disk boundary to the cache/DRAM boundary. However, we find that blindly applying such techniques is ineffective because of the much smaller performance gap between DRAM and cache. We present two techniques that take advantage of the cache with minimal or no instruction overhead. The first, frequency based clustering, groups together frequently accessed vertices to improve the utilization of each cache line with no runtime overhead. The second, CSR segmenting, partitions the graph to restrict all random accesses to the cache, makes all DRAM access sequential, and merges partition results using a very low overhead cache-aware merge. Both techniques can be easily implemented on top of optimized graph frameworks. Our techniques combined give speedups of up to 4 $\\\\times$ for PageRank, Label Propagation and Collaborative Filtering, and 2 $\\\\times$ for Betweenness Centrality over the best published results',\n",
       " \"Large, human-annotated datasets are central to the development of natural language processing models. Collecting these datasets can be the most challenging part of the development process. We address this problem by introducing a general purpose technique for ``simulation-to-real'' transfer in language understanding problems with a delimited set of target behaviors, making it possible to develop models that can interpret natural utterances without natural training data. We begin with a synthetic data generation procedure, and train a model that can accurately interpret utterances produced by the data generator. To generalize to natural utterances, we automatically find projections of natural language utterances onto the support of the synthetic language, using learned sentence embeddings to define a distance metric. With only synthetic training data, our approach matches or outperforms state-of-the-art models trained on natural language data in several domains. These results suggest that simulation-to-real transfer is a practical framework for developing NLP applications, and that improved models for transfer might provide wide-ranging improvements in downstream tasks.\",\n",
       " 'Electric circuits are usually described by charge- and flux-oriented modified nodal analysis. In this paper, we derive models as port-Hamiltonian systems on several levels: overall systems, multiply coupled systems and systems within dynamic iteration procedures. To this end, we introduce new classes of port-Hamiltonian differential-algebraic equations. Thereby, we additionally allow for nonlinear dissipation on a subspace of the state space. Both, each subsystem and the overall system, possess a port-Hamiltonian structure. A structural analysis is performed for the new setups. Dynamic iteration schemes are investigated and we show that the Jacobi approach as well as an adapted Gauss-Seidel approach lead to port-Hamiltonian differential-algebraic equations.',\n",
       " 'A new method to enclose the pseudospectrum via the numerical range of the inverse of a matrix or linear operator is presented. The method is applied to finite-dimensional discretizations of an operator on an infinite-dimensional Hilbert space, and convergence results for different approximation schemes are obtained, including finite element methods. We show that the pseudospectrum of the full operator is contained in an intersection of sets which are expressed in terms of the numerical ranges of shifted inverses of the approximating matrices. The results are illustrated by means of two examples: the advection-diffusion operator and the Hain-LÃ¼st operator.',\n",
       " \"Successful linguistic communication relies on a shared experience of the world, and it is this shared experience that makes utterances meaningful. Despite the incredible effectiveness of language processing models trained on text alone, today's best systems still make mistakes that arise from a failure to relate language to the physical world it describes and to the social interactions it facilitates.\\n  Natural Language Processing is a diverse field, and progress throughout its development has come from new representational theories, modeling techniques, data collection paradigms, and tasks. We posit that the present success of representation learning approaches trained on large text corpora can be deeply enriched from the parallel tradition of research on the contextual and social nature of language.\\n  In this article, we consider work on the contextual foundations of language: grounding, embodiment, and social interaction. We describe a brief history and possible progression of how contextual information can factor into our representations, with an eye towards how this integration can move the field forward and where it is currently being pioneered. We believe this framing will serve as a roadmap for truly contextual language understanding.\",\n",
       " 'Human language users easily interpret expressions that describe unfamiliar situations composed from familiar parts (\"greet the pink brontosaurus by the ferris wheel\"). Modern neural networks, by contrast, struggle to interpret compositions unseen in training. In this paper, we introduce a new benchmark, gSCAN, for evaluating compositional generalization in models of situated language understanding. We take inspiration from standard models of meaning composition in formal linguistics. Going beyond an earlier related benchmark that focused on syntactic aspects of generalization, gSCAN defines a language grounded in the states of a grid world. This allows us to build novel generalization tasks that probe the acquisition of linguistically motivated rules. For example, agents must understand how adjectives such as \\'small\\' are interpreted relative to the current world state or how adverbs such as \\'cautiously\\' combine with new verbs. We test a strong multi-modal baseline model and a state-of-the-art compositional method finding that, in most cases, they fail dramatically when generalization requires systematic compositional rules.',\n",
       " 'The quadratic numerical range $W^2(A)$ is a subset of the standard numerical range of a linear operator which still contains its spectrum. It arises naturally in operators which have a $2 \\\\times 2$ block structure, and it consists of at most two connected components, none of which necessarily convex. The quadratic numerical range can thus reveal spectral gaps, and it can in particular indicate that the spectrum of an operator is bounded away from $0$.\\n  We exploit this property in the finite-dimensional setting to derive Krylov subspace type methods to solve the system $Ax = b$, in which the iterates arise as solutions of low-dimensional models of the operator whose quadratic numerical ranges is contained in $W^2(A)$. This implies that the iterates are always well-defined and that, as opposed to standard FOM, large variations in the approximation quality of consecutive iterates are avoided, although $0$ lies within the convex hull of the spectrum. We also consider GMRES variants which are obtained in a similar spirit. We derive theoretical results on basic properties of these methods, review methods on how to compute the required bases in a stable manner and present results of several numerical experiments illustrating improvements over standard FOM and GMRES.',\n",
       " \"Despite impressive performance on numerous visual tasks, Convolutional Neural Networks (CNNs) --- unlike brains --- are often highly sensitive to small perturbations of their input, e.g. adversarial noise leading to erroneous decisions. We propose to regularize CNNs using large-scale neuroscience data to learn more robust neural features in terms of representational similarity. We presented natural images to mice and measured the responses of thousands of neurons from cortical visual areas. Next, we denoised the notoriously variable neural activity using strong predictive models trained on this large corpus of responses from the mouse visual system, and calculated the representational similarity for millions of pairs of images from the model's predictions. We then used the neural representation similarity to regularize CNNs trained on image classification by penalizing intermediate representations that deviated from neural ones. This preserved performance of baseline models when classifying images under standard benchmarks, while maintaining substantially higher performance compared to baseline or control models when classifying noisy images. Moreover, the models regularized with cortical representations also improved model robustness in terms of adversarial attacks. This demonstrates that regularizing with neural data can be an effective tool to create an inductive bias towards more robust inference.\",\n",
       " 'On 2019 August 14, the Advanced LIGO and Virgo interferometers detected the high-significance gravitational wave (GW) signal S190814bv. The GW data indicated that the event resulted from a neutron star--black hole (NSBH) merger, or potentially a low-mass binary black hole merger. Due to the low false alarm rate and the precise localization (23 deg$^2$ at 90\\\\%), S190814bv presented the community with the best opportunity yet to directly observe an optical/near-infrared counterpart to a NSBH merger. To search for potential counterparts, the GROWTH collaboration performed real-time image subtraction on 6 nights of public Dark Energy Camera (DECam) images acquired in the three weeks following the merger, covering $>$98\\\\% of the localization probability. Using a worldwide network of follow-up facilities, we systematically undertook spectroscopy and imaging of optical counterpart candidates. Combining these data with a photometric redshift catalog, we ruled out each candidate as the counterpart to S190814bv and we placed deep, uniform limits on the optical emission associated with S190814bv. For the nearest consistent GW distance, radiative transfer simulations of NSBH mergers constrain the ejecta mass of S190814bv to be $M_\\\\mathrm{ej} < 0.04$~$M_{\\\\odot}$ at polar viewing angles, or $M_\\\\mathrm{ej} < 0.03$~$M_{\\\\odot}$ if the opacity is $Îº< 2$~cm$^2$g$^{-1}$. Assuming a tidal deformability for the neutron star at the high end of the range compatible with GW170817 results, our limits would constrain the BH spin component aligned with the orbital momentum to be $ Ï‡< 0.7$ for mass ratios $Q < 6$, with weaker constraints for more compact neutron stars. We publicly release the photometry from this campaign at http://www.astro.caltech.edu/~danny/static/s190814bv.',\n",
       " \"The question of whether a singularity can form in an initially regular flow, described by the 3D incompressible Navier-Stokes (NS) equations, is a fundamental problem in mathematical physics. The NS regularity problem is super-critical, i.e., there is a 'scaling gap' between what can be established by mathematical analysis and what is needed to rule out a singularity. A recently introduced mathematical framework--based on a suitably defined `scale of sparseness' of the regions of intense vorticity--brought the first scaling reduction of the NS super-criticality since the 1960s. Here, we put this framework to the first numerical test using a spatially highly resolved computational simulation performed near a 'burst' of the vorticity magnitude. The results confirm that the scale is well suited to detect the onset of dissipation and provide strong numerical evidence that ongoing mathematical efforts may succeed in closing the scaling gap.\",\n",
       " 'SciPy is an open source scientific computing library for the Python programming language. SciPy 1.0 was released in late 2017, about 16 years after the original version 0.1 release. SciPy has become a de facto standard for leveraging scientific algorithms in the Python programming language, with more than 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories, and millions of downloads per year. This includes usage of SciPy in almost half of all machine learning projects on GitHub, and usage by high profile projects including LIGO gravitational wave analysis and creation of the first-ever image of a black hole (M87). The library includes functionality spanning clustering, Fourier transforms, integration, interpolation, file I/O, linear algebra, image processing, orthogonal distance regression, minimization algorithms, signal processing, sparse matrix handling, computational geometry, and statistics. In this work, we provide an overview of the capabilities and development practices of the SciPy library and highlight some recent technical developments.',\n",
       " 'The All-sky Medium Energy Gamma-ray Observatory (AMEGO) is a probe class mission concept that will provide essential contributions to multimessenger astrophysics in the late 2020s and beyond. AMEGO combines high sensitivity in the 200 keV to 10 GeV energy range with a wide field of view, good spectral resolution, and polarization sensitivity. Therefore, AMEGO is key in the study of multimessenger astrophysical objects that have unique signatures in the gamma-ray regime, such as neutron star mergers, supernovae, and flaring active galactic nuclei. The order-of-magnitude improvement compared to previous MeV missions also enables discoveries of a wide range of phenomena whose energy output peaks in the relatively unexplored medium-energy gamma-ray band.',\n",
       " 'To be successful in real-world tasks, Reinforcement Learning (RL) needs to exploit the compositional, relational, and hierarchical structure of the world, and learn to transfer it to the task at hand. Recent advances in representation learning for language make it possible to build models that acquire world knowledge from text corpora and integrate this knowledge into downstream decision making problems. We thus argue that the time is right to investigate a tight integration of natural language understanding into RL in particular. We survey the state of the field, including work on instruction following, text games, and learning from textual domain knowledge. Finally, we call for the development of new environments as well as further investigation into the potential uses of recent Natural Language Processing (NLP) techniques for such tasks.',\n",
       " 'We propose a simple data augmentation protocol aimed at providing a compositional inductive bias in conditional and unconditional sequence models. Under this protocol, synthetic training examples are constructed by taking real training examples and replacing (possibly discontinuous) fragments with other fragments that appear in at least one similar environment. The protocol is model-agnostic and useful for a variety of tasks. Applied to neural sequence-to-sequence models, it reduces error rate by as much as 87% on diagnostic tasks from the SCAN dataset and 16% on a semantic parsing task. Applied to n-gram language models, it reduces perplexity by roughly 1% on small corpora in several languages.',\n",
       " 'We improve the informativeness of models for conditional text generation using techniques from computational pragmatics. These techniques formulate language production as a game between speakers and listeners, in which a speaker should generate output text that a listener can use to correctly identify the original input that the text describes. While such approaches are widely used in cognitive science and grounded language learning, they have received less attention for more standard language generation tasks. We consider two pragmatic modeling methods for text generation: one where pragmatics is imposed by information preservation, and another where pragmatics is imposed by explicit modeling of distractors. We find that these methods improve the performance of strong existing systems for abstractive summarization and generation from structured meaning representations.',\n",
       " 'Since the 2017 Nobel Prize in Physics was awarded for the observation of gravitational waves, it is fair to say that the epoch of gravitational wave astronomy (GWs) has begun. However, a number of interesting sources of GWs can only be observed from space. To demonstrate the feasibility of the Laser Interferometer Space Antenna (LISA), a future gravitational wave observatory in space, the LISA Pathfinder satellite was launched on December, 3rd 2015. Measurements of the spurious forces accelerating an otherwise free-falling test mass, and detailed investigations of the individual subsystems needed to achieve the free-fall, have been conducted throughout the mission. This overview article starts with the purpose and aim of the mission, explains satellite hardware and mission operations and ends with a summary of selected important results and an outlook towards LISA. From the LISA Pathfinder experience, we can conclude that the proposed LISA mission is feasible.',\n",
       " \"Many machine learning algorithms represent input data with vector embeddings or discrete codes. When inputs exhibit compositional structure (e.g. objects built from parts or procedures from subroutines), it is natural to ask whether this compositional structure is reflected in the the inputs' learned representations. While the assessment of compositionality in languages has received significant attention in linguistics and adjacent fields, the machine learning literature lacks general-purpose tools for producing graded measurements of compositional structure in more general (e.g. vector-valued) representation spaces. We describe a procedure for evaluating compositionality by measuring how well the true representation-producing model can be approximated by a model that explicitly composes a collection of inferred representational primitives. We use the procedure to provide formal and empirical characterizations of compositional structure in a variety of settings, exploring the relationship between compositionality and learning dynamics, human judgments, representational similarity, and generalization.\",\n",
       " 'This paper presents a multilevel convergence framework for multigrid-reduction-in-time (MGRIT) as a generalization of previous two-grid estimates. The framework provides a priori upper bounds on the convergence of MGRIT V- and F-cycles, with different relaxation schemes, by deriving the respective residual and error propagation operators. The residual and error operators are functions of the time stepping operator, analyzed directly and bounded in norm, both numerically and analytically. We present various upper bounds of different computational cost and varying sharpness. These upper bounds are complemented by proposing analytic formulae for the approximate convergence factor of V-cycle algorithms that take the number of fine grid time points, the temporal coarsening factors, and the eigenvalues of the time stepping operator as parameters.\\n  The paper concludes with supporting numerical investigations of parabolic (anisotropic diffusion) and hyperbolic (wave equation) model problems. We assess the sharpness of the bounds and the quality of the approximate convergence factors. Observations from these numerical investigations demonstrate the value of the proposed multilevel convergence framework for estimating MGRIT convergence a priori and for the design of a convergent algorithm. We further highlight that observations in the literature are captured by the theory, including that two-level Parareal and multilevel MGRIT with F-relaxation do not yield scalable algorithms and the benefit of a stronger relaxation scheme. An important observation is that with increasing numbers of levels MGRIT convergence deteriorates for the hyperbolic model problem, while constant convergence factors can be achieved for the diffusion equation. The theory also indicates that L-stable Runge-Kutta schemes are more amendable to multilevel parallel-in-time integration with MGRIT than A-stable Runge-Kutta schemes.',\n",
       " 'The science operations of the LISA Pathfinder mission has demonstrated the feasibility of sub-femto-g free-fall of macroscopic test masses necessary to build a LISA-like gravitational wave observatory in space. While the main focus of interest, i.e. the optical axis or the $x$-axis, has been extensively studied, it is also of interest to evaluate the stability of the spacecraft with respect to all the other degrees of freedom. The current paper is dedicated to such a study, with a focus set on an exhaustive and quantitative evaluation of the imperfections and dynamical effects that impact the stability with respect to its local geodesic. A model of the complete closed-loop system provides a comprehensive understanding of each part of the in-loop coordinates spectra. As will be presented, this model gives very good agreements with LISA Pathfinder flight data. It allows one to identify the physical noise source at the origin and the physical phenomena underlying the couplings. From this, the performances of the stability of the spacecraft, with respect to its geodesic, are extracted as a function of frequency. Close to $1 mHz$, the stability of the spacecraft on the $X_{SC}$, $Y_{SC}$ and $Z_{SC}$ degrees of freedom is shown to be of the order of $5.0\\\\ 10^{-15} m\\\\ s^{-2}/\\\\sqrt{Hz}$ for X and $4.0 \\\\ 10^{-14} m\\\\ s^{-2}/\\\\sqrt{Hz}$ for Y and Z. For the angular degrees of freedom, the values are of the order $3\\\\ 10^{-12} rad\\\\ s^{-2}/\\\\sqrt{Hz}$ for $Î˜_{SC}$ and $3\\\\ 10^{-13} rad\\\\ s^{-2}/\\\\sqrt{Hz}$ for $H_{SC}$ and $Î¦_{SC}$.',\n",
       " 'Behavioral skills or policies for autonomous agents are conventionally learned from reward functions, via reinforcement learning, or from demonstrations, via imitation learning. However, both modes of task specification have their disadvantages: reward functions require manual engineering, while demonstrations require a human expert to be able to actually perform the task in order to generate the demonstration. Instruction following from natural language instructions provides an appealing alternative: in the same way that we can specify goals to other humans simply by speaking or writing, we would like to be able to specify tasks for our machines. However, a single instruction may be insufficient to fully communicate our intent or, even if it is, may be insufficient for an autonomous agent to actually understand how to perform the desired task. In this work, we propose an interactive formulation of the task specification problem, where iterative language corrections are provided to an autonomous agent, guiding it in acquiring the desired skill. Our proposed language-guided policy learning algorithm can integrate an instruction and a sequence of corrections to acquire new skills very quickly. In our experiments, we show that this method can enable a policy to follow instructions and corrections for simulated navigation and manipulation tasks, substantially outperforming direct, non-interactive instruction following.',\n",
       " 'Episodic accretion may be a common occurrence in the evolution of young pre-main sequence stars and has important implications for our understanding of star and planet formation. Many fundamental aspects of what drives the accretion physics, however, are still unknown. The ngVLA will be a key tool in understanding the nature of these events. The high spatial resolution, broad spectral coverage, and unprecedented sensitivity will allow for the detailed analysis of outburst systems. The proposed frequency range of the ngVLA allows for observations of the gas, dust, and non-thermal emission from the star and disk.',\n",
       " 'Graphene is an ideal material to study fundamental Coulomb- and phonon-induced carrier scattering processes. Its remarkable gapless and linear band structure opens up new carrier relaxation channels. In particular, Auger scattering bridging the valence and the conduction band changes the number of charge carriers and gives rise to a significant carrier multiplication - an ultrafast many-particle phenomenon that is promising for the design of highly efficient photodetectors. Furthermore, the vanishing density of states at the Dirac point combined with ultrafast phonon-induced intraband scattering results in an accumulation of carriers and a population inversion suggesting the design of graphene-based terahertz lasers. Here, we review our work on the ultrafast carrier dynamics in graphene and Landau-quantized graphene is presented providing a microscopic view on the appearance of carrier multiplication and population inversion.',\n",
       " \"Classical models describe primary visual cortex (V1) as a filter bank of orientation-selective linear-nonlinear (LN) or energy models, but these models fail to predict neural responses to natural stimuli accurately. Recent work shows that models based on convolutional neural networks (CNNs) lead to much more accurate predictions, but it remains unclear which features are extracted by V1 neurons beyond orientation selectivity and phase invariance. Here we work towards systematically studying V1 computations by categorizing neurons into groups that perform similar computations. We present a framework to identify common features independent of individual neurons' orientation selectivity by using a rotation-equivariant convolutional neural network, which automatically extracts every feature at multiple different orientations. We fit this model to responses of a population of 6000 neurons to natural images recorded in mouse primary visual cortex using two-photon imaging. We show that our rotation-equivariant network not only outperforms a regular CNN with the same number of feature maps, but also reveals a number of common features shared by many V1 neurons, which deviate from the typical textbook idea of V1 as a bank of Gabor filters. Our findings are a first step towards a powerful new tool to study the nonlinear computations in V1.\",\n",
       " 'Effects of heavy sea quarks on the low energy physics are described by an effective theory where the expansion parameter is the inverse quark mass, 1/$M$. At leading order in 1/$M$ (and neglecting light quark masses) the dependence of any low energy quantity on $M$ is given in terms of the ratio of $Î›$ parameters of the effective and the fundamental theory. We define a function describing the scaling with the mass $M$. We find that its perturbative expansion is very reliable for the bottom quark and also seems to work very well at the charm quark mass. The same is then true for the ratios of $Î›^{(4)}/Î›^{(5)}$ and $Î›^{(3)}/Î›^{(4)}$, which play a major rÃ´le in connecting lattice determinations of $Î±^{(3)}_{MSbar}$ from the three-flavor theory with $Î±^{(5)}_{MSbar}(M_Z)$. Also the charm quark content of the nucleon, relevant for dark matter searches, can be computed accurately from perturbation theory.\\n  We investigate a very closely related model, namely QCD with $N_f=2$ heavy quarks. Our non-perturbative information is derived from simulations on the lattice, with masses up to the charm quark mass and lattice spacings down to about 0.023 fm followed by a continuum extrapolation. The non-perturbative mass dependence agrees within rather small errors with the perturbative prediction at masses around the charm quark mass. Surprisingly, from studying solely the massive theory we can make a prediction for the ratio $Q^{1/\\\\sqrt{t_0}}_{0,2}=[Î›\\\\sqrt{t_0(0)}]_{N_f=2}/[Î›\\\\sqrt{t_0}]_{N_f=0}$, which refers to the chiral limit in $N_f=2$. Here $t_0$ is the Gradient Flow scale of [1]. The uncertainty for $Q$ is estimated to be 2.5%. For the phenomenologically interesting $Î›^{(3)}/Î›^{(4)}$, we conclude that perturbation theory introduces errors which are at most at the 1.5% level, far smaller than other current uncertainties.',\n",
       " 'Objective: To (1) demonstrate the implementation of a data science platform built on open-source technology within a large, academic healthcare system and (2) describe two computational healthcare applications built on such a platform. Materials and Methods: A data science platform based on several open source technologies was deployed to support real-time, big data workloads. Data acquisition workflows for Apache Storm and NiFi were developed in Java and Python to capture patient monitoring and laboratory data for downstream analytics. Results: The use of emerging data management approaches along with open-source technologies such as Hadoop can be used to create integrated data lakes to store large, real-time data sets. This infrastructure also provides a robust analytics platform where healthcare and biomedical research data can be analyzed in near real-time for precision medicine and computational healthcare use cases. Discussion: The implementation and use of integrated data science platforms offer organizations the opportunity to combine traditional data sets, including data from the electronic health record, with emerging big data sources, such as continuous patient monitoring and real-time laboratory results. These platforms can enable cost-effective and scalable analytics for the information that will be key to the delivery of precision medicine initiatives. Conclusion: Organizations that can take advantage of the technical advances found in data science platforms will have the opportunity to provide comprehensive access to healthcare data for computational healthcare and precision medicine research.',\n",
       " \"In complex inferential tasks like question answering, machine learning models must confront two challenges: the need to implement a compositional reasoning process, and, in many applications, the need for this reasoning process to be interpretable to assist users in both development and prediction. Existing models designed to produce interpretable traces of their decision-making process typically require these traces to be supervised at training time. In this paper, we present a novel neural modular approach that performs compositional reasoning by automatically inducing a desired sub-task decomposition without relying on strong supervision. Our model allows linking different reasoning tasks though shared modules that handle common routines across tasks. Experiments show that the model is more interpretable to human evaluators compared to other state-of-the-art models: users can better understand the model's underlying reasoning procedure and predict when it will succeed or fail based on observing its intermediate outputs.\",\n",
       " 'Calcium imaging has revolutionized systems neuroscience, providing the ability to image large neural populations with single-cell resolution. The resulting datasets are quite large, which has presented a barrier to routine open sharing of this data, slowing progress in reproducible research. State of the art methods for analyzing this data are based on non-negative matrix factorization (NMF); these approaches solve a non-convex optimization problem, and are effective when good initializations are available, but can break down in low-SNR settings where common initialization approaches fail. Here we introduce an approach to compressing and denoising functional imaging data. The method is based on a spatially-localized penalized matrix decomposition (PMD) of the data to separate (low-dimensional) signal from (temporally-uncorrelated) noise. This approach can be applied in parallel on local spatial patches and is therefore highly scalable, does not impose non-negativity constraints or require stringent identifiability assumptions (leading to significantly more robust results compared to NMF), and estimates all parameters directly from the data, so no hand-tuning is required. We have applied the method to a wide range of functional imaging data (including one-photon, two-photon, three-photon, widefield, somatic, axonal, dendritic, calcium, and voltage imaging datasets): in all cases, we observe ~2-4x increases in SNR and compression rates of 20-300x with minimal visible loss of signal, with no adjustment of hyperparameters; this in turn facilitates the process of demixing the observed activity into contributions from individual neurons. We focus on two challenging applications: dendritic calcium imaging data and voltage imaging data in the context of optogenetic stimulation. In both cases, we show that our new approach leads to faster and much more robust extraction of activity from the data.',\n",
       " 'Navigation guided by natural language instructions presents a challenging reasoning problem for instruction followers. Natural language instructions typically identify only a few high-level decisions and landmarks rather than complete low-level motor behaviors; much of the missing information must be inferred based on perceptual context. In machine learning settings, this is doubly challenging: it is difficult to collect enough annotated data to enable learning of this reasoning process from scratch, and also difficult to implement the reasoning process using generic sequence models. Here we describe an approach to vision-and-language navigation that addresses both these issues with an embedded speaker model. We use this speaker model to (1) synthesize new instructions for data augmentation and to (2) implement pragmatic reasoning, which evaluates how well candidate action sequences explain an instruction. Both steps are supported by a panoramic action space that reflects the granularity of human-generated instructions. Experiments show that all three components of this approach---speaker-driven data augmentation, pragmatic reasoning and panoramic action space---dramatically improve the performance of a baseline instruction follower, more than doubling the success rate over the best existing approach on a standard benchmark.',\n",
       " 'MAROON-X is a red-optical, high precision radial velocity spectrograph currently nearing completion and undergoing extensive performance testing at the University of Chicago. The instrument is scheduled to be installed at Gemini North in the first quarter of 2019. MAROON-X will be the only RV spectrograph on a large telescope with full access by the entire US community. In these proceedings we discuss the latest addition of the red wavelength arm and the two science grade detector systems, as well as the design and construction of the telescope front end. We also present results from ongoing RV stability tests in the lab. First results indicate that MAROON-X can be calibrated at the sub-m/s level, and perhaps even much better than that using a simultaneous reference approach.',\n",
       " 'Particle physics has an ambitious and broad experimental programme for the coming decades. This programme requires large investments in detector hardware, either to build new facilities and experiments, or to upgrade existing ones. Similarly, it requires commensurate investment in the R&D of software to acquire, manage, process, and analyse the shear amounts of data to be recorded. In planning for the HL-LHC in particular, it is critical that all of the collaborating stakeholders agree on the software goals and priorities, and that the efforts complement each other. In this spirit, this white paper describes the R&D activities required to prepare for this software upgrade.',\n",
       " 'We show that explicit pragmatic inference aids in correctly generating and following natural language instructions for complex, sequential tasks. Our pragmatics-enabled models reason about why speakers produce certain instructions, and about how listeners will react upon hearing them. Like previous pragmatic models, we use learned base listener and speaker models to build a pragmatic speaker that uses the base listener to simulate the interpretation of candidate descriptions, and a pragmatic listener that reasons counterfactually about alternative descriptions. We extend these models to tasks with sequential structure. Evaluation of language generation and interpretation shows that pragmatic inference improves state-of-the-art listener models (at correctly interpreting human instructions) and speaker models (at producing instructions correctly interpreted by humans) in diverse settings.',\n",
       " 'Deep reinforcement learning has achieved many recent successes, but our understanding of its strengths and limitations is hampered by the lack of rich environments in which we can fully characterize optimal behavior, and correspondingly diagnose individual actions against such a characterization. Here we consider a family of combinatorial games, arising from work of Erdos, Selfridge, and Spencer, and we propose their use as environments for evaluating and comparing different approaches to reinforcement learning. These games have a number of appealing features: they are challenging for current learning approaches, but they form (i) a low-dimensional, simply parametrized environment where (ii) there is a linear closed form solution for optimal behavior from any state, and (iii) the difficulty of the game can be tuned by changing environment parameters in an interpretable way. We use these Erdos-Selfridge-Spencer games not only to compare different algorithms, but test for generalization, make comparisons to supervised learning, analyse multiagent play, and even develop a self play algorithm. Code can be found at: https://github.com/rubai5/ESS_Game',\n",
       " \"The named concepts and compositional operators present in natural language provide a rich source of information about the kinds of abstractions humans use to navigate the world. Can this linguistic background knowledge improve the generality and efficiency of learned classifiers and control policies? This paper aims to show that using the space of natural language strings as a parameter space is an effective way to capture natural task structure. In a pretraining phase, we learn a language interpretation model that transforms inputs (e.g. images) into outputs (e.g. labels) given natural language descriptions. To learn a new concept (e.g. a classifier), we search directly in the space of descriptions to minimize the interpreter's loss on training examples. Crucially, our models do not require language data to learn these concepts: language is used only in pretraining to impose structure on subsequent learning. Results on image classification, text editing, and reinforcement learning show that, in all settings, models with a linguistic parameterization outperform those without.\",\n",
       " 'Results from a search for neutrinoless double-beta decay $0Î½Î²Î²$ of $^{136}$Xe are presented using the first year of data taken with the upgraded EXO-200 detector. Relative to previous searches by EXO-200, the energy resolution of the detector has been improved to $Ïƒ/E$=1.23%, the electric field in the drift region has been raised by 50%, and a system to suppress radon in the volume between the cryostat and lead shielding has been implemented. In addition, analysis techniques that improve topological discrimination between $0Î½Î²Î²$ and background events have been developed. Incorporating these hardware and analysis improvements, the median 90% confidence level $0Î½Î²Î²$ half-life sensitivity after combining with the full data set acquired before the upgrade has increased 2-fold to $3.7 \\\\times 10^{25}$ yr. No statistically significant evidence for $0Î½Î²Î²$ is observed, leading to a lower limit on the $0Î½Î²Î²$ half-life of $1.8\\\\times10^{25}$ yr at the 90% confidence level.',\n",
       " 'We investigate the compositional structure of message vectors computed by a deep network trained on a communication game. By comparing truth-conditional representations of encoder-produced message vectors to human-produced referring expressions, we are able to identify aligned (vector, utterance) pairs with the same meaning. We then search for structured relationships among these aligned pairs to discover simple vector space transformations corresponding to negation, conjunction, and disjunction. Our results suggest that neural representations are capable of spontaneously developing a \"syntax\" with functional analogues to qualitative properties of natural language.',\n",
       " 'In this paper, we develop a system for the low-cost indoor localization and tracking problem using radio signal strength indicator, Inertial Measurement Unit (IMU), and magnetometer sensors. We develop a novel and simplified probabilistic IMU motion model as the proposal distribution of the sequential Monte-Carlo technique to track the robot trajectory. Our algorithm can globally localize and track a robot with a priori unknown location, given an informative prior map of the Bluetooth Low Energy (BLE) beacons. Also, we formulate the problem as an optimization problem that serves as the Back-end of the algorithm mentioned above (Front-end). Thus, by simultaneously solving for the robot trajectory and the map of BLE beacons, we recover a continuous and smooth trajectory of the robot, corrected locations of the BLE beacons, and the time-varying IMU bias. The evaluations achieved using hardware show that through the proposed closed-loop system the localization performance can be improved; furthermore, the system becomes robust to the error in the map of beacons by feeding back the optimized map to the Front-end.',\n",
       " 'In this work, we present a minimal neural model for constituency parsing based on independent scoring of labels and spans. We show that this model is not only compatible with classical dynamic programming techniques, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input. We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring model are capable of achieving state-of-the-art single-model performance on the Penn Treebank (91.79 F1) and strong performance on the French Treebank (82.23 F1).',\n",
       " \"Several approaches have recently been proposed for learning decentralized deep multiagent policies that coordinate via a differentiable communication channel. While these policies are effective for many tasks, interpretation of their induced communication strategies has remained a challenge. Here we propose to interpret agents' messages by translating them. Unlike in typical machine translation problems, we have no parallel data to learn from. Instead we develop a translation model based on the insight that agent messages and natural language strings mean the same thing if they induce the same belief about the world in a listener. We present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmatics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward relative to players with a common language.\",\n",
       " 'Natural language questions are inherently compositional, and many are most easily answered by reasoning about their decomposition into modular sub-problems. For example, to answer \"is there an equal number of balls and boxes?\" we can look for balls, look for boxes, count them, and compare the results. The recently proposed Neural Module Network (NMN) architecture implements this approach to question answering by parsing questions into linguistic substructures and assembling question-specific deep networks from smaller modules that each solve one subtask. However, existing NMN implementations rely on brittle off-the-shelf parsers, and are restricted to the module configurations proposed by these parsers rather than learning them from data. In this paper, we propose End-to-End Module Networks (N2NMNs), which learn to reason by directly predicting instance-specific network layouts without the aid of a parser. Our model learns to generate network structures (by imitating expert demonstrations) while simultaneously learning network parameters (using the downstream task loss). Experimental results on the new CLEVR dataset targeted at compositional question answering show that N2NMNs achieve an error reduction of nearly 50% relative to state-of-the-art attentional approaches, while discovering interpretable network architectures specialized for each question.',\n",
       " 'Searches for double beta decay of $^{134}$Xe were performed with EXO-200, a single-phase liquid xenon detector designed to search for neutrinoless double beta decay of $^{136}$Xe. Using an exposure of $29.6\\\\text{ kg}\\\\!\\\\cdot\\\\!\\\\text{yr}$, the lower limits of $\\\\text{T}_{1/2}^{2Î½Î²\\\\!Î²}>8.7\\\\cdot10^{20}\\\\text{ yr}$ and $\\\\text{T}_{1/2}^{0Î½Î²\\\\!Î²}>1.1\\\\cdot10^{23}\\\\text{ yr}$ at 90% confidence level were derived, with corresponding half-life sensitivities of $1.2\\\\cdot10^{21}\\\\text{ yr}$ and $1.9\\\\cdot10^{23}\\\\text{ yr}$. These limits exceed those in the literature for $^{134}$Xe, improving by factors of nearly $10^{5}$ and 2 for the two antineutrino and neutrinoless modes, respectively.',\n",
       " \"We report results from a systematic measurement campaign conducted to identify low radioactivity materials for the construction of the EXO-200 double beta decay experiment. Partial results from this campaign have already been reported in a 2008 paper by the EXO collaboration. Here we release the remaining data, collected since 2007, to the public. The data reported were obtained using a variety of analytic techniques. The measurement sensitivities are among the best in the field. Construction of the EXO-200 detector has been concluded, and Phase-I data was taken from 2011 to 2014. The detector's extremely low background implicitly verifies the measurements and the analysis assumptions made during construction and reported in this paper.\",\n",
       " 'The potential impact of future quantum networks hinges on high-quality quantum entanglement shared between network nodes. Unavoidable real-world imperfections necessitate means to improve remote entanglement by local quantum operations. Here we realize entanglement distillation on a quantum network primitive of distant electron-nuclear two-qubit nodes. We demonstrate the heralded generation of two copies of a remote entangled state through single-photon-mediated entangling of the electrons and robust storage in the nuclear spins. After applying local two-qubit gates, single-shot measurements herald the distillation of an entangled state with increased fidelity that is available for further use. In addition, this distillation protocol significantly speeds up entanglement generation compared to previous two-photon-mediated schemes. The key combination of generating, storing and processing entangled states demonstrated here opens the door to exploring and utilizing multi-particle entanglement on an extended quantum network.',\n",
       " 'People often refer to entities in an image in terms of their relationships with other entities. For example, \"the black cat sitting under the table\" refers to both a \"black cat\" entity and its relationship with another \"table\" entity. Understanding these relationships is essential for interpreting and grounding such natural language expressions. Most prior work focuses on either grounding entire referential expressions holistically to one region, or localizing relationships based on a fixed set of categories. In this paper we instead present a modular deep architecture capable of analyzing referential expressions into their component parts, identifying entities and relationships mentioned in the input expression and grounding them all in the scene. We call this approach Compositional Modular Networks (CMNs): a novel architecture that learns linguistic analysis and visual inference end-to-end. Our approach is built around two types of neural modules that inspect local regions and pairwise interactions between regions. We evaluate CMNs on multiple referential expression datasets, outperforming state-of-the-art approaches on all tasks.',\n",
       " 'We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate tasks with sequences of named subtasks, providing information about high-level structural relationships among tasks but not how to implement them---specifically not providing the detailed guidance used by much previous work on learning policy abstractions for RL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations). To learn from sketches, we present a model that associates every subtask with a modular subpolicy, and jointly maximizes reward over full task-specific policies by tying parameters across shared subpolicies. Optimization is accomplished via a decoupled actor--critic training objective that facilitates learning common behaviors from multiple dissimilar reward functions. We evaluate the effectiveness of our approach in three environments featuring both discrete and continuous control, and with sparse rewards that can be obtained only after completing a number of high-level subgoals. Experiments show that using our approach to learn policies guided by sketches gives better performance than existing techniques for learning task-specific or shared policies, while naturally inducing a library of interpretable primitive behaviors that can be recombined to rapidly adapt to new tasks.',\n",
       " 'We present the Adaptive Aggregation-based Domain Decomposition Multigrid method extended to the twisted mass fermion discretization action. We show comparisons of results as a function of tuning the parameters that enter the twisted mass version of the DDalphaAMG library (https://github.com/sbacchio/DDalphaAMG). Moreover, we linked the DDalphaAMG library to the tmLQCD software package and give details on the performance of the multigrid solver during HMC simulations at the physical point.',\n",
       " 'We present synthetic far- and near-ultraviolet (FUV and NUV) maps of M31, both with and without dust reddening. These maps were constructed from spatially-resolved star formation histories (SFHs) derived from optical Hubble Space Telescope imaging of resolved stars, taken as part of the Panchromatic Hubble Andromeda Treasury program. We use stellar population synthesis modeling to generate synthetic UV maps with projected spatial resolution of $\\\\sim$100 pc ($\\\\sim$24 arcseconds) The predicted UV flux agrees well with the observed flux, with median ratios between the modeled and observed flux of $\\\\log_{10}(f^{syn}/f^{obs}) = 0.03\\\\pm0.24$ and $-0.03\\\\pm0.16$ in the FUV and NUV, respectively. This agreement is particularly impressive given that we used only optical photometry to construct these UV maps. We use the dust-free maps to examine properties of obscured flux and star formation by comparing our reddened and dust-free FUV flux maps with the observed FUV and FUV+24Î¼m flux to examine the fraction of obscured flux. The synthetic flux maps require that $\\\\sim$90% of the FUV flux in M31 is obscured by dust, while the GALEX-based methods suggest that $\\\\sim$70% of the flux is obscured. This increase in the obscured flux estimate is driven by significant differences between the dust-free synthetic FUV flux and that derived when correcting the observed FUV for dust with 24Î¼m observations. The difference is further illustrated when we compare the SFRs derived from the FUV+24Î¼m flux with the 100 Myr average SFR from the SFHs. The 24Î¼m-corrected FUV flux underestimates the SFR by a factor of $\\\\sim$2.3 - 2.5. [abridged]',\n",
       " 'The Adaptive Aggregation-based Domain Decomposition Multigrid method (arXiv:1303.1377) is extended for two degenerate flavors of twisted mass fermions. By fine-tuning the parameters we achieve a speed-up of the order of hundred times compared to the conjugate gradient algorithm for the physical value of the pion mass. A thorough analysis of the aggregation parameters is presented, which provides a novel insight into multigrid methods for lattice QCD independently of the fermion discretization.',\n",
       " \"In order to facilitate an intuitive understanding of classical physics concepts we have developed Potential Penguin - a game where players manipulate the landscape around a sliding penguin in order to control its movement. The learning goal of Potential Penguin is to familiarize players with kinetic energy and potential energy - the energies associated with movement and position in the landscape respectively. The game levels introduce the concepts one by one, as players are tasked with sliding the penguin through a landscape towards a specific location, while keeping the velocity under control. When the player manipulates the landscape, the potential energy of the penguin is changed, which determines the penguin's movement. To build a strong connection between theory and game the analytical expressions for kinetic and potential energy are displayed during play with font sizes continually growing and shrinking according to changes in each energy type. With Potential Penguin we hope to study whether visualizing the amount of kinetic and potential energy through visible mathematical expressions generates a connection between the intuitive actions taken in the game and the underlying physics concepts. The knowledge about kinetic and potential energy gained with Potential Penguin can also be used to understand most of the physics behind the citizen science game Quantum Moves, which has the goal of building a working quantum computer. The two games share the principle of the core interaction - manipulating the potential-energy landscape. We aim to investigate whether a proficiency and understanding of Potential Penguin predicts a better performance in Quantum Moves and a deeper understanding of the quantum physics behind that game.\",\n",
       " 'The next grand challenges for society and science are in the brain sciences. A collection of 60+ scientists from around the world, together with 10+ observers from national, private, and foundations, spent two days together discussing the top challenges that we could solve as a global community in the next decade. We eventually settled on three challenges, spanning anatomy, physiology, and medicine. Addressing all three challenges requires novel computational infrastructure. The group proposed the advent of The International Brain Station (TIBS), to address these challenges, and launch brain sciences to the next level of understanding.',\n",
       " 'We report on the construction and testing of a vacuum-gap Fabry-PÃ©rot etalon calibrator for high precision radial velocity spectrographs. Our etalon is traced against a rubidium frequency standard to provide a cost effective, yet ultra-precise wavelength reference. We describe here a turn-key system working at 500 nm to 900 nm, ready to be installed at any current and next generation radial velocity spectrograph that requires calibration over a wide spectral bandpass. Where appropriate, we have used off-the-shelf, commercial components with demonstrated long-term performance to accelerate the development timescale of this instrument. Our system combines for the first time the advantages of passively stabilized etalons for optical and near-infrared wavelengths with the laser-locking technique demonstrated for single-mode fiber etalons. We realize uncertainties in the position of one etalon line at the 10 cm/s level in individual measurements taken at 4 Hz. When binning the data over 10 s, we are able to trace the etalon line with a precision of better than 3 cm/s . We present data obtained during a week of continuous operation where we detect (and correct for) the predicted, but previously unobserved shrinking of the etalon Zerodur spacer corresponding to a shift of 13 cm/s per day.',\n",
       " \"Optical fibers are a key component for high-resolution spectrographs to attain high precision in radial velocity measurements. We present a custom fiber with a novel core geometry - a 'D'-shape. From a theoretical standpoint, such a fiber should provide superior scrambling and modal noise mitigation, since unlike the commonly used circular and polygonal fiber cross sections, it shows chaotic scrambling. We report on the fabrication process of a test fiber and compare the optical properties, scrambling performance, and modal noise behavior of the D-fiber with those of common polygonal fibers.\",\n",
       " 'We report on the scrambling performance and focal-ratio-degradation (FRD) of various octagonal and rectangular fibers considered for MAROON-X. Our measurements demonstrate the detrimental effect of thin claddings on the FRD of octagonal and rectangular fibers and that stress induced at the connectors can further increase the FRD. We find that fibers with a thick, round cladding show low FRD. We further demonstrate that the scrambling behavior of non-circular fibers is often complex and introduce a new metric to fully capture non-linear scrambling performance, leading to much lower scrambling gain values than are typically reported in the literature (<1000 compared to 10,000 or more). We find that scrambling gain measurements for small-core, non-circular fibers are often speckle dominated if the fiber is not agitated.',\n",
       " 'Liggett and Steif (2006) proved that, for the supercritical contact process on certain graphs, the upper invariant measure stochastically dominates an i.i.d.\\\\ Bernoulli product measure. In particular, they proved this for $\\\\mathbb{Z}^d$ and (for infection rate sufficiently large) $d$-ary homogeneous trees $T_d$.\\n  In this paper we prove some space-time versions of their results. We do this by combining their methods with specific properties of the contact process and general correlation inequalities.\\n  One of our main results concerns the contact process on $T_d$ with $d\\\\geq2$. We show that, for large infection rate, there exists a subset $Î”$ of the vertices of $T_d$, containing a \"positive fraction\" of all the vertices of $T_d$, such that the following holds: The contact process on $T_d$ observed on $Î”$ stochastically dominates an independent spin-flip process. (This is known to be false for the contact process on graphs having subexponential growth.)\\n  We further prove that the supercritical contact process on $\\\\mathbb{Z}^d$ observed on certain $d$-dimensional space-time slabs stochastically dominates an i.i.d.\\\\ Bernoulli product measure, from which we conclude strong mixing properties important in the study of certain random walks in random environment.',\n",
       " 'We report on the development and construction of a new fiber-fed, red-optical, high-precision radial-velocity spectrograph for one of the twin 6.5m Magellan Telescopes in Chile. MAROON-X will be optimized to find and characterize rocky planets around nearby M dwarfs with an intrinsic per measurement noise floor below 1 m/s. The instrument is based on a commercial echelle spectrograph customized for high stability and throughput. A microlens array based pupil slicer and double scrambler, as well as a rubidium-referenced etalon comb calibrator will turn this spectrograph into a high-precision radial-velocity machine. MAROON-X will undergo extensive lab tests in the second half of 2016.',\n",
       " 'We report on the design and construction of a microlens-array (MLA)-based pupil slicer and double scrambler for MAROON-X, a new fiber-fed, red-optical, high-precision radial-velocity spectrograph for one of the twin 6.5m Magellan Telescopes in Chile. We have constructed a 3X slicer based on a single cylindrical MLA and show that geometric efficiencies of >85% can be achieved, limited by the fill factor and optical surface quality of the MLA. We present here the final design of the 3x pupil slicer and double scrambler for MAROON-X, based on a dual MLA design with (a)spherical lenslets. We also discuss the techniques used to create a pseudo-slit of rectangular core fibers with low FRD levels.',\n",
       " 'We measure the energy emitted by extensive air showers in the form of radio emission in the frequency range from 30 to 80 MHz. Exploiting the accurate energy scale of the Pierre Auger Observatory, we obtain a radiation energy of 15.8 \\\\pm 0.7 (stat) \\\\pm 6.7 (sys) MeV for cosmic rays with an energy of 1 EeV arriving perpendicularly to a geomagnetic field of 0.24 G, scaling quadratically with the cosmic-ray energy. A comparison with predictions from state-of-the-art first-principle calculations shows agreement with our measurement. The radiation energy provides direct access to the calorimetric energy in the electromagnetic cascade of extensive air showers. Comparison with our result thus allows the direct calibration of any cosmic-ray radio detector against the well-established energy scale of the Pierre Auger Observatory.',\n",
       " 'In 1933, Meissner and Ochsenfeld reported the expulsion of magnetic flux, the diamagnetic Meissner effect, from the interior of superconducting lead. This discovery was crucial in formulating the Bardeen-Cooper-Schrieffer (BCS) theory of superconductivity. In exotic superconducting systems BCS theory does not strictly apply. A classical example is a superconductor-magnet hybrid system where magnetic ordering breaks time-reversal symmetry of the superconducting condensate and results in the stabilisation of an odd-frequency superconducting state. It has been predicted that under appropriate conditions, odd-frequency superconductivity should manifest in the Meissner state as fluctuations in the sign of the magnetic susceptibility meaning that the superconductivity can either repel (diamagnetic) or attract (paramagnetic) external magnetic flux. Here we report local probe measurements of faint magnetic fields in a Au/Ho/Nb trilayer system using low energy muons, where antiferromagnetic Ho (4.5 nm) breaks time-reversal symmetry of the proximity induced pair correlations in Au. From depth-resolved measurements below the superconducting transition of Nb we observe a local enhancement of the magnetic field in Au that exceeds the externally applied field, thus proving the existence of an intrinsic paramagnetic Meissner effect arising from an odd-frequency superconducting state.',\n",
       " 'We present a model for pragmatically describing scenes, in which contrastive behavior results from a combination of inference-driven pragmatics and learned semantics. Like previous learned approaches to language generation, our model uses a simple feature-driven architecture (here a pair of neural \"listener\" and \"speaker\" models) to ground language in the world. Like inference-driven approaches to pragmatics, our model actively reasons about listener behavior when selecting utterances. For training, our approach requires only ordinary captions, annotated _without_ demonstration of the pragmatic behavior the model ultimately exhibits. In human evaluations on a referring expression game, our approach succeeds 81% of the time, compared to a 69% success rate using existing techniques.',\n",
       " 'Convergent Cross-Mapping (CCM) has shown high potential to perform causal inference in the absence of models. We assess the strengths and weaknesses of the method by varying coupling strength and noise levels in coupled logistic maps. We find that CCM fails to infer accurate coupling strength and even causality direction in synchronized time-series and in the presence of intermediate coupling. We find that the presence of noise deterministically reduces the level of cross-mapping fidelity, while the convergence rate exhibits higher levels of robustness. Finally, we propose that controlled noise injections in intermediate-to-strongly coupled systems could enable more accurate causal inferences. Given the inherent noisy nature of real-world systems, our findings enable a more accurate evaluation of CCM applicability and advance suggestions on how to overcome its weaknesses.',\n",
       " 'The Second Workshop on Extreme Precision Radial Velocities defined circa 2015 the state of the art Doppler precision and identified the critical path challenges for reaching 10 cm/s measurement precision. The presentations and discussion of key issues for instrumentation and data analysis and the workshop recommendations for achieving this precision are summarized here.\\n  Beginning with the HARPS spectrograph, technological advances for precision radial velocity measurements have focused on building extremely stable instruments. To reach still higher precision, future spectrometers will need to produce even higher fidelity spectra. This should be possible with improved environmental control, greater stability in the illumination of the spectrometer optics, better detectors, more precise wavelength calibration, and broader bandwidth spectra. Key data analysis challenges for the precision radial velocity community include distinguishing center of mass Keplerian motion from photospheric velocities, and the proper treatment of telluric contamination. Success here is coupled to the instrument design, but also requires the implementation of robust statistical and modeling techniques. Center of mass velocities produce Doppler shifts that affect every line identically, while photospheric velocities produce line profile asymmetries with wavelength and temporal dependencies that are different from Keplerian signals.\\n  Exoplanets are an important subfield of astronomy and there has been an impressive rate of discovery over the past two decades. Higher precision radial velocity measurements are required to serve as a discovery technique for potentially habitable worlds and to characterize detections from transit missions. The future of exoplanet science has very different trajectories depending on the precision that can ultimately be achieved with Doppler measurements.',\n",
       " \"Differentiation is a mathematical skill applied throughout science in order to describe the change of a function with respect to a dependent variable. Thus, an intuitive understanding of differentiation is necessary to work with the mathematical frameworks used to describe physical systems in the higher levels of education. In order to obtain this intuition repeated practice is required. This paper presents the development of DiffGame, which consists of a series of exercises that introduce the basic principles of differentiation for high-school students through game-like elements. DiffGame have been tested with 117 first-year students from a single Danish high school, who did not have any prior training in differentiation. The students' learning was assessed by the data obtained directly from DiffGame. The test demonstrated the efficacy of DiffGame, since students at all levels demonstrate a learning gain. In contrast to previous studies demonstrating most learning in the lower tier of students, the middle tier of students (based on overall performance) exhibits the largest learning gains.\",\n",
       " 'We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural model network, achieves state-of-the-art results on benchmark datasets in both visual and structured domains.',\n",
       " \"The characterization of a physically-diverse set of transiting exoplanets is an important and necessary step towards establishing the physical properties linked to the production of obscuring clouds or hazes. It is those planets with identifiable spectroscopic features that can most effectively enhance our understanding of atmospheric chemistry and metallicity. The newly-commissioned LDSS-3C instrument on Magellan provides enhanced sensitivity and suppressed fringing in the red optical, thus advancing the search for the spectroscopic signature of water in exoplanetary atmospheres from the ground. Using data acquired by LDSS-3C and the Spitzer Space Telescope, we search for evidence of water vapor in the transmission spectrum of the Neptune-mass planet HAT-P-26b. Our measured spectrum is best explained by the presence of water vapor, a lack of potassium, and either a high-metallicity, cloud-free atmosphere or a solar-metallicity atmosphere with a cloud deck at ~10 mbar. The emergence of multi-scale-height spectral features in our data suggests that future observations at higher precision could break this degeneracy and reveal the planet's atmospheric chemical abundances. We also update HAT-P-26b's transit ephemeris, t_0 = 2455304.65218(25) BJD_TDB, and orbital period, p = 4.2345023(7) days.\",\n",
       " 'Visual question answering is fundamentally compositional in nature---a question like \"where is the dog?\" shares substructure with questions like \"what color is the dog?\" and \"where is the cat?\" This paper seeks to simultaneously exploit the representational capacity of deep networks and the compositional linguistic structure of questions. We describe a procedure for constructing and learning *neural module networks*, which compose collections of jointly-trained neural \"modules\" into deep networks for question answering. Our approach decomposes questions into their linguistic substructures, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.). The resulting compound networks are jointly trained. We evaluate our approach on two challenging datasets for visual question answering, achieving state-of-the-art results on both the VQA natural image dataset and a new dataset of complex questions about abstract shapes.',\n",
       " 'A virtual learning environment can engage university students in the learning process in ways that the traditional lectures and lab formats can not. We present our virtual learning environment \\\\emph{StudentResearcher} which incorporates simulations, multiple-choice quizzes, video lectures and gamification into a learning path for quantum mechanics at the advanced university level. \\\\emph{StudentResearcher} is built upon the experiences gathered from workshops with the citizen science game Quantum Moves at the high-school and university level, where the games were used extensively to illustrate the basic concepts of quantum mechanics. The first test of this new virtual learning environment was a 2014 course in advanced quantum mechanics at Aarhus University with 47 enrolled students. We found increased learning for the students who were more active on the platform independent of their previous performances.',\n",
       " 'Crowdscience games may hold unique potentials as learning opportunities compared to games made for fun or education. They are part of an actual science problem solving process: By playing, players help scientists, and thereby interact with real continuous research processes. This mixes the two worlds of play and science in new ways. During usability testing we discovered that users of the crowdscience game Quantum Dreams tended to answer questions in game terms, even when directed explicitly to give science explanations.We then examined these competing frames of understanding through a mixed correlational and grounded theory analysis. This essay presents the core ideas of crowdscience games as learning opportunities, and reports how a group of players used \"game\", \"science\" and \"conceptual\" frames to interpret their experience. Our results suggest that oscillating between the frames instead of sticking to just one led to the largest number of correct science interpretations, as players could participate legitimately and autonomously at multiple levels of understanding.',\n",
       " \"This paper describes an alignment-based model for interpreting natural language instructions in context. We approach instruction following as a search over plans, scoring sequences of actions conditioned on structured observations of text and the environment. By explicitly modeling both the low-level compositional structure of individual actions and the high-level structure of full plans, we are able to learn both grounded representations of sentence meaning and pragmatic constraints on interpretation. To demonstrate the model's flexibility, we apply it to a diverse set of benchmark tasks. On every task, we outperform strong task-specific baselines, and achieve several new state-of-the-art results.\",\n",
       " 'The Auger Engineering Radio Array (AERA) is part of the Pierre Auger Observatory and is used to detect the radio emission of cosmic-ray air showers. These observations are compared to the data of the surface detector stations of the Observatory, which provide well-calibrated information on the cosmic-ray energies and arrival directions. The response of the radio stations in the 30 to 80 MHz regime has been thoroughly calibrated to enable the reconstruction of the incoming electric field. For the latter, the energy deposit per area is determined from the radio pulses at each observer position and is interpolated using a two-dimensional function that takes into account signal asymmetries due to interference between the geomagnetic and charge-excess emission components. The spatial integral over the signal distribution gives a direct measurement of the energy transferred from the primary cosmic ray into radio emission in the AERA frequency range. We measure 15.8 MeV of radiation energy for a 1 EeV air shower arriving perpendicularly to the geomagnetic field. This radiation energy -- corrected for geometrical effects -- is used as a cosmic-ray energy estimator. Performing an absolute energy calibration against the surface-detector information, we observe that this radio-energy estimator scales quadratically with the cosmic-ray energy as expected for coherent emission. We find an energy resolution of the radio reconstruction of 22% for the data set and 17% for a high-quality subset containing only events with at least five radio stations with signal.',\n",
       " 'Using soft x-ray spectromicroscopy, we investigate the magnetic domain structure in embedded nanomagnets defined in La$_{0.7}$Sr$_{0.3}$MnO$_3$ thin films and LaFeO$_3$/La$_{0.7}$Sr$_{0.3}$MnO$_3$ bilayers. We find that shape-controlled antiferromagnetic domain states give rise to a significant reduction of the switching field of the rectangular nanomagnets. This is discussed in the framework of competition between an intrinsic spin-flop coupling and shape anisotropy. The data demonstrates that shape effects in antiferromagnets may be used to control the magnetic properties in nanomagnets.',\n",
       " 'Humans routinely solve problems of immense computational complexity by intuitively forming simple, low-dimensional heuristic strategies. Citizen science exploits this ability by presenting scientific research problems to non-experts. Gamification is an effective tool for attracting citizen scientists to provide solutions to research problems. While citizen science games Foldit, EteRNA and EyeWire have been used successfully to study protein and RNA folding and neuron mapping, so far gamification has not been applied to problems in quantum physics. Does the fact that everyday experiences are based on classical physics hinder the use of non-expert citizen scientists in the realm of quantum mechanics? Here we report on Quantum Moves, an online platform gamifying optimization problems in quantum physics. We show that human players are able to find solutions to difficult problems associated with the task of quantum computing. Players succeed where purely numerical optimization fails, and analyses of their solutions provide insights into the problem of optimization of a more profound and general nature. Based on player strategies, we have thus developed a new, few-parameter heuristic optimization method which efficiently outperforms the most prominent established numerical methods. The numerical complexity associated with time-optimal solutions increases for shorter process durations. To better understand this, we have made a low-dimensional rendering of the optimization landscape. These studies show why traditional optimization methods fail near the quantum speed limit, and they bring promise that combined analyses of optimization landscapes and heuristic solution strategies may benefit wider classes of optimization problems in quantum physics and beyond.',\n",
       " \"The game Quantum Moves was designed to pit human players against computer algorithms, combining their solutions into hybrid optimization to control a scalable quantum computer. In this midstream report, we open our design process and describe the series of constitutive building stages going into a quantum physics citizen science game. We present our approach from designing a core gameplay around quantum simulations, to putting extra game elements in place in order to frame, structure, and motivate players' difficult path from curious visitors to competent science contributors. The player base is extremely diverse - for instance, two top players are a 40 year old female accountant and a male taxi driver. Among statistical predictors for retention and in-game high scores, the data from our first year suggest that people recruited based on real-world physics interest and via real-world events, but only with an intermediate science education, are more likely to become engaged and skilled contributors. Interestingly, female players tended to perform better than male players, even though men played more games per day. To understand this relationship, we explore the profiles of our top players in more depth. We discuss in-world and in-game performance factors departing in psychological theories of intrinsic and extrinsic motivation, and the implications for using real live humans to do hybrid optimization via initially simple, but ultimately very cognitively complex games.\",\n",
       " 'We study the center structure of full dynamical QCD at finite temperatures and nonzero values of the background magnetic field using continuum extrapolated lattice data. We concentrate on two particular observables characterizing center clusters: their fractality and the probability for percolation. For temperatures below and around the transition region, the fractal dimension is found to be significantly smaller than three, leading to a vanishing mean free path inside the cluster structure. This finding might be relevant for center symmetry-based models of heavy-ion collisions. In addition, the percolation probability is employed to define the transition temperature and to map out the QCD phase diagram in the magnetic field-temperature plane.',\n",
       " 'Calculation of the log-normalizer is a major computational obstacle in applications of log-linear models with large output spaces. The problem of fast normalizer computation has therefore attracted significant attention in the theoretical and applied machine learning literature. In this paper, we analyze a recently proposed technique known as \"self-normalization\", which introduces a regularization term in training to penalize log normalizers for deviating from zero. This makes it possible to use unnormalized model scores as approximate probabilities. Empirical evidence suggests that self-normalization is extremely effective, but a theoretical understanding of why it should work, and how generally it can be applied, is largely lacking. We prove generalization bounds on the estimated variance of normalizers and upper bounds on the loss in accuracy due to self-normalization, describe classes of input distributions that self-normalize easily, and construct explicit examples of high-variance input distributions. Our theoretical results make predictions about the difficulty of fitting self-normalized models to several classes of distributions, and we conclude with empirical validation of these predictions.',\n",
       " 'Neutrinos in the cosmic ray flux with energies near 1 EeV and above are detectable with the Surface Detector array of the Pierre Auger Observatory. We report here on searches through Auger data from 1 January 2004 until 20 June 2013. No neutrino candidates were found, yielding a limit to the diffuse flux of ultra-high energy neutrinos that challenges the Waxman-Bahcall bound predictions. Neutrino identification is attempted using the broad time-structure of the signals expected in the SD stations, and is efficiently done for neutrinos of all flavors interacting in the atmosphere at large zenith angles, as well as for \"Earth-skimming\" neutrino interactions in the case of tau neutrinos. In this paper the searches for downward-going neutrinos in the zenith angle bins $60^\\\\circ-75^\\\\circ$ and $75^\\\\circ-90^\\\\circ$ as well as for upward-going neutrinos, are combined to give a single limit. The $90\\\\%$ C.L. single-flavor limit to the diffuse flux of ultra-high energy neutrinos with an $E^{-2}$ spectrum in the energy range $1.0 \\\\times 10^{17}$ eV - $2.5 \\\\times 10^{19}$ eV is $E_Î½^2 dN_Î½/dE_Î½< 6.4 \\\\times 10^{-9}~ {\\\\rm GeV~ cm^{-2}~ s^{-1}~ sr^{-1}}$.',\n",
       " \"We measure the recent star formation history (SFH) across M31 using optical images taken with the \\\\texit{Hubble Space Telescope} as part of the Panchromatic Hubble Andromeda Treasury (PHAT). We fit the color-magnitude diagrams in ~9000 regions that are ~100 pc $\\\\times$ 100 pc in projected size, covering a 0.5 square degree area (~380 kpc$^2$, deprojected) in the NE quadrant of M31. We show that the SFHs vary significantly on these small spatial scales but that there are also coherent galaxy-wide fluctuations in the SFH back to ~500 Myr, most notably in M31's 10-kpc star-forming ring. We find that the 10-kpc ring is at least 400 Myr old, showing ongoing star formation over the past ~500 Myr. This indicates the presence of molecular gas in the ring over at least 2 dynamical times at this radius. We also find that the ring's position is constant throughout this time, and is stationary at the level of 1 km/s, although there is evidence for broadening of the ring due to diffusion of stars into the disk. Based on existing models of M31's ring features, the lack of evolution in the ring's position makes a purely collisional ring origin highly unlikely. We find that the global SFR has been fairly constant over the last ~500 Myr, though it does show a small increase at 50 Myr that is 1.3 times the average SFR over the past 100 Myr. During the last ~500 Myr, ~60% of all SF occurs in the 10-kpc ring. Finally, we find that in the past 100 Myr, the average SFR over the PHAT survey area is $0.28\\\\pm0.03$ M$_\\\\odot$ yr$^{-1}$ with an average deprojected intensity of $7.3 \\\\times 10^{-4}$ M$_\\\\odot$ yr$^{-1}$ kpc$^{-2}$, which yields a total SFR of ~0.7 M$_\\\\odot$ yr$^{-1}$ when extrapolated to the entire area of M31's disk. This SFR is consistent with measurements from broadband estimates. [abridged]\",\n",
       " 'A measurement of the cosmic-ray spectrum for energies exceeding $4{\\\\times}10^{18}$ eV is presented, which is based on the analysis of showers with zenith angles greater than $60^{\\\\circ}$ detected with the Pierre Auger Observatory between 1 January 2004 and 31 December 2013. The measured spectrum confirms a flux suppression at the highest energies. Above $5.3{\\\\times}10^{18}$ eV, the \"ankle\", the flux can be described by a power law $E^{-Î³}$ with index $Î³=2.70 \\\\pm 0.02 \\\\,\\\\text{(stat)} \\\\pm 0.1\\\\,\\\\text{(sys)}$ followed by a smooth suppression region. For the energy ($E_\\\\text{s}$) at which the spectral flux has fallen to one-half of its extrapolated value in the absence of suppression, we find $E_\\\\text{s}=(5.12\\\\pm0.25\\\\,\\\\text{(stat)}^{+1.0}_{-1.2}\\\\,\\\\text{(sys)}){\\\\times}10^{19}$ eV.',\n",
       " 'A fundamental challenge in calcium imaging has been to infer the timing of action potentials from the measured noisy calcium fluorescence traces. We systematically evaluate a range of spike inference algorithms on a large benchmark dataset recorded from varying neural tissue (V1 and retina) using different calcium indicators (OGB-1 and GCamp6). We show that a new algorithm based on supervised learning in flexible probabilistic models outperforms all previously published techniques, setting a new standard for spike inference from calcium signals. Importantly, it performs better than other algorithms even on datasets not seen during training. Future data acquired in new experimental conditions can easily be used to further improve its spike prediction accuracy and generalization performance. Finally, we show that comparing algorithms on artificial data is not informative about performance on real population imaging data, suggesting that a benchmark dataset may greatly facilitate future algorithmic developments.',\n",
       " 'We simulate a theory with two dynamical O($a$) improved Wilson quarks whose mass $M$ ranges from a factor eight up to a factor two below the charm quark mass and at three values of the lattice spacing ranging from 0.066 to 0.034 fm. This theory is a prototype to study the decoupling of heavy quarks. We measure the mass and cut-off dependence of ratios of gluonic observables defined from the Wilson flow or the static potential. The size of the 1/$M$ corrections can be determined and disentangled from the lattice artifacts. The difference with the pure gauge theory is at the percent level when two quarks with a mass of the charm quark are present.',\n",
       " \"The Transiting Exoplanet Survey Satellite (TESS) will search for planets transiting bright and nearby stars. TESS has been selected by NASA for launch in 2017 as an Astrophysics Explorer mission. The spacecraft will be placed into a highly elliptical 13.7-day orbit around the Earth. During its two-year mission, TESS will employ four wide-field optical CCD cameras to monitor at least 200,000 main-sequence dwarf stars with I = 4-13 for temporary drops in brightness caused by planetary transits. Each star will be observed for an interval ranging from one month to one year, depending mainly on the star's ecliptic latitude. The longest observing intervals will be for stars near the ecliptic poles, which are the optimal locations for follow-up observations with the James Webb Space Telescope. Brightness measurements of preselected target stars will be recorded every 2 min, and full frame images will be recorded every 30 min. TESS stars will be 10-100 times brighter than those surveyed by the pioneering Kepler mission. This will make TESS planets easier to characterize with follow-up observations. TESS is expected to find more than a thousand planets smaller than Neptune, including dozens that are comparable in size to the Earth. Public data releases will occur every four months, inviting immediate community-wide efforts to study the new planets. The TESS legacy will be a catalog of the nearest and brightest stars hosting transiting planets, which will endure as highly favorable targets for detailed investigations.\",\n",
       " 'Recent surveys have revealed that planets intermediate in size between Earth and Neptune (\"super-Earths\") are among the most common planets in the Galaxy. Atmospheric studies are the next step toward developing a comprehensive understanding of this new class of object. Much effort has been focused on using transmission spectroscopy to characterize the atmosphere of the super-Earth archetype GJ 1214b, but previous observations did not have sufficient precision to distinguish between two interpretations for the atmosphere. The planet\\'s atmosphere could be dominated by relatively heavy molecules, such as water (e.g., a 100% water vapor composition), or it could contain high-altitude clouds that obscure its lower layers. Here we report a measurement of the transmission spectrum of GJ 1214b at near-infrared wavelengths that definitively resolves this ambiguity. These data, obtained with the Hubble Space Telescope, are sufficiently precise to detect absorption features from a high mean molecular mass atmosphere. The observed spectrum, however, is featureless. We rule out cloud-free atmospheric models with water-, methane-, carbon monoxide-, nitrogen-, or carbon dioxide-dominated compositions at greater than 5$Ïƒ$ confidence. The planet\\'s atmosphere must contain clouds to be consistent with the data.',\n",
       " 'One salient characteristic of Quantum Cascade Laser (QCL) is its very shor gain recovery time that so far thwarted the attempts to achieve self-mode locking of the device into a train of single pulses. We show theoretically that four wave mixing, combined with the short gain recovery time causes QCL to operate in the self-frequency-modulated regime characterized by a constant power in time domain and stable coherent comb in the frequency domain.Coherent frequency comb may enable many potential applications of QCL in sensing and measurement.',\n",
       " \"LP 876-10 is a nearby active M4 dwarf in Aquarius at a distance of 7.6 pc. The star is a new addition to the 10-pc census, with a parallax measured via the Research Consortium on Nearby Stars (RECONS) astrometric survey on the Small & Moderate Aperture Research Telescope System's (SMARTS) 0.9-m telescope. We demonstrate that the astrometry, radial velocity, and photometric data for LP 876-10 are consistent with the star being a third, bound, stellar component to the Fomalhaut multiple system, despite the star lying nearly 6 degrees away from Fomalhaut A in the sky. The 3D separation of LP 876-10 from Fomalhaut is only 0.77+-0.01 pc, and 0.987+-0.006 pc from TW PsA (Fomalhaut B), well within the estimated tidal radius of the Fomalhaut system (1.9 pc). LP 876-10 shares the motion of Fomalhaut within ~1 km/s, and we estimate an interloper probability of ~10^{-5}. Neither our echelle spectroscopy nor astrometry are able to confirm the close companion to LP 876-10 reported in the Washington Double Star Catalog (WSI 138). We argue that the Castor Moving Group to which the Fomalhaut system purportedly belongs, is likely to be a dynamical stream, and hence membership to the group does not provide useful age constraints for group members. LP 876-10 (Fomalhaut C) has now risen from obscurity to become a rare example of a field M dwarf with well-constrained age (440+-40 Myr) and metallicity. Besides harboring a debris disk system and candidate planet, Fomalhaut now has two of the widest known stellar companions.\",\n",
       " 'Since the first report of a potentially non-solar carbon-to-oxygen ratio (C/O) in its dayside atmosphere, the highly irradiated exoplanet WASP-12b has been under intense scrutiny and the subject of many follow-up observations. Additionally, the recent discovery of stellar binary companions ~1\" from WASP-12 has obfuscated interpretation of the observational data. Here we present new ground-based multi-object transmission-spectroscopy observations of WASP-12b that we acquired over two consecutive nights in the red optical with Gemini-N/GMOS. After correcting for the influence of WASP-12\\'s stellar companions, we find that these data rule out a cloud-free, H2 atmosphere with no additional opacity sources. We detect features in the transmission spectrum that may be attributed to metal oxides (such as TiO and VO) for an O-rich atmosphere or to metal hydrides (such as TiH) for a C-rich atmosphere. We also reanalyzed NIR transit-spectroscopy observations of WASP-12b from HST/WFC3 and broadband transit photometry from Warm Spitzer. We attribute the broad spectral features in the WFC3 data to either H2O or CH4 and HCN for an O-rich or C-rich atmosphere, respectively. The Spitzer data suggest shallower transit depths than the models predict at infrared wavelengths, albeit at low statistical significance. A multi-instrument, broad-wavelength analysis of WASP-12b suggests that the transmission spectrum is well approximated by a simple Rayleigh scattering model with a planet terminator temperature of 1870 +/- 130 K. We conclude that additional high-precision data and isolated spectroscopic measurements of the companion stars are required to place definitive constraints on the composition of WASP-12b\\'s atmosphere.',\n",
       " 'The existing Nuclear Resonance Fluorescence (NRF) setup at the HIÎ³S facility at the Triangle Universities Nuclear Laboratory at Duke University has been extended in order to perform Î³-Î³ coincidence experiments. The new setup combines large volume LaBr3:Ce detectors and high resolution HPGe detectors in a very close geometry to offer high efficiency, high energy resolution as well as high count rate capabilities at the same time. The combination of a highly efficient Î³-ray spectroscopy setup with the mono-energetic high-intensity photon beam of HIÎ³S provides a worldwide unique experimental facility to investigate the Î³-decay pattern of dipole excitations in atomic nuclei. The performance of the new setup has been assessed by studying the nucleus \\\\sulfur at 8.125 MeV beam energy. The Î³-decay branching ratio from the $1^+$ level at 8125.4 keV to the first excited $2^+$ state was determined to 15.7(3)%.',\n",
       " \"We present ground-based measurements of the transmission and emission spectra of the hot-Jupiter WASP-19b in nine spectroscopic channels from 1.25 to 2.35 microns. The measurements are based on the combined analysis of time-series spectroscopy obtained during two complete transits and two complete secondary eclipses of the planet. The observations were performed with the MMIRS instrument on the Magellan II telescope using the technique of multi-object spectroscopy with wide slits. We compare the transmission and emission data to theoretical models to constrain the composition and thermal structure of the planet's atmosphere. Our measured transmission spectrum exhibits a scatter that corresponds to 1.3 scale heights of the planet's atmosphere, which is consistent with the size of spectral features predicted by theoretical models for a clear atmosphere. We detected the secondary eclipses of the planet at significances ranging from 2.2 to 14.4 sigma. The secondary eclipse depths, and the significances of the detections increase towards longer wavelengths. Our measured emission spectrum is consistent with a 2250 K effectively isothermal 1-D model for the planet's dayside atmosphere. This model also matches previously published photometric measurements from the Spitzer Space Telescope and ground-based telescopes. These results demonstrate the important role that ground-based observations using multi-object spectroscopy can play in constraining the properties of exoplanet atmospheres, and they also emphasize the need for high-precision measurements based on observations of multiple transits and eclipses.\",\n",
       " 'We investigate models for learning the class of context-free and context-sensitive languages (CFLs and CSLs). We begin with a brief discussion of some early hardness results which show that unrestricted language learning is impossible, and unrestricted CFL learning is computationally infeasible; we then briefly survey the literature on algorithms for learning restricted subclasses of the CFLs. Finally, we introduce a new family of subclasses, the principled parametric context-free grammars (and a corresponding family of principled parametric context-sensitive grammars), which roughly model the \"Principles and Parameters\" framework in psycholinguistics. We present three hardness results: first, that the PPCFGs are not efficiently learnable given equivalence and membership oracles, second, that the PPCFGs are not efficiently learnable from positive presentations unless P = NP, and third, that the PPCSGs are not efficiently learnable from positive presentations unless integer factorization is in P.',\n",
       " 'Recently, several laser schemes have been proposed to separate racemic mixtures of enantiomers by splitting a molecular beam into subbeams consisting of molecules of definite chirality [Y. Li, C. Bruder, and C. P. Sun, Phys. Rev. Lett. 99, 130403 (2007); X. Li and M. Shapiro, J. Chem. Phys. 132, 194315 (2010)]. These ideas rely on laser-induced effective gauge potentials in an adiabatic basis which lead to a chirality dependent force on the center-of-mass. However, the effect of molecular rotation has been neglected in these studies. Accounting for the full molecular quantum state we find that the potentials from the adiabatic dressed state approach cannot be recovered once the molecular orientation dynamics is included, even in the rotational ground state. This affects substantially the ability to perform enantioseparation in the above mentioned setups.',\n",
       " 'We calculate the thermal Casimir--Polder potential of C60 and C70 fullerene molecules near gold and silicon nitride surfaces, motivated by their relevance for molecular matter wave interference experiments. We obtain the coefficients governing the asymptotic power laws of the interaction in the thermal, retarded and nonretarded distance regimes and evaluate the full potential numerically. The interaction is found to be dominated by electronic transitions, and hence independent of the internal temperature of the molecules. The contributions from phonon transitions, which are affected by the molecular temperature, give rise to only a small correction. Moreover, we find that the sizeable molecular line widths of thermal fullerenes may modify the nonretarded interaction, depending on the model used. Detailed measurements of the nonretarded potential of fullerene thus allow one to distinguish between different theories of incorporating damping.',\n",
       " 'Theoretical models predict that some of the first stars ended their lives as extremely energetic pair-instability supernovae (PISNe). With energies approaching 10^53 ergs, these supernovae are expected to be within the detection limits of the upcoming James Webb Space telescope (JWST), allowing observational constraints to be placed on the properties of the first stars. We estimate the source density of PISNe using a semi-analytic halo mass function based approach, accounting for the effects of feedback from star formation on the PISN rates using cosmological simulations. We estimate an upper limit of ~0.2 PISNe per JWST field of view at any given time. Feedback can reduce this rate significantly, e.g., lowering it to as little as one PISN per 4000 JWST fields of view for the most pessimistic explosion models. We also find that the main obstacle to observing PISNe from the first stars is their scarcity, not their faintness; exposures longer than a few times 10^4s will do little to increase the number of PISNe found. Given this we suggest a mosaic style search strategy for detecting PISNe from the first stars. Even rather high redshift PISNe are unlikely to be missed by moderate exposures, and a large number of pointings will be required to ensure a detection.',\n",
       " \"We present an investigation of the transmission spectrum of the 6.5 M_earth planet GJ1214b based on new ground-based observations of transits of the planet in the optical and near-infrared, and on previously published data. Observations with the VLT+FORS and Magellan+MMIRS using the technique of multi-object spectroscopy with wide slits yielded new measurements of the planet's transmission spectrum from 0.61 to 0.85 micron, and in the J, H, and K atmospheric windows. We also present a new measurement based on narrow-band photometry centered at 2.09 micron with the VLT+HAWKI. We combined these data with results from a re-analysis of previously published FORS data from 0.78 to 1.00 micron using an improved data reduction algorithm, and previously reported values based on Spitzer data at 3.6 and 4.5 micron. All of the data are consistent with a featureless transmission spectrum for the planet. Our K-band data are inconsistent with the detection of spectral features at these wavelengths reported by Croll and collaborators at the level of 4.1 sigma. The planet's atmosphere must either have at least 70% water by mass or optically thick high-altitude clouds or haze to be consistent with the data.\",\n",
       " 'Building on the legacy of the Sloan Digital Sky Survey (SDSS-I and II), SDSS-III is a program of four spectroscopic surveys on three scientific themes: dark energy and cosmological parameters, the history and structure of the Milky Way, and the population of giant planets around other stars. In keeping with SDSS tradition, SDSS-III will provide regular public releases of all its data, beginning with SDSS DR8 (which occurred in Jan 2011). This paper presents an overview of the four SDSS-III surveys. BOSS will measure redshifts of 1.5 million massive galaxies and Lya forest spectra of 150,000 quasars, using the BAO feature of large scale structure to obtain percent-level determinations of the distance scale and Hubble expansion rate at z<0.7 and at z~2.5. SEGUE-2, which is now completed, measured medium-resolution (R=1800) optical spectra of 118,000 stars in a variety of target categories, probing chemical evolution, stellar kinematics and substructure, and the mass profile of the dark matter halo from the solar neighborhood to distances of 100 kpc. APOGEE will obtain high-resolution (R~30,000), high signal-to-noise (S/N>100 per resolution element), H-band (1.51-1.70 micron) spectra of 10^5 evolved, late-type stars, measuring separate abundances for ~15 elements per star and creating the first high-precision spectroscopic survey of all Galactic stellar populations (bulge, bar, disks, halo) with a uniform set of stellar tracers and spectral diagnostics. MARVELS will monitor radial velocities of more than 8000 FGK stars with the sensitivity and cadence (10-40 m/s, ~24 visits per star) needed to detect giant planets with periods up to two years, providing an unprecedented data set for understanding the formation and dynamical evolution of giant planet systems. (Abridged)',\n",
       " '  We present high-precision relative radial velocities of the very low-mass star VB10 that were obtained over a time span of 0.61 yr as part of an ongoing search for planets around stars at the end of the main sequence. The radial velocities were measured from high-resolution near-infrared spectra obtained using the CRIRES instrument on the VLT with an ammonia gas cell. The typical internal precision of the measurements is 10 m/s. These data do not exhibit significant variability and are essentially constant at a level consistent with the measurement uncertainties. Therefore, we do not detect the radial velocity variations of VB10 expected due to the presence of an orbiting giant planet similar to that recently proposed by Pravdo and Shaklan based on apparent astrometric perturbations. In addition, we do not confirm the ~1 km/s radial velocity variability of the star tentatively detected by Zapatero Osorio and colleagues with lower precision measurements. Our measurements rule out planets with M_p > 3 M_Jup and the orbital period and inclination suggested by Pravdo and Shaklan at better than 5 sigma confidence. We conclude that the planet detection claimed by Pravdo and Shaklan is spurious on the basis of this result. Although the outcome of this work is a non-detection, it illustrates the potential of using ammonia cell radial velocities to detect planets around very low-mass stars.',\n",
       " '  Radial velocities measured from near-infrared spectra are a potentially powerful tool to search for planets around cool stars and sub-stellar objects. However, no technique currently exists that yields near-infrared radial velocity precision comparable to that routinely obtained in the visible. We describe a method for measuring high-precision relative radial velocities of these stars from K-band spectra. The method makes use of a glass cell filled with ammonia gas to calibrate the spectrograph response similar to the \"iodine cell\" technique that has been used very successfully in the visible. Stellar spectra are obtained through the ammonia cell and modeled as the product of a Doppler-shifted template spectrum of the object and a spectrum of the cell, convolved with a variable instrumental profile model. A complicating factor is that a significant number of telluric absorption lines are present in the spectral regions containing useful stellar and ammonia lines. The telluric lines are modeled simultaneously as well using spectrum synthesis with a time-resolved model of the atmosphere over the observatory. The free parameters in the complete model are the wavelength scale of the spectrum, the instrumental profile, adjustments to the water and methane abundances in the atmospheric model, telluric spectrum Doppler shift, and stellar Doppler shift. Tests of the method based on the analysis of hundreds of spectra obtained for late M dwarfs over six months demonstrate that precisions of ~5 m/s are obtainable over long timescales, and precisions of better than 3 m/s can be obtained over timescales up to a week. The obtained precision is comparable to the predicted photon-limited errors, but primarily limited over long timescales by the imperfect modeling of the telluric lines.',\n",
       " \"  We present a combined analysis of previously published high-precision radial velocities and astrometry for the GJ876 planetary system using a self-consistent model that accounts for the planet-planet interactions. Assuming the three planets so far identified in the system are coplanar, we find that including the astrometry in the analysis does not result in a best-fit inclination significantly different than that found by Rivera and collaborators from analyzing the radial velocities alone. In this unique case, the planet-planet interactions are of such significance that the radial velocity data set is more sensitive to the inclination of the system through the dependence of the interactions on the true masses of the two gas giant planets in the system (planets b and c). The astrometry does allow determination of the absolute orbital inclination (i.e. distinguishing between i and 180-i) and longitude of the ascending node for planet b, which allows us to quantify the mutual inclination angle between its orbit and planet c's orbit when combined with the dynamical considerations. We find that the planets have a mutual inclination of 5.0 +3.9 -2.3 degrees. This result constitutes the first determination of the degree of coplanarity in an exoplanetary system around a normal star. That we find the two planets' orbits are nearly coplanar, like the orbits of the Solar System planets, indicates that the planets likely formed in a circumstellar disk, and that their subsequent dynamical evolution into a 2:1 mean motion resonance only led to excitation of a small mutual inclination. This investigation demonstrates how the degree of coplanarity for other exoplanetary systems could also be established using data obtained from existing facilities.\",\n",
       " \"  Ribas and collaborators have recently proposed that an additional, ~5 M_earth planet orbits the transiting planet host star GJ436. Long-term dynamical interactions between the two planets leading to eccentricity excitation might provide an explanation for the transiting planet's unexpectedly large orbital eccentricity. In this paper we examine whether the existence of such a second planet is supported by the available observational data when the short-term interactions that would result from its presence are accounted for. We find that the model for the system suggested by Ribas and collaborators lead to predictions that are strongly inconsistent with the measured host star radial velocities, transiting planet primary and secondary eclipse times, and transiting planet orbital inclinations. A search for an alternative two planet model that is consistent with the data yields a number of plausible solutions, although no single one stands out as particularly unique by giving a significantly better fit to the data than the nominal single planet model. We conclude from this study that Ribas and collaborator's general hypothesis of an additional short-period planet in the GJ436 system is still plausible, but that there is not sufficient evidence to support their claim of a planet detection.\",\n",
       " '  The Landau levels of cold atomic gases in non-Abelian gauge fields are analyzed. In particular we identify effects on the energy spectrum and density distribution which are purely due to the non-Abelian character of the fields. We investigate in detail non-Abelian generalizations of both the Landau and the symmetric gauge. Finally, we discuss how these non-Abelian Landau and symmetric gauges may be generated by means of realistically feasible lasers in a tripod scheme.',\n",
       " '  The dynamics of ultracold neutral atoms subject to a non-Abelian gauge field is investigated. In particular we analyze in detail a simple experimental scheme to achieve a constant, but non-Abelian gauge field, and discuss in the frame of this gauge field the non-Abelian Aharanov-Bohm effect. In the last part of this paper, we discuss intrinsic non-Abelian effects in the dynamics of cold atomic wavepackets.',\n",
       " '  Atom reflection is studied in the presence of a non-Abelian vector potential proportional to a spin-1/2 operator. The potential is produced by a relatively simple laser configuration for atoms with a tripod level scheme. We show that the atomic motion is described by two different dispersion branches with positive or negative chirality. As a consequence of that, atom reflection shows unusual features, since an incident wave may split into two reflected ones at a barrier, an ordinary specular reflection, and an additional non-specular one. Remarkably, the latter wave can exhibit negative reflection and may become evanescent if the angle of incidence exceeds a critical value. These reflection properties are crucial for future designs in non-Abelian atom optics.',\n",
       " '  In 1940, Luis SantalÃ³ proved a Helly-type theorem for line transversals to boxes in R^d. An analysis of his proof reveals a convexity structure for ascending lines in R^d that is isomorphic to the ordinary notion of convexity in a convex subset of R^{2d-2}. This isomorphism is through a Cremona transformation on the Grassmannian of lines in P^d, which enables a precise description of the convex hull and affine span of up to d ascending lines: the lines in such an affine span turn out to be the rulings of certain classical determinantal varieties. Finally, we relate Cremona convexity to a new convexity structure that we call frame convexity, which extends to arbitrary-dimensional flats.',\n",
       " '  We present optical lightcurves of the gravitationally lensed components A (=A1+A2+A3) and B of the quadruple quasar RX J0911.4+0551 (z = 2.80). The observations were primarily obtained at the Nordic Optical Telescope between 1997 March and 2001 April and consist of 74 I-band data points for each component. The data allow the measurement of a time delay of 146 +- 8 days (2 sigma) between A and B, with B as the leading component. This value is significantly shorter than that predicted from simple models and indicates a very large external shear. Mass models including the main lens galaxy and the surrounding massive cluster of galaxies at z = 0.77, responsible for the external shear, yield H_0 = 71 +- 4 (random, 2 sigma) +- 8 (systematic) km/s/Mpc. The systematic model uncertainty is governed by the surface-mass density (convergence) at the location of the multiple images.',\n",
       " '  We use the recently proposed supergravity approach to large $N$ gauge theories to calculate ordinary and spatial Wilson loops of gauge theories in various dimensions. In this framework we observe an area law for spatial Wilson loops in four and five dimensional supersymmetric Yang-Mills at finite temperature. This can be interpreted as the area law of ordinary Wilson loops in three and four dimensional non-supersymmetric gauge theories at zero temperature which indicates confinement in these theories. Furthermore, we show that super Yang Mills theories with 16 supersymmetries at finite temperature do not admit phase transitions between the weakly coupled super Yang Mills and supergravity regimes. This result is derived by analyzing the entropy and specific heat of those systems as well as by computing ordinary Wilson loops at finite temperature. The calculation of the entropy was carried out in all different regimes and indicates that there is no first order phase transition in these systems. For the same theories at zero temperature we also compute the dependence of the quark anti-quark potential on the separating distance.',\n",
       " 'We present a compact model for Tunnel Field Effect Transistors (TFET), that captures sev- eral non-idealities such as the Trap Assisted Tunneling (TAT) originating from interface traps (Dit), along with Verilog-A implementation. We show that the TAT, together with band edge non-abruptness known as the Urbach tail, sets the lower limit of the sub-threshold swing and the minimum achievable current at a given temperature. Presence of charged trap states also contributes to reduced gate efficiency. We show that we can decouple the contribution of each of these processes and extract the intrinsic sub-threshold swing from a given experimental data. We derive closed form expressions of channel potential, electric field and effective tunnel energy window to accurately capture the essential device physics of TFETs. We test the model against recently published exper- imental data, and simulate simple TFET circuits using the Verilog-A model. The compact model provides a framework for TFET technology projections with improved device metrics such as better electrostatic design, reduced TAT, material with better transport properties etc.',\n",
       " 'We provide a detailed study of the interface Trap Assisted Tunneling (TAT) mechanism in tunnel field effect transistors to show how it contributes a major leakage current path before the Band To Band Tunneling (BTBT) is initiated. With a modified Shockley-Read-Hall formalism, we show that at room temperature, the phonon assisted TAT current always dominates and obscures the steep turn ON of the BTBT current for common densities of traps. Our results are applicable to top gate, double gate and gate all around structures where the traps are positioned between the source-channel tunneling region. Since the TAT has strong dependence on electric field, any effort to increase the BTBT current by enhancing local electric field also increases the leakage current. Unless the BTBT current can be increased separately, calculations show that the trap density Dit has to be decreased by 40-100 times compared with the state of the art in order for the steep turn ON (for III-V materials) to be clearly observable at room temperature. We find that the combination of the intrinsic sharpness of the band edges (Urbach tail) and the surface trap density determines the subthreshold swing.',\n",
       " 'With rapid advancements in the Internet of Things (IoT) paradigm, electrical devices in the near future is expected to have IoT capabilities. This enables fine-grained tracking of individual energy consumption data of such devices, offering location-independent per-device billing. Thus, it is more fine-grained than the location-based metering of state-of-the-art infrastructure, which traditionally aggregates on a building or household level, defining the entity to be billed. However, such in-device energy metering is susceptible to manipulation and fraud. As a remedy, we propose a decentralized metering architecture that enables devices with IoT capabilities to measure their own energy consumption. In this architecture, the device-level consumption is additionally reported to a system-level aggregator that verifies distributed information and provides secure data storage using Blockchain, preventing data manipulation by untrusted entities. Using evaluations on an experimental testbed, we show that the proposed architecture supports device mobility and enables location-independent monitoring of energy consumption.',\n",
       " \"Artificial Intelligence (AI) and Internet of Things (IoT) applications are rapidly growing in today's world where they are continuously connected to the internet and process, store and exchange information among the devices and the environment. The cloud and edge platform is very crucial to these applications due to their inherent compute-intensive and resource-constrained nature. One of the foremost challenges in cloud and edge resource allocation is the efficient management of computation and communication resources to meet the performance and latency guarantees of the applications. The heterogeneity of cloud resources (processors, memory, storage, bandwidth), variable cost structure and unpredictable workload patterns make the design of resource allocation techniques complex. Numerous research studies have been carried out to address this intricate problem. In this paper, the current state-of-the-art resource allocation techniques for the cloud continuum, in particular those that consider time-sensitive applications, are reviewed. Furthermore, we present the key challenges in the resource allocation problem for the cloud continuum, a taxonomy to classify the existing literature and the potential research gaps.\",\n",
       " 'Maintaining peer-to-peer connectivity with low energy overhead is a key requirement for several emerging Internet of Things (IoT) applications. It is also desirable to develop such connectivity solutions for non-static network topologies, so that resilience to device failures can be fully realized. Decentralized clustering has emerged as a promising technique to address this critical challenge. Clustering of nodes around cluster heads (CHs) provides an energy-efficient two-tier framework for peer-to-peer communication. At the same time, decentralization ensures that the framework can quickly adapt to a dynamically changing network topology. Although some decentralized clustering solutions have been proposed in the literature, they either lack guarantees on connectivity or incur significant energy overhead to maintain the clusters. In this paper, we present Decentralized Connected Resilient IoT Clustering (DeCoRIC), an energy-efficient clustering scheme that is self-organizing and resilient to network changes while guaranteeing connectivity. Using experiments implemented on the Contiki simulator, we show that our clustering scheme adapts itself to node faults in a time-bound manner. Our experiments show that DeCoRIC achieves 100% connectivity among all nodes while improving the power efficiency of nodes in the system compared to the state-of-the-art techniques BEEM and LEACH by up to 110% and 70%, respectively. The improved power efficiency also translates to longer lifetime before first node death with a best-case of 109% longer than BEEM and 42% longer than LEACH.',\n",
       " \"Let $W$ be a finite Weyl group and $\\\\widetilde W$ the corresponding affine Weyl group. A random element of $\\\\widetilde W$ can be obtained as a reduced random walk on the alcoves of $\\\\widetilde W$. By a theorem of Lam (Ann. Prob. 2015), such a walk almost surely approaches one of $|W|$ many directions. We compute these directions when $W$ is $B_n$, $C_n$ and $D_n$ and the random walk is weighted by Kac and dual Kac labels. This settles Lam's questions for types $B$ and $C$ in the affirmative and for type $D$ in the negative. The main tool is a combinatorial two row model for a totally asymmetric simple exclusion process called the $D^*$-TASEP, with four parameters. By specializing the parameters in different ways, we obtain TASEPs for each of the Weyl groups mentioned above. Computing certain correlations in these TASEPs gives the desired limiting directions.\",\n",
       " 'With declining sequencing costs a promising and affordable tool is emerging in cancer diagnostics: genomics. By using association studies, genomic variants that predispose patients to specific cancers can be identified, while by using tumor genomics cancer types can be characterized for targeted treatment. However, a severe disparity is rapidly emerging in this new area of precision cancer diagnosis and treatment planning, one which separates a few genetically well-characterized populations (predominantly European) from all other global populations. Here we discuss the problem of population-specific genetic associations, which is driving this disparity, and present a novel solution--coordinate-based local ancestry--for helping to address it. We demonstrate our boosting-based method on whole genome data from divergent groups across Africa and in the process observe signals that may stem from the transcontinental Bantu-expansion.',\n",
       " 'Identifying direct links between gene pathways and clinical endpoints for highly fatal diseases such as cancer is a formidable task. Integrative analyses play a crucial role in modeling these links by relying upon associations between a wealth of intermediary variables and genomic measurements. Motivated to harness phenotypic information about tumors towards a molecular characterization of low-grade gliomas, we develop a data driven Bayesian framework to define sharp models, and calibrate accurately and efficiently uncertainties associated with the promising biological findings.\\n  The Bayesian methods we propose in the article (i) are amenable to a flexible class of adaptive models, determined via a complex interplay between signals sifted from variable selection algorithms and domain specific knowledge; (ii) have the advantage of computationally efficient high dimensional inference due to a focus on sharp models with fewer parameters, when compared to their non-adaptive counterparts; (iii) exhibit a significantly better reconciliation between model adaptivity and inferential power than state-of-art approaches, when constrained by the curse of dimensionality. Our main workforce involves a carefully constructed conditional likelihood and utilizes a reparameterization map to obtain compact formulae for a selection-corrected posterior. Deploying our methods to investigate radiogenomic characteristics for diffuse low-grade gliomas, we successfully uncover associations between several biologically important gene pathways and patient survival times.',\n",
       " 'Electron-electron interactions (EEIs) in 2D van der Waals structures is one of the topics with high current interest in physics. We report the observation of a negative parabolic magnetoresistance (MR) in multilayer 2D semiconductor InSe beyond the low-field weak localization/antilocalization regime, and provide evidence for the EEI origin of this MR behavior. Further, we analyze this negative parabolic MR and other observed quantum transport signatures of EEIs (temperature dependent conductance and Hall coefficient) within the framework of Fermi liquid theory and extract the gate voltage tunable Fermi liquid parameter $F_0^Ïƒ$ which quantifies the electron spin-exchange interaction strength.',\n",
       " \"For open-ended language generation tasks such as storytelling and dialogue, choosing the right decoding algorithm is critical to controlling the tradeoff between generation quality and diversity. However, there presently exists no consensus on which decoding procedure is best or even the criteria by which to compare them. We address these issues by casting decoding as a multi-objective optimization problem aiming to simultaneously maximize both response quality and diversity. Our framework enables us to perform the first large-scale evaluation of decoding methods along the entire quality-diversity spectrum. We find that when diversity is a priority, all methods perform similarly, but when quality is viewed as more important, the recently proposed nucleus sampling (Holtzman et al. 2019) outperforms all other evaluated decoding algorithms. Our experiments also confirm the existence of the `likelihood trap', the counter-intuitive observation that high likelihood sequences are often surprisingly low quality. We leverage our findings to create and evaluate an algorithm called \\\\emph{selective sampling} which tractably approximates globally-normalized temperature sampling.\",\n",
       " \"Machine learning models are known to perpetuate the biases present in the data, but oftentimes these biases aren't known until after the models are deployed. We present the Visual Bias Extraction (ViBE) Tool that assists in the investigation of a visual dataset, surfacing potential dataset biases along three dimensions: (1) object-based, (2) gender-based, and (3) geography-based. Object-based biases relate to things like size, context, or diversity of object representation in the dataset; gender-based metrics aim to reveal the stereotypical portrayal of people of different genders within the dataset, with future iterations of our tool extending the analysis to additional axes of identity; geography-based analysis considers the representation of different geographic locations. Our tool is designed to shed light on the dataset along these three axes, allowing both dataset creators and users to gain a better understanding of what exactly is portrayed in their dataset. The responsibility then lies with the tool user to determine which of the revealed biases may be problematic, taking into account the cultural and historical context, as this is difficult to determine automatically. Nevertheless, the tool also provides actionable insights that may be helpful for mitigating the revealed concerns. Overall, our work allows for the machine learning bias problem to be addressed early in the pipeline at the dataset stage. ViBE is available at https://github.com/princetonvisualai/vibe-tool.\",\n",
       " 'We present optimal schemes, based on photon number measurements, for Gaussian state tomography and for Gaussian process tomography. An $n$-mode Gaussian state is completely specified by $2 n^2+3n$ parameters. Our scheme requires exactly $2 n^2+3n$ distinct photon number measurements to tomograph the state and is therefore optimal. Further, we describe an optimal scheme to characterize Gaussian processes by using coherent state probes and photon number measurements. With much recent progress in photon number measurement experimental techniques, we hope that our scheme will be useful in various quantum information processing protocols including entanglement detection, quantum computation, quantum key distribution and quantum teleportation. This work builds upon the works of Parthasarathy et al. [Infin. Dimens. Anal. Quantum Probab. Relat. Top., 18(4): 1550023, 21, 2015].',\n",
       " 'Let $R=\\\\mathbb{K}[X_1, \\\\ldots , X_n ]$ be a polynomial ring over a field $\\\\mathbb{K}$. We introduce an endomorphism $\\\\mathcal{F}^{[m]}: R \\\\rightarrow R $ and denote the image of an ideal $I$ of $R$ via this endomorphism as $I^{[m]}$ and call it to be the $m$ \\\\textit{-th square power} of $I$. In this article, we study some homological invariants of $I^{[m]}$ such as regularity, projective dimension, associated primes and depth for some families of ideals e.g. monomial ideals. We also relate the asymptotic constants e.g. Waldschmidt constant and resurgence number, of a monomial ideal $I$ with that of $I^{[m]}$.',\n",
       " 'Recent technological advances have fostered the development of complex industrial cyber-physical systems which demand real-time communication with delay guarantees. The consequences of delay requirement violation in such systems may become increasingly severe. In this paper, we propose a contract-based fault-resilient methodology which aims at managing the communication delays of real-time flows in industries. With this objective, we present a light-weight mechanism to estimate end-to-end delay in the network in which the clocks of the switches are not synchronized. The mechanism aims at providing high level of accuracy with lower communication overhead. We then propose a contract-based framework using software-defined networking where the components are associated with delay contracts and a resilience manager. The proposed resilience management framework contains: (1) contracts which state guarantees about components behaviors, (2) observers which are responsible to detect contract failure (fault), (3) monitors to detect events such as run-time changes in the delay requirements and link failure, (4) control logic to take suitable decisions based on the type of the fault, (5) resilience manager to decide response strategies containing the best course of action as per the control logic decision. Finally, we present a delay-aware path finding algorithm which is used to route/reroute the real-time flows to provide resiliency in the case of faults and, to adapt to the changes in the network state. Performance of the proposed framework is evaluated with the Ryu SDN controller and Mininet network emulator.',\n",
       " 'In this work, relative Underlying Event (UE) transverse multiplicity activity classifier ($R_{\\\\rm {T}}$) is used to study the strange and multi-strange hadron production in proton-proton collisions. Our study with $R_{\\\\rm {T}}$ would allow to disentangle these particles which are originating from the soft and hard QCD processes. We have used the PYTHIA 8 MC with different implementation of color reconnection and rope hydronisation models to demonstrate the proton-proton collisions data at $\\\\sqrt{s}$ = 13 TeV. The relative production of strange and multi-strange hadrons are discussed extensively in low and high transverse activity region. In this contribution, the relative strange hadron production is enhanced with increasing $R_{\\\\rm {T}}$. This enhancement is significant for strange baryons as compared to mesons. In addition, the particle ratios as a function of $R_{\\\\rm {T}}$ confirms the baryon enhancement in newCR, whereas Rope model confirms the baryon enhancement only with strange quark content. An experimental confirmation of such results will provide more insight into the soft physics in the transverse region which will be useful to investigate various tunes based on hadronization and color reconnection schemes.',\n",
       " 'With the growing scale of Cyber-Physical Systems (CPSs), it is challenging to maintain their stability under all operating conditions. How to reduce the downtime and locate the failures becomes a core issue in system design. In this paper, we employ a hierarchical contract-based resilience framework to guarantee the stability of CPS. In this framework, we use Assume Guarantee (A-G) contracts to monitor the non-functional properties of individual components (e.g., power and latency), and hierarchically compose such contracts to deduce information about faults at the system level. The hierarchical contracts enable rapid fault detection in large-scale CPS. However, due to the vast number of components in CPS, manually designing numerous contracts and the hierarchy becomes challenging. To address this issue, we propose a technique to automatically decompose a root contract into multiple lower-level contracts depending on I/O dependencies between components. We then formulate a multi-objective optimization problem to search the optimal parameters of each lower-level contract. This enables automatic contract refinement taking into consideration the communication overhead between components. Finally, we use a case study from the manufacturing domain to experimentally demonstrate the benefits of the proposed framework.',\n",
       " 'We study the use of methods based on the real symplectic groups $Sp(2n,\\\\mathcal{R})$ in the analysis of the Arthurs-Kelly model of proposed simultaneous measurements of position and momentum in quantum mechanics. Consistent with the fact that such measurements are in fact not possible, we show that the observable consequences of the Arthurs-Kelly interaction term are contained in the symplectic transformation law connecting the system plus apparatus variance matrices at an initial and a final time. The individual variance matrices are made up of averages and spreads or uncertainties for single hermitian observables one at a time, which are quantum mechanically well defined. The consequences of the multimode symplectic covariant Uncertainty Principle in the Arthurs-Kelly context are examined.',\n",
       " 'This demonstration presents a framework for building a resilient Cyber-Physical Systems (CPS) cyber-infrastructure through the use of hierarchical parametric assume-guarantee contracts. A Fischertechnik Sorting Line with Color Detection training model is used to showcase our framework.',\n",
       " 'Industrial cyber-infrastructure is normally a multilayered architecture. The purpose of the layered architecture is to hide complexity and allow independent evolution of the layers. In this paper, we argue that this traditional strict layering results in poor transparency across layers affecting the ability to significantly improve resiliency. We propose a contract-based methodology where components across and within the layers of the cyber-infrastructure are associated with contracts and a light-weight resilience manager. This allows the system to detect faults (contract violation monitored using observers) and react (change contracts dynamically) effectively. It results in (1) improving transparency across layers; helps resiliency, (2) decoupling fault-handling code from application code; helps code maintenance, (3) systematically generate error-free fault handling code; reduces development time. Using an industrial case study, we demonstrate the proposed methodology.',\n",
       " 'As the industrial cyber-infrastructure become increasingly important to realise the objectives of Industry~4.0, the consequence of disruption due to internal or external faults become increasingly severe. Thus there is a need for a resilient infrastructure. In this paper, we propose a contract-based methodology where components across layers of the cyber-infrastructure are associated with contracts and a light-weight resilience manager. This allows the system to detect faults (contract violation monitored using observers) and react (change contracts dynamically) effectively.',\n",
       " 'Orchestrated collaborative effort of physical and cyber components to satisfy given requirements is the central concept behind Cyber-Physical Systems (CPS). To duly ensure the performance of components, a software-based resilience manager is a flexible choice to detect and recover from faults quickly. However, a single resilience manager, placed at the centre of the system to deal with every fault, suffers from decision-making overburden; and therefore, is out of the question for distributed large-scale CPS. On the other hand, prompt detection of failures and efficient recovery from them are challenging for decentralised resilience managers. In this regard, we present a novel resilience management framework that utilises the concept of management hierarchy. System design contracts play a key role in this framework for prompt fault-detection and recovery. Besides the details of the framework, an Industry 4.0 related test case is presented in this article to provide further insights.',\n",
       " 'We report a metal-insulator like transition in single crystalline 3D topological insulator Bi2Te3 at a temperature of 230K in presence of an external magnetic field applied normal to the surface. This transition becomes more prominent at larger magnetic field strength with the residual resistance value increasing linearly with the magnetic field. At low temperature, the magnetic field dependence of the magnetoresistance shows a transition from logarithmic to linear behavior and the onset magnetic field value for this transition decreases with increasing temperature. The logarithmic magnetoresistance indicates the anti-localization of the surface Dirac electrons while the high temperature behavior originates from the bulk carriers due to intrinsic impurities. At even higher temperatures beyond~230 K, a completely classical Lorentz model type quadratic behavior of the magnetoresistance is observed. We also show that the experimentally observed anomalies at ~230K in the magneto-transport properties do not originate from any stacking fault in Bi2Te3.',\n",
       " 'Bismuth telluride is a low energy bulk band-gap topological system with conducting surface states. Besides its very good thermoelectric properties, it also makes a very good candidate for broadband photodetectors. Here, we report temperature-dependent photo-Seebeck effect in a bulk single crystalline bismuth telluride. On light illumination, an electrically biased sample shows distinguishable contributions in the measured current due to both the Seebeck effect and the normal photo-generated carriers within a narrow layer of the sample. Detailed experiments are performed to elucidate the distinction between the Seebeck contribution and the photogenerated current. The temperature-dependence of the photocurrent without Seebeck contribution shows a sign reversal from negative to positive at a specific temperature depending on the wavelength of photoexcitation light.',\n",
       " 'Scheduling of constrained deadline sporadic task systems on multiprocessor platforms is an area which has received much attention in the recent past. It is widely believed that finding an optimal scheduler is hard, and therefore most studies have focused on developing algorithms with good processor utilization bounds. These algorithms can be broadly classified into two categories: partitioned scheduling in which tasks are statically assigned to individual processors, and global scheduling in which each task is allowed to execute on any processor in the platform. In this paper we consider a third, more general, approach called cluster-based scheduling. In this approach each task is statically assigned to a processor cluster, tasks in each cluster are globally scheduled among themselves, and clusters in turn are scheduled on the multiprocessor platform. We develop techniques to support such cluster-based scheduling algorithms, and also consider properties that minimize total processor utilization of individual clusters. In the last part of this paper, we develop new virtual cluster-based scheduling algorithms. For implicit deadline sporadic task systems, we develop an optimal scheduling algorithm that is neither Pfair nor ERfair. We also show that the processor utilization bound of US-EDF{m/(2m-1)} can be improved by using virtual clustering. Since neither partitioned nor global strategies dominate over the other, cluster-based scheduling is a natural direction for research towards achieving improved processor utilization bounds.',\n",
       " 'Mixed-criticality real-time scheduling has been developed to improve resource utilization while guaranteeing safe execution of critical applications. These studies use optimistic resource reservation for all the applications to improve utilization, but prioritize critical applications when the reservations become insufficient at runtime. Many of them however share an impractical assumption that all the critical applications will simultaneously demand additional resources. As a consequence, they under-utilize resources by penalizing all the low-criticality applications. In this paper we overcome this shortcoming using a novel mechanism that comprises a parameter to model the expected number of critical applications simultaneously demanding more resources, and an execution strategy based on the parameter to improve resource utilization. Since most mixed-criticality systems in practice are component-based, we design our mechanism such that the component boundaries provide the isolation necessary to support the execution of low-criticality applications, and at the same time protect the critical ones. We also develop schedulability tests for the proposed mechanism under both a flat as well as a hierarchical scheduling framework. Finally, through simulations, we compare the performance of the proposed approach with existing studies in terms of schedulability and the capability to support low-criticality applications.',\n",
       " 'In this paper we improve the image embeddings generated in the graph neural network solution for few shot learning. We propose alternate architectures for existing networks such as Inception-Net, U-Net, Attention U-Net, and Squeeze-Net to generate embeddings and increase the accuracy of the models. We improve the quality of embeddings created at the cost of the time taken to generate them. The proposed implementations outperform the existing state of the art methods for 1-shot and 5-shot learning on the Omniglot dataset. The experiments involved a testing set and training set which had no common classes between them. The results for 5-way and 10-way/20-way tests have been tabulated.',\n",
       " 'We observed the impact of finite magnetic field on the in-medium mass and decay constant of isospin averaged vector $D^*(D^{*^+},D^{*^0})$ and axial-vector $D_1(D^+_1, D^0_1)$ mesons. The quark and gluon condensates of the nuclear medium at finite magnetic field, temperature, isospin asymmetry, and density have been obtained by the meson exchange scalar fields within the chiral SU(3) model. The medium attributes modify the scalar and vector density of nuclear medium and this variation reflects in the in-medium mass and decay constant of spin 1 $D$ mesons. We calculate these observables by comparing the Operator Product Expansion (OPE) and the phenomenological side in the QCD Sum Rules. In the results, we observed a positive mass shift for charged vector and axial-vector $D$ mesons with respect to magnetic field. For neutral vector (axial-vector) $D$ mesons we observed negative (positive) mass shift as a function of magnetic field. In the application part, we calculate the in-medium partial decay width of the process $D^*_s$(2715/2860) $\\\\rightarrow$ $D^* K$ by using $^3P_0$ model. The in-medium effects are incorporated through the in-medium masses of $D^*$ and $K$ mesons.',\n",
       " 'Quantum discord is a measure based on local projective measurements which captures quantum correlations that go beyond entanglement. A change in the measurement process, achieved by replacing rank-one projectors with a weak positive operator-valued measure (POVM), allows one to define a quantity termed as weak quantum discord. In this work, we experimentally simulate the effect of a weak POVM on a nuclear magnetic resonance quantum information processor. The two-qubit system under investigation is part of a three-qubit system, where one of the qubits is used as an ancillary to implement the phase damping channel. The strength of the weak POVM is controlled by varying the strength of the phase damping channel. We experimentally observed weak discord in two-qubit states and computed a cost function whose purpose is to optimally extract correlations representing the difference between weak and strong quantum discord. The resultant dynamics of the states is investigated as a function of the measurement strength.',\n",
       " 'At the LHC, a TeV-scale leptoquark (LQ) that decays dominantly to a top quark ($t$) and a light charged lepton ($\\\\ell=e,Î¼$) would form a resonance system of boosted-$t$ $+$ high-$p_{\\\\rm T}$-$\\\\ell$. We consider all possible vector LQ models within the BuchmÃ¼ller-RÃ¼ckl-Wyler classifications with the desired decay. We propose simple phenomenological Lagrangians that are suitable for bottom-up/experimental studies and, at the same time, can cover the relevant parameter spaces of these models. In this simplified framework, we study the pair and single production channels of vector LQs at the LHC. Interestingly, we find that, like the pair production, the cross sections of some single production processes also depend on the parameter $Îº$ that appears in the gluon-vector LQ coupling. We adopt a search strategy of selecting events with at least one boosted hadronic top quark and exactly two high-$p_{\\\\rm T}$ leptons of the same flavor and opposite sign. This combines events from the pair and single production processes and, therefore, can enhance the discovery potential than that of the pair-production-only searches. For $5Ïƒ$ discovery we find that vector LQs can be probed up to 2.55 TeV for $100\\\\%$ branching ratio in the $t\\\\ell $ decay mode and $\\\\mathcal{O}(1)$ new couplings at the 14 TeV LHC with 3 ab$^{-1}$ of integrated luminosity.',\n",
       " 'We use a constrained convex optimization (CCO) method to experimentally characterize arbitrary quantum states and unknown quantum processes on a two-qubit NMR quantum information processor. Standard protocols for quantum state and quantum process tomography are based on linear inversion, which often result in an unphysical density matrix and hence an invalid process matrix. The CCO method on the other hand, produces physically valid density matrices and process matrices, with significantly improved fidelity as compared to the standard methods. The constrainedoptimization problem is solved with the help of a semi-definite programming (SDP) protocol. We use the CCO method to estimate the Kraus operators and characterize gates in the presence of errors due to decoherence. We then assume Markovian system dynamics and use a Lindblad master equation in conjunction with the CCO method to completely characterize the noise processes present in the NMR qubits.',\n",
       " 'Thermodynamical properties of asymmetric strange quark matter using the Polyakov Chiral $\\\\text{SU(3)}$ quark mean field (PCQMF) model at finite temperature and chemical potential have been investigated. Within the PCQMF model, the properties of quark matter are calculated through the scalar fields $Ïƒ$, $Î¶$, $Î´$ and $Ï‡$, the vector fields $Ï‰$, $Ï$ and $Ï†$ and the Polyakov loop fields $Î¦$ and $\\\\barÎ¦$. The isospin splitting of constituent quark masses is observed at large isospin asymmetry. The effect of temperature and strangeness fraction on energy per baryon and equation of state is found to be appreciable in quark matter. The effect of the Polyakov loop dynamics on several thermodynamical bulk quantities such as energy density, entropy density, and trace anomaly is presented and compared with recent lattice QCD results.',\n",
       " 'Eigensystem Realization Algorithm (ERA) is a data-driven approach for subspace system identification and is widely used in many areas of engineering. However, the computational cost of the ERA is dominated by a step that involves the singular value decomposition (SVD) of a large, dense matrix with block Hankel structure. This paper develops computationally efficient algorithms for reducing the computational cost of the SVD step by using randomized subspace iteration and exploiting the block Hankel structure of the matrix. We provide a detailed analysis of the error in the identified system matrices and the computational cost of the proposed algorithms. We demonstrate the accuracy and computational benefits of our algorithms on two test problems: the first involves a partial differential equation that models the cooling of steel rails, and the second is an application from power systems engineering.',\n",
       " 'We report the detection of a Saturn-size exoplanet orbiting HD 332231 (TOI 1456) in light curves from the Transiting Exoplanet Survey Satellite (TESS). HD 332231, an F8 dwarf star with a V-band magnitude of 8.56, was observed by TESS in Sectors 14 and 15. We detect a single-transit event in the Sector 15 presearch data conditioning (PDC) light curve. We obtain spectroscopic follow-up observations of HD 332231 with the Automated Planet Finder, Keck I, and SONG telescopes. The orbital period we infer from the radial velocity (RV) observations leads to the discovery of another transit in Sector 14 that was masked by PDC due to scattered light contamination. A joint analysis of the transit and RV data confirms the planetary nature of HD 332231 b, a Saturn-size ($0.867^{+0.027}_{-0.025} \\\\; R_{\\\\rm J}$), sub-Saturn-mass ($0.244\\\\pm0.021 \\\\; M_{\\\\rm J}$) exoplanet on a 18.71 day circular orbit. The low surface gravity of HD 332231 b and the relatively low stellar flux it receives make it a compelling target for transmission spectroscopy. Also, the stellar obliquity is likely measurable via the Rossiter-McLaughlin effect, an exciting prospect given the 0.14 au orbital separation of HD 332231 b. The spectroscopic observations do not provide substantial evidence for any additional planets in the HD 332231 system, but continued RV monitoring is needed to further characterize this system. We also predict that the frequency and duration of masked data in the PDC light curves for TESS Sectors 14-16 could hide transits of some exoplanets with orbital periods between 10.5 and 17.5 days.',\n",
       " 'Digital twin is a virtual replica of a real-world object that lives simultaneously with its physical counterpart. Since its first introduction in 2003 by Grieves, digital twin has gained momentum in a wide range of applications such as industrial manufacturing, automotive and artificial intelligence. However, many digital-twin-related approaches, found in industries as well as literature, mainly focus on modelling individual physical things with high-fidelity methods with limited scalability. In this paper, we introduce a digital-twin architecture called TiLA (Twin-in-the-Loop Architecture). TiLA employs heterogeneous models and online data to create a digital twin, which follows a Globally Asynchronous Locally Synchronous (GALS) model of computation. It facilitates the creation of a scalable digital twin with different levels of modelling abstraction as well as giving GALS formalism for execution strategy. Furthermore, TiLA provides facilities to develop applications around the twin as well as an interface to synchronise the twin with the physical system through an industrial communication protocol. A digital twin for a manufacturing line has been developed as a case study using TiLA. It demonstrates the use of digital twin models together with online data for monitoring and analysing failures in the physical system.',\n",
       " 'Learning Enabled Components (LECs) are widely being used in a variety of perception based autonomy tasks like image segmentation, object detection, end-to-end driving, etc. These components are trained with large image datasets with multimodal factors like weather conditions, time-of-day, traffic-density, etc. The LECs learn from these factors during training, and while testing if there is variation in any of these factors, the components get confused resulting in low confidence predictions. The images with factors not seen during training is commonly referred to as Out-of-Distribution (OOD). For safe autonomy it is important to identify the OOD images, so that a suitable mitigation strategy can be performed. Classical one-class classifiers like SVM and SVDD are used to perform OOD detection. However, the multiple labels attached to the images in these datasets, restricts the direct application of these techniques. We address this problem using the latent space of the $Î²$-Variational Autoencoder ($Î²$-VAE). We use the fact that compact latent space generated by an appropriately selected $Î²$-VAE will encode the information about these factors in a few latent variables, and that can be used for computationally inexpensive detection. We evaluate our approach on the nuScenes dataset, and our results shows the latent space of $Î²$-VAE is sensitive to encode changes in the values of the generative factor.',\n",
       " 'Many existing studies on mixed-criticality (MC) scheduling assume that low-criticality budgets for high-criticality applications are known apriori. These budgets are primarily used as guidance to determine when the scheduler should switch the system mode from low to high. Based on this key observation, in this paper we propose a dynamic MC scheduling model under which low-criticality budgets for individual high-criticality applications are determined at runtime as opposed to being fixed offline. To ensure sufficient budget for high-criticality applications at all times, we use offline schedulability analysis to determine a system-wide total low-criticality budget allocation for all the high-criticality applications combined. This total budget is used as guidance in our model to determine the need for a mode-switch. The runtime strategy then distributes this total budget among the various applications depending on their execution requirement and with the objective of postponing mode-switch as much as possible. We show that this runtime strategy is able to postpone mode-switches for a longer time than any strategy that uses a fixed low-criticality budget allocation for each application. Finally, since we are able to control the total budget allocation for high-criticality applications before mode-switch, we also propose techniques to determine these budgets considering system-wide objectives such as schedulability and service guarantee for low-criticality applications.',\n",
       " 'Past experiments about hydrogen absorption in niobium have revealed specific properties about interactions between interstitial hydrogen atoms. It has been reported that there are long-range attractive and short-range repulsive interactions between interstitial hydrogen atoms in niobium. It has also been reported that these interactions are of many-body nature. While previous understanding of these interactions is based on experimental inferences from past experiments, through these calculations, for the first time, we can understand the nature of the interactions at a fundamental level. In this work, we use Density Functional Theory calculations to study the interactions of interstitial hydrogen atoms in niobium. We report here that these interactions are a combination of an attractive, indirect image interaction and a repulsive, direct interaction. Through our calculations, we also infer here that these interactions indeed have many-body characteristics.',\n",
       " 'Mixed-Criticality (MC) systems consolidate multiple functionalities with different criticalities onto a single hardware platform. Such systems improve the overall resource utilization while guaranteeing resources to critical tasks. In this paper, we focus on the problem of partitioned multiprocessor MC scheduling, in particular the problem of designing efficient partitioning strategies. We develop two new partitioning strategies based on the principle of evenly distributing the difference between total high-critical utilization and total low-critical utilization for the critical tasks among all processors. By balancing this difference, we are able to reduce the pessimism in uniprocessor MC schedulability tests that are applied on each processor, thus improving overall schedulability. To evaluate the schedulability performance of the proposed strategies, we compare them against existing partitioned algorithms using extensive experiments. We show that the proposed strategies are effective with both dynamic-priority Earliest Deadline First with Virtual Deadlines (EDF-VD) and fixed-priority Adaptive Mixed-Criticality (AMC) algorithms. Specifically, our results show that the proposed strategies improve schedulability by as much as 28.1% and 36.2% for implicit and constrained-deadline task systems respectively.',\n",
       " 'Strategies that artificially tighten high-criticality task deadlines in low-criticality behaviors have been successfully employed for scheduling mixed-criticality systems. Although efficient scheduling algorithms have been developed for implicit deadline task systems, the same is not true for more general sporadic tasks. In this paper we develop a new demand-based schedulability test for such general mixed-criticality task systems, in which we collectively bound the low- and high-criticality demand of tasks. We show that the new test strictly dominates the only other known demand-based test for such systems. We also propose a new deadline tightening strategy based on this test, and show through simulations that the strategy significantly outperforms all known scheduling algorithms for a variety of sporadic task systems.',\n",
       " 'Different scheduling algorithms for mixed criticality systems have been recently proposed. The common denominator of these algorithms is to discard low critical tasks whenever high critical tasks are in lack of computation resources. This is achieved upon a switch of the scheduling mode from Normal to Critical. We distinguish two main categories of the algorithms: system-level mode switch and task-level mode switch. System-level mode algorithms allow low criticality (LC) tasks to execute only in normal mode. Task-level mode switch algorithms enable to switch the mode of an individual high criticality task (HC), from low (LO) to high (HI), to obtain priority over all LC tasks. This paper investigates an online scheduling algorithm for mixed-criticality systems that supports dynamic mode switches for both task level and system level. When a HC task job overruns its LC budget, then only that particular job is switched to HI mode. If the job cannot be accommodated, then the system switches to Critical mode. To accommodate for resource availability of the HC jobs, the LC tasks are degraded by stretching their periods until the Critical mode exhibiting job complete its execution. The stretching will be carried out until the resource availability is met. We have mechanized and implemented the proposed algorithm using Uppaal. To study the efficiency of our scheduling algorithm, we examine a case study and compare our results to the state of the art algorithms.',\n",
       " 'In this paper we consider the problem of mixed-criticality (MC) scheduling of implicit-deadline sporadic task systems on a homogenous multiprocessor platform. Focusing on dual-criticality systems, algorithms based on the fluid scheduling model have been proposed in the past. These algorithms use a dual-rate execution model for each high-criticality task depending on the system mode. Once the system switches to the high-criticality mode, the execution rates of such tasks are increased to meet their increased demand. Although these algorithms are speed-up optimal, they are unable to schedule several feasible dual-criticality task systems. This is because a single fixed execution rate for each high-criticality task after the mode switch is not efficient to handle the high variability in demand during the transition period immediately following the mode switch. This demand variability exists as long as the carry-over jobs of high-criticality tasks, that is jobs released before the mode switch, have not completed. Addressing this shortcoming, we propose a multi-rate fluid execution model for dual-criticality task systems in this paper. Under this model, high-criticality tasks are allocated varying execution rates in the transition period after the mode switch to efficiently handle the demand variability. We derive a sufficient schedulability test for the proposed model and show its dominance over the dual-rate fluid execution model. Further, we also present a speed-up optimal rate assignment strategy for the multi-rate model, and experimentally show that the proposed model outperforms all the existing MC scheduling algorithms with known speed-up bounds.',\n",
       " 'Systems in many safety-critical application domains are subject to certification requirements. In such a system, there are typically different applications providing functionalities that have varying degrees of criticality. Consequently, the certification requirements for functionalities at these different criticality levels are also varying, with very high levels of assurance required for a highly critical functionality, whereas relatively low levels of assurance required for a less critical functionality. Considering the timing assurance given to various applications in the form of guaranteed budgets within deadlines, a theory of real-time scheduling for such multi-criticality systems has been under development in the recent past. In particular, an algorithm called Earliest Deadline First with Virtual Deadlines (EDF-VD) has shown a lot of promise for systems with two criticality levels, especially in terms of practical performance demonstrated through experiment results. In this paper we design a new schedulability test for EDF-VD that extend these performance benefits to multi-criticality systems. We propose a new test based on demand bound functions and also present a novel virtual deadline assignment strategy. Through extensive experiments we show that the proposed technique significantly outperforms existing strategies for a variety of generic real-time systems.',\n",
       " \"SREC markets are a market-based system designed to incentivize solar energy generation. A regulatory body imposes a lower bound on the amount of energy each regulated firm must generate via solar means, providing them with a certificate for each MWh generated. Regulated firms seek to navigate the market to minimize the cost imposed on them, by modulating their SREC generation and trading activities. As such, the SREC market can be viewed through the lens of a large stochastic game with heterogeneous agents, where agents interact through the market price of the certificates. We study this stochastic game by solving the mean-field game (MFG) limit with sub-populations of heterogeneous agents. Our market participants optimize costs accounting for trading frictions, cost of generation, SREC penalty, and generation uncertainty. Using techniques from variational analysis, we characterize firms' optimal controls as the solution of a new class of McKean-Vlasov FBSDE and determine the equilibrium SREC price. We further prove that MFG strategy has the $Îµ$-Nash property for the finite player game. Finally, we numerically solve the MV-FBSDEs and conclude by demonstrating how firms behave in equilibrium using simulated examples.\",\n",
       " 'This paper presents a novel solution technique for scheduling multi-energy system (MES) in a commercial urban building to perform price-based demand response and reduce energy costs. The MES scheduling problem is formulated as a mixed integer nonlinear program (MINLP), a non-convex NPhard problem with uncertainties due to renewable generation and demand. A model predictive control approach is used to handle the uncertainties and price variations. This in-turn requires solving a time-coupled multi-time step MINLP during each time-epoch which is computationally intensive. This investigation proposes an approach called the Scenario-Based Branch-and-Bound (SB3), a light-weight solver to reduce the computational complexity. It combines the simplicity of convex programs with the ability of meta-heuristic techniques to handle complex nonlinear problems. The performance of the SB3 solver is validated in the Cleantech building, Singapore and the results demonstrate that the proposed algorithm reduces energy cost by about 17.26% and 22.46% as against solving a multi-time step heuristic optimization model.',\n",
       " 'High-index dielectric metasurfaces featuring Mie-type electric and magnetic resonances have been of a great interest in a variety of applications such as imaging, sensing, photovoltaics and others, which led to the necessity of an efficient large-scale fabrication technique. To address this, here we demonstrate the use of single-pulse laser interference for direct patterning of an amorphous silicon film into an array of Mie resonators. The proposed technique is based on laser-interference-induced dewetting. A precise control of the laser pulse energy enables the fabrication of ordered dielectric metasurfaces in areas spanning tens of micrometers and consisting of thousands of hemispherical nanoparticles with a single laser shot. The fabricated nanoparticles exhibit a wavelength-dependent optical response with a strong electric dipole signature. Variation of the pre-deposited silicon film thickness allows tailoring of the resonances in the targeted visible and infrared spectral ranges. Such direct and high-throughput fabrication paves the way towards a simple realization of spatially invariant metasurface-based devices.',\n",
       " 'The Standard Model (SM) when extended with a Leptoquark (LQ) and right handed neutrinos, can have interesting new implications for Higgs physics. We show that the sterile neutrinos can induce a significant boost to the down-type quark Yukawa interactions through a diagonal coupling associated with the quarks and a scalar LQ of electromagnetic charge $1/3$. The relative change is much more pronounced in case of the first two generations of quarks as they have vanishingly small Yukawa couplings in the SM. The enhancement in the couplings would also lead to a non-negligible contribution of the quark fusion process to the production of the 125 GeV Higgs scalar in the SM, though the gluon fusion always dominates. However, this may not be true for a general scalar. As an example, we consider a scenario with a SM-gauge-singlet scalar $Ï†$ where an $\\\\mathcal O(1)$ coupling between $Ï†$ and the LQ is allowed. The $Ï†q \\\\bar{q}$ Yukawa couplings can only be generated radiatively through a loop of LQ and sterile neutrinos. Here, the quark fusion process can have significant cross section, specially for a light $Ï†$. It can even supersede the normally dominant gluon fusion process for a moderate to large value of the LQ mass. This model can be tested/constrained at the high luminosity run of the LHC through a potentially large branching fraction of the scalar to two-jets.',\n",
       " 'A common way to analyze electrostatic Micro Electro Mechanical Systems (MEMS) actuators is to use their energy-displacement landscape. Here, we introduce an alternative approach to analyze electrostatic MEMS actuators using their energy-charge landscape. This technique involves coordinate transformation from displacement to charge, thereby formulating the Hamiltonian of electrostatic MEMS actuators in terms of charge. We present the first investigation on using the energy-charge landscape to analyze static pull-in, dynamic pull-in and pull-out phenomena in a unified manner. The voltage expressions derived using this method are identical with those derived using the conventional energy-displacement landscape. In addition, we also obtain the expressions for charge under static and dynamic pull-in conditions. This work can aid in the design and analysis of electrostatic MEMS devices.',\n",
       " 'In this article, we analyse the relationship between the Bell violation and the secure key rate of entanglement assisted quantum key distribution (QKD) protocols. Specifically, we address the question whether Bell violation is necessary or sufficient for secure communication. We construct a class of states which do not show Bell violation, however, which can be used for secure communication after local filtering. Similarly, we identify another class of states which show Bell violation but can not be used for generating secure key even after local filtering. The existence of these two classes of states demonstrates that Bell violation as an initial resource is neither necessary nor sufficient for QKD. Our work therefore forces a departure from traditional thinking that the degree of Bell violation is a key resource for quantum communication and brings out the role of local filtering.',\n",
       " 'The small scale observations indicate that the dark matter may be self-interacting. In this paper, we calculate the shear ($Î·$) and bulk viscosity ($Î¶$) of Self Interacting Dark Matter (SIDM) fluid, in kinetic theory formalism. Further, using the KSS bound on $Î·/\\\\mathfrak{s}$, we derive a new upper limit on the ratio of dark matter self interaction cross section to its mass, $ Ïƒ/m $. Then, using the $ Ïƒ/m $ constraint, we show that KSS bound allows only sub-GeV mass of SIDM particle. Further, with the assumption of a power-law form of $Î·$ and $Î¶$, we study its evolution in the light of low redshift observations. We find that at the large redshift, the SIDM viscosity is small but at the small redshift it becomes sufficiently large and contributes significantly in cosmic dissipation. As a consequence, viscous SIDM can explain the low redshift observations and also consistent with the standard cosmological prediction.',\n",
       " 'Solar contamination, due to moonlight and atmospheric scattering of sunlight, can cause systematic errors in stellar radial velocity (RV) measurements that significantly detract from the ~10cm/s sensitivity required for the detection and characterization of terrestrial exoplanets in or near Habitable Zones of Sun-like stars. The addition of low-level spectral contamination at variable effective velocity offsets introduces systematic noise when measuring velocities using classical mask-based or template-based cross-correlation techniques. Here we present simulations estimating the range of RV measurement error induced by uncorrected scattered sunlight contamination. We explore potential correction techniques, using both simultaneous spectrometer sky fibers and broadband imaging via coherent fiber imaging bundles, that could reliably reduce this source of error to below the photon-noise limit of typical stellar observations. We discuss the limitations of these simulations, the underlying assumptions, and mitigation mechanisms. We also present and discuss the components designed and built into the NEID precision RV instrument for the WIYN 3.5m telescope, to serve as an ongoing resource for the community to explore and evaluate correction techniques. We emphasize that while \"bright time\" has been traditionally adequate for RV science, the goal of 10cm/s precision on the most interesting exoplanetary systems may necessitate access to darker skies for these next-generation instruments.',\n",
       " 'Several theoretical models based on totally asymmetric simple exclusion process (TASEP) have been extensively utilized to study various non-equilibrium transport phenomena. Inspired by the the role of microtubule-transported vesicles in intracellular transport, we propose a generalized TASEP model where two distinct particles are directed to hop stochastically in opposite directions on a flexible lattice immersed in a three dimensional pool of diffusing particles. We investigate the interplay between lattice conformation and bidirectional transport by obtaining the stationary phase diagrams and density profiles within the framework of mean field theory. For the case when recycling strength is independent of density of particles, the topology of phase diagram alters quantitatively. However, if the lattice occupancy governs the global conformation of lattice, in addition to the pre-existing phases for bidirectional transport a new asymmetric shock-low density phase originates in the system. We identified that this phase is sensitive to finite size effect and vanishes in the thermodynamic limit.',\n",
       " 'We study \\\\emph{multiplicity equivalence} testing of automata over partially commutative monoids (pc monoids) and show efficient algorithms in special cases, exploiting the structure of the underlying non-commutation graph of the monoid.\\n  Specifically, if the clique cover number of the non-commutation graph (the minimum number of cliques covering the graph) of the pc monoid is a constant, we obtain a deterministic quasi-polynomial time algorithm. As a consequence, we also obtain the first deterministic quasi-polynomial time algorithms for multiplicity equivalence testing of $k$-tape automata and for equivalence testing of deterministic $k$-tape automata for constant $k$. Prior to this, a randomized polynomial-time algorithm for the above problems was shown by Worrell [ICALP 2013].\\n  We also consider pc monoids for which the non-commutation graphs have cover consisting of at most $k$ cliques and star graphs for any constant $k$. We obtain randomized polynomial-time algorithm for multiplicity equivalence testing of automata over such monoids.',\n",
       " 'We study the non-equilibrium steady states in a closed system consisting of interacting particles obeying exclusion principle with quenched hopping rate. Cluster mean field approach is utilized to theoretically analyze the system dynamics in terms of phase diagram, density profiles, current, etc. with respect to interaction energy $E$. It turns out that on increasing the interaction energy beyond a critical value, $E_c$, shock region shows non-monotonic behavior and contracts until another critical value $E_{c_1}$ is attained; a further increase leads to its expansion. Moreover, the phase diagram of an interacting system with specific set of parameters has a good agreement with its non-interacting analogue. For interaction energy below $E_c$, a new shock phase displaying features different from non-interacting version is observed leading to two distinct shock phases. We have also performed Monte Carlo simulations extensively to validate our theoretical findings.',\n",
       " 'The purpose of this study is to analyze the efficacy of transfer learning techniques and transformer-based models as applied to medical natural language processing (NLP) tasks, specifically radiological text classification. We used 1,977 labeled head CT reports, from a corpus of 96,303 total reports, to evaluate the efficacy of pretraining using general domain corpora and a combined general and medical domain corpus with a bidirectional representations from transformers (BERT) model for the purpose of radiological text classification. Model performance was benchmarked to a logistic regression using bag-of-words vectorization and a long short-term memory (LSTM) multi-label multi-class classification model, and compared to the published literature in medical text classification. The BERT models using either set of pretrained checkpoints outperformed the logistic regression model, achieving sample-weighted average F1-scores of 0.87 and 0.87 for the general domain model and the combined general and biomedical-domain model. General text transfer learning may be a viable technique to generate state-of-the-art results within medical NLP tasks on radiological corpora, outperforming other deep models such as LSTMs. The efficacy of pretraining and transformer-based models could serve to facilitate the creation of groundbreaking NLP models in the uniquely challenging data environment of medical text.',\n",
       " 'The generalized singular value decomposition (GSVD) is a valuable tool that has many applications in computational science. However, computing the GSVD for large-scale problems is challenging. Motivated by applications in hyper-differential sensitivity analysis (HDSA), we propose new randomized algorithms for computing the GSVD which use randomized subspace iteration and weighted QR factorization. Detailed error analysis is given which provides insight into the accuracy of the algorithms and the choice of the algorithmic parameters. We demonstrate the performance of our algorithms on test matrices and a large-scale model problem where HDSA is used to study subsurface flow.',\n",
       " 'In the recent years, deep learning approaches have shown much promise in modeling complex systems in the physical sciences. A major challenge in deep learning of PDEs is enforcing physical constraints and boundary conditions. In this work, we propose a general framework to directly embed the notion of an incompressible fluid into Convolutional Neural Networks, and apply this to coarse-graining of turbulent flow. These physics-embedded neural networks leverage interpretable strategies from numerical methods and computational fluid dynamics to enforce physical laws and boundary conditions by taking advantage the mathematical properties of the underlying equations. We demonstrate results on three-dimensional fully-developed turbulence, showing that this technique drastically improves local conservation of mass, without sacrificing performance according to several other metrics characterizing the fluid flow.',\n",
       " 'Measurement of short-lived hadronic resonances are used to study different aspects of particle production and collision dynamics in pp, p-A and relativistic heavy-ion collisions. The yields of resonances are sensitive to the competing processes of hadron rescattering and regeneration, thus making these particles unique probes of the properties of the late hadronic phase. Measurements of resonances with different masses and quantum numbers also provide insight into strangeness production and processes that determine the shapes of particle momentum spectra at intermediate transverse momenta, as well as the species dependence of hadron suppression at high momentum. We present the comprehensive set of results in the ALICE experiment with unprecedented precision for $Ï(770)^{0}$, K$^{*}(892)$, $Ï†(1020)$, $Î£(1385)^{\\\\pm}$, $Î›(1520)$, and $Îž(1530)^{0}$ production in pp, p-Pb, Xe-Xe and Pb-Pb collisions in the energy range $\\\\sqrt{s_{\\\\rm NN}}$ = 2.76-13 TeV, including the latest measurements from LHC Run 2. The obtained results are used to study the system-size and collision-energy evolution of transverse momentum spectra, particle ratios and nuclear modification factors and to search for the onset of collectivity in small collision systems. We compare these results to lower energy measurements and model calculations where available.',\n",
       " 'Talek is a private group messaging system that sends messages through potentially untrustworthy servers, while hiding both data content and the communication patterns among its users. Talek explores a new point in the design space of private messaging; it guarantees access sequence indistinguishability, which is among the strongest guarantees in the space, while assuming an anytrust threat model, which is only slightly weaker than the strongest threat model currently found in related work. Our results suggest that this is a pragmatic point in the design space, since it supports strong privacy and good performance: we demonstrate a 3-server Talek cluster that achieves throughput of 9,433 messages/second for 32,000 active users with 1.7-second end-to-end latency. To achieve its security goals without coordination between clients, Talek relies on information-theoretic private information retrieval. To achieve good performance and minimize server-side storage, Talek introduces new techniques and optimizations that may be of independent interest, e.g., a novel use of blocked cuckoo hashing and support for private notifications. The latter provide a private, efficient mechanism for users to learn, without polling, which logs have new messages.',\n",
       " 'The goal of this paper is to present a method for simultaneous trajectory and local stabilizing policy optimization to generate local policies for trajectory-centric model-based reinforcement learning (MBRL). This is motivated by the fact that global policy optimization for non-linear systems could be a very challenging problem both algorithmically and numerically. However, a lot of robotic manipulation tasks are trajectory-centric, and thus do not require a global model or policy. Due to inaccuracies in the learned model estimates, an open-loop trajectory optimization process mostly results in very poor performance when used on the real system. Motivated by these problems, we try to formulate the problem of trajectory optimization and local policy synthesis as a single optimization problem. It is then solved simultaneously as an instance of nonlinear programming. We provide some results for analysis as well as achieved performance of the proposed technique under some simplifying assumptions.',\n",
       " \"Mt. Abu Faint Object Spectrograph and Camera (MFOSC-P) is an in-house developed instrument for Physical Research Laboratory (PRL) 1.2m telescope at Mt. Abu India, commissioned in February 2019. Here we present the first science results derived from the low resolution spectroscopy program of a sample of M Dwarfs carried out during the commissioning run of MFOSC-P between February-June 2019. M dwarfs carry great significance for exoplanets searches in habitable zone and are among the promising candidates for the observatory's several ongoing observational campaigns. Determination of their accurate atmospheric properties and fundamental parameters is essential to constrain both their atmospheric and evolutionary models. In this study, we provide a low resolution (R$\\\\sim$500) spectroscopic catalogue of 80 bright M dwarfs (J$<$10) and classify them using their optical spectra. We have also performed the spectral synthesis and $Ï‡^2$ minimisation techniques to determine their fundamental parameters viz. effective temperature and surface gravity by comparing the observed spectra with the most recent BT-Settl synthetic spectra. Spectral type of M dwarfs in our sample ranges from M0 to M5. The derived effective temperature and surface gravity are ranging from 4000 K to 3000 K and 4.5 to 5.5 dex, respectively. In most of the cases, the derived spectral types are in good agreement with previously assigned photometric classification.\",\n",
       " 'This paper studies the capacities of input-driven finite-state channels, i.e., channels whose current state is a time-invariant deterministic function of the previous state and the current input. We lower bound the capacity of such a channel using a dynamic programming formulation of a bound on the maximum reverse directed information rate. We show that the dynamic programming-based bounds can be simplified by solving the corresponding Bellman equation explicitly. In particular, we provide analytical lower bounds on the capacities of $(d, k)$-runlength-limited input-constrained binary symmetric and binary erasure channels. Furthermore, we provide a single-letter lower bound based on a class of input distributions with memory.',\n",
       " 'We consider an exclusion process on a periodic one-dimensional lattice where all particles perform simple symmetric exclusion at rate $1$ except for a single tracer particle, which performs partially simple asymmetric exclusion with rate $p$ to the right and rate $q$ to the left. This model was first considered by Ferrari, Goldstein and Lebowitz (Progr. Phys., 1985) as a test for the validity of the Einstein relation in microscopic systems.\\n  The main thrust of this work is an exact solution for the steady state of this exclusion process. We show that the stationary probabilities factorize and give an exact formula for the nonequilibrium partition function. Perhaps surprisingly, we find that the nonequilibrium free energy in the steady state is not well-defined for this system in the thermodynamic limit for any values of $p$ and $q$ if $p \\\\neq q$. We provide formulas for the current and two-point correlations. When the tracer particle performs asymmetric exclusion ($q=0$), the results are shown to simplify significantly and we find an unexpected connection with the combinatorics of set partitions. Finally, we study the system from the point of view of the tracer particle, the so-called environment process. In the environment process, we show that the density of particles decays exponentially with the scaled position in front of the tracer particle in the thermodynamic limit.',\n",
       " 'The non-local nature of the correlations possessed by quantum systems may be revealed by experimental demonstrations of the violation of Bell-type inequalities. Recent work has placed bounds on the correlations that quantum systems can possess in an actual experiment. These bounds were limited to a composite quantum system comprising of a few lower-dimensional subsystems. In a more general approach, it has been shown that fewer body correlations can reveal the non-local nature of the correlations arising from a quantum mechanical description of nature. Such tests on the correlations can be transformed to a semi-definite program (SDP). This study reports the experimental implementation of a local measurement-based hierarchy on the nuclear magnetic resonance (NMR) hardware utilizing three nuclear spins as qubits. The protocol has been experimentally tested on genuinely entangled tripartite states such as W state, GHZ state and a few graph states. In all the cases, the experimentally measured correlations were used to formulate the SDP, using linear constraints on the entries of the moment matrix. We observed that for each genuinely entangled state, the SDP failed to find a semi-definite positive moment matrix consistent with the experimental data. This implies that the observed correlations can not arise from local measurements on a separable state and are hence non-local in nature, and also confirms that the states being tested are indeed entangled.',\n",
       " 'Parameter inference of dynamical systems is a challenging task faced by many researchers and practitioners across various fields. In many applications, it is common that only limited variables are observable. In this paper, we propose a method for parameter inference of a system of nonlinear coupled ODEs with partial observations. Our method combines fast Gaussian process based gradient matching (FGPGM) and deterministic optimization algorithms. By using initial values obtained by Bayesian steps with low sampling numbers, our deterministic optimization algorithm is both accurate and efficient.',\n",
       " 'We propose a trust region method for policy optimization that employs Quasi-Newton approximation for the Hessian, called Quasi-Newton Trust Region Policy Optimization QNTRPO. Gradient descent is the de facto algorithm for reinforcement learning tasks with continuous controls. The algorithm has achieved state-of-the-art performance when used in reinforcement learning across a wide range of tasks. However, the algorithm suffers from a number of drawbacks including: lack of stepsize selection criterion, and slow convergence. We investigate the use of a trust region method using dogleg step and a Quasi-Newton approximation for the Hessian for policy optimization. We demonstrate through numerical experiments over a wide range of challenging continuous control tasks that our particular choice is efficient in terms of number of samples and improves performance',\n",
       " 'We relate character theory of the symmetric groups $S_{2n}$ and $S_{2n+1}$ with that of the hyperoctahedral group $B_n = ({\\\\mathbb Z}/2)^n \\\\rtimes S_n$, as part of the expectation that the character theory of reductive groups with diagram automorphism and their Weyl groups, is related to the character theory of the fixed subgroup of the diagram automorphism.',\n",
       " 'Embeddings -- mappings from high-dimensional discrete input to lower-dimensional continuous vector spaces -- have been widely adopted in machine learning, linguistics, and computational biology as they often surface interesting and unexpected domain semantics. Through semi-structured interviews with embedding model researchers and practitioners, we find that current tools poorly support a central concern: comparing different embeddings when developing fairer, more robust models. In response, we present the Embedding Comparator, an interactive system that balances gaining an overview of the embedding spaces with making fine-grained comparisons of local neighborhoods. For a pair of models, we compute the similarity of the k-nearest neighbors of every embedded object, and visualize the results as Local Neighborhood Dominoes: small multiples that facilitate rapid comparisons. Using case studies, we illustrate the types of insights the Embedding Comparator reveals including how fine-tuning embeddings changes semantics, how language changes over time, and how training data differences affect two seemingly similar models.',\n",
       " 'In this paper, we begin with the study of elements in $C^*$-algebras which are mapped to Schatten class ideals through faithful left regular representation. We further give some functorial properties of Schatten classes on the category of representations of a $C^*$-algebra and category of unitary representations of a group.',\n",
       " 'In this work we perform a study of various unsupervised methods to identify mental stress in firefighter trainees based on unlabeled heart rate variability data. We collect RR interval time series data from nearly 100 firefighter trainees that participated in a drill. We explore and compare three methods in order to perform unsupervised stress detection: 1) traditional K-Means clustering with engineered time and frequency domain features 2) convolutional autoencoders and 3) long short-term memory (LSTM) autoencoders, both trained on the raw RRI measurements combined with DBSCAN clustering and K-Nearest-Neighbors classification. We demonstrate that K-Means combined with engineered features is unable to capture meaningful structure within the data. On the other hand, convolutional and LSTM autoencoders tend to extract varying structure from the data pointing to different clusters with different sizes of clusters. We attempt at identifying the true stressed and normal clusters using the HRV markers of mental stress reported in the literature. We demonstrate that the clusters produced by the convolutional autoencoders consistently and successfully stratify stressed versus normal samples, as validated by several established physiological stress markers such as RMSSD, Max-HR, Mean-HR and LF-HF ratio.',\n",
       " 'Let $G$ be a simple graph on $n$ vertices. Let $L_G \\\\text{ and } \\\\mathcal{I}_G \\\\: $ denote the\\n  LovÃ¡sz-Saks-Schrijver(LSS) ideal and parity binomial edge ideal of $G$ in the polynomial ring $S = \\\\mathbb{K}[x_1, \\\\ldots, x_n, y_1, \\\\ldots, y_n] $ respectively. We classify graphs whose LSS ideals and parity binomial edge ideals are complete intersections. We also classify graphs whose LSS ideals, parity binomial edge ideals are almost complete intersection and we prove that their Rees algebra is Cohen-Macaulay. We compute the second graded Betti number and obtain a minimal presentation of LSS ideals of trees and odd unicyclic graphs. We also obtain an explicit description of the defining ideal of the symmetric algebra of LSS ideals of trees and odd unicyclic graphs.',\n",
       " 'Business research practice is witnessing a surge in the integration of predictive modeling and prescriptive analysis. We describe a modeling framework JANOS that seamlessly integrates the two streams of analytics, for the first time allowing researchers and practitioners to embed machine learning models in an optimization framework. JANOS allows for specifying a prescriptive model using standard optimization modeling elements such as constraints and variables. The key novelty lies in providing modeling constructs that allow for the specification of commonly used predictive models and their features as constraints and variables in the optimization model. The framework considers two sets of decision variables; regular and predicted. The relationship between the regular and the predicted variables are specified by the user as pre-trained predictive models. JANOS currently supports linear regression, logistic regression, and neural network with rectified linear activation functions, but we plan to expand on this set in the future. In this paper, we demonstrate the flexibility of the framework through an example on scholarship allocation in a student enrollment problem and provide a numeric performance evaluation.',\n",
       " 'A coupled dielectric-metal metasurface (CDMM) consisting of amorphous silicon (a-$\\\\textit{Si}$) rings and subwavelength holes in $\\\\textit{Au}$ layer separated by a $\\\\textit{SiO}_{2}$ layer is presented. The design parameters of the CDMM is numerically optimized to have a polarization independent peak transmittance of 0.55 at 1540 nm with a Full Width at Half Maximum ($FWHM$) of 10 nm. The filter also has a 100 nm quite zone with $\\\\sim 10^{-2}$ transmittance. A radiating two-oscillator model reveals the fundamental resonances in the filter which interfere to produce the electromagnetically induced transparency (EIT) like effect. Multipole expansion of the currents in the structure validates the fundamental resonances predicted by the two-oscillator model. The presented CDMM filter is robust to artifacts in device fabrication and has performances comparable to a conventional Fabry-PÃ©rot filter. However, it is easier to be integrated in image sensors as the transmittance peak can be tuned by only changing the periodicity resulting in a planar structure with a fixed height.',\n",
       " 'One key use of k-means clustering is to identify cluster prototypes which can serve as representative points for a dataset. However, a drawback of using k-means cluster centers as representative points is that such points distort the distribution of the underlying data. This can be highly disadvantageous in problems where the representative points are subsequently used to gain insights on the data distribution, as these points do not mimic the distribution of the data. To this end, we propose a new clustering method called \"distributional clustering\", which ensures cluster centers capture the distribution of the underlying data. We first prove the asymptotic convergence of the proposed cluster centers to the data generating distribution, then present an efficient algorithm for computing these cluster centers in practice. Finally, we demonstrate the effectiveness of distributional clustering on synthetic and real datasets.',\n",
       " 'Fix $t \\\\geq 2$. We first give an asymptotic formula for certain sums of the number of $t$-cores. We then use this result to compute the distribution of the size of the $t$-core of a uniformly random partition of an integer $n$. We show that this converges weakly to a gamma distribution after appropriate rescaling. As a consequence, we find that the size of the $t$-core is of the order of $\\\\sqrt{n}$ in expectation. We then apply this result to show that the probability that $t$ divides the hook length of a uniformly random cell in a uniformly random partition equals $1/t$ in the limit. Finally, we extend this result to all modulo classes of $t$ using continual Young diagrams and abacus representations for cores and quotients.',\n",
       " 'Fast Radio Bursts (FRBs) are short lived ($\\\\sim$ msec), energetic transients (having a peak flux density of $\\\\sim$ Jy) with no known prompt emission in other energy bands. We present results of a search for prompt X-ray emissions from 41 FRBs using the Cadmium Zinc Telluride Imager (CZTI) on AstroSat which continuously monitors $\\\\sim70\\\\%$ of the sky. Our searches on various timescales in the 20--200~keV range, did not yield any counterparts in this hard X-ray band. We calculate upper limits on hard X-ray flux, in the same energy range and convert them to upper bounds for $Î·$: the ratio X-ray to radio fluence of FRBs. We find $Î·\\\\leq 10^{8-10}$ for hard X-ray emission. Our results will help constrain the theoretical models of FRBs as the models become more quantitative and nearer, brighter FRBs are discovered.',\n",
       " 'Task-oriented dialog presents a difficult challenge encompassing multiple problems including multi-turn language understanding and generation, knowledge retrieval and reasoning, and action prediction. Modern dialog systems typically begin by converting conversation history to a symbolic object referred to as belief state by using supervised learning. The belief state is then used to reason on an external knowledge source whose result along with the conversation history is used in action prediction and response generation tasks independently. Such a pipeline of individually optimized components not only makes the development process cumbersome but also makes it non-trivial to leverage session-level user reinforcement signals. In this paper, we develop Neural Assistant: a single neural network model that takes conversation history and an external knowledge source as input and jointly produces both text response and action to be taken by the system as output. The model learns to reason on the provided knowledge source with weak supervision signal coming from the text generation and the action prediction tasks, hence removing the need for belief state annotations. In the MultiWOZ dataset, we study the effect of distant supervision, and the size of knowledge base on model performance. We find that the Neural Assistant without belief states is able to incorporate external knowledge information achieving higher factual accuracy scores compared to Transformer. In settings comparable to reported baseline systems, Neural Assistant when provided with oracle belief state significantly improves language generation performance.',\n",
       " 'Industrial process control systems are time-critical systems where reliable communications between sensors and actuators need to be guaranteed within strict deadlines to maintain safe operation of all the components of the system. WirelessHART is the most widely adopted standard which serve as the medium of communication in industrial setups due to its support for Time Division Multiple Access (TDMA)based communication, multiple channels, channel hopping, centralized architecture, redundant routes and avoidance of spatial re-use of channels. However, the communication schedule in WirelessHART network is decided by a centralized network manager at the time of network initialization and the same communication schedule repeats every hyper-period. Due to predictability in the time slots of the communication schedule, these systems are vulnerable to timing attacks which eventually can disrupt the safety of the system. In this work, we present a moving target defense mechanism, the SlotSwapper, which uses schedule randomization techniques to randomize the time slots over a hyper-period schedule, while still preserving all the feasibility constraints of a real-time WirelessHART network and makes the schedule uncertain every hyper-period. We tested the feasibility of the generated schedules on random topologies with 100 simulated motes in Cooja simulator. We use schedule entropy to measure the confidentiality of our algorithm in terms of randomness in the time slots of the generated schedules.',\n",
       " 'The $k$-dimensional Weisfeiler-Leman procedure ($k$-WL), which colors $k$-tuples of vertices in rounds based on the neighborhood structure in the graph, has proven to be immensely fruitful in the algorithmic study of Graph Isomorphism. More generally, it is of fundamental importance in understanding and exploiting symmetries in graphs in various settings. Two graphs are $k$-WL-equivalent if the $k$-dimensional Weisfeiler-Leman procedure produces the same final coloring on both graphs. 1-WL-equivalence is known as fractional isomorphism of graphs, and the $k$-WL-equivalence relation becomes finer as $k$ increases.\\n  We investigate to what extent standard graph parameters are preserved by $k$-WL-equivalence, focusing on fractional graph packing numbers. The integral packing numbers are typically NP-hard to compute, and we discuss applicability of $k$-WL-invariance for estimating the integrality gap of the LP relaxation provided by their fractional counterparts.',\n",
       " 'Mechanical metamaterials are usually designed to show desired responses to prescribed forces. In some applications, the desired force-response relationship might be hard to specify exactly, although examples of forces and corresponding desired responses are easily available. Here we propose a framework for supervised learning in a thin creased sheet that learns the desired force-response behavior from training examples of spatial force patterns and can then respond correctly to previously unseen test forces. During training, we fold the sheet using different training forces and assume a learning rule that changes stiffness of creases in response to their folding strain. We find that this learning process reshapes non-linearities inherent in folding a sheet so as to show the correct response for previously unseen test forces. We study the relationship between training error, test error and sheet size which plays the role of model complexity. Our framework shows how the complex energy landscape of disordered mechanical materials can be reshaped using an iterative local learning rule.',\n",
       " \"We show that simple, commercially sourced n-channel silicon field-effect transistors (nFETs) operating under closed loop control exhibit an ~3-fold improvement in pH readout resolution to (7.2+/-0.3)x10^-3 at a bandwidth of 10 Hz when compared with the open loop operation commonly employed by integrated ion-sensitive field-effect transistors (ISFETs). We leveraged the improved nFET performance to measure the change in solution pH arising from the activity of a pathological form of the kinase Cdk5, an enzyme implicated in Alzheimer's disease, and showed quantitative agreement with previous measurements. The improved pH resolution was realized while the devices were operated in a remote sensing configuration with the pH sensing element off-chip and connected electrically to the FET gate terminal. We compared these results with those measured by using a custom-built dual-gate 2D field-effect transistor (dg2DFET) fabricated with 2D semi-conducting MoS2 channels and a moderate device gain, alpha=8. Under identical solution conditions the pH resolution of the nFETs was only 2-fold worse than the dg2DFETs pH resolution of (3.9+/-0.7)x10^-3. Finally, using the nFETs, we demonstrated the effectiveness of a custom polypeptide, p5, as a therapeutic agent in restoring the function of Cdk5. We expect that the straight-forward modifications to commercially sourced nFETs demonstrated here will lower the barrier to widespread adoption of these remote-gate devices and enable sensitive bioanalytical measurements for high throughput screening in drug discovery and precision medicine applications.\",\n",
       " 'Living systems evolve one mutation at a time, but a single mutation can alter the effect of subsequent mutations. The underlying mechanistic determinants of such epistasis are unclear. Here, we demonstrate that the physical dynamics of a biological system can generically constrain epistasis. We analyze models and experimental data on proteins and regulatory networks. In each, we find that if the long-time physical dynamics is dominated by a slow, collective mode, then the dimensionality of mutational effects is reduced. Consequently, epistatic coefficients for different combinations of mutations are no longer independent, even if individually strong. Such epistasis can be summarized as resulting from a global non-linearity applied to an underlying linear trait, i.e., as global epistasis. This constraint, in turn, reduces the ruggedness of the sequence-to-function map. By providing a generic mechanistic origin for experimentally observed global epistasis, our work suggests that slow collective physical modes can make biological systems evolvable.',\n",
       " 'Solar X-ray Monitor (XSM) is one of the scientific instruments on-board Chandrayaan-2 orbiter. The XSM along with instrument CLASS (Chandras Large Area Soft x-ray Spectrometer) comprise the remote X-ray fluorescence spectroscopy experiment of Chandrayaan-2 mission with an objective to determine the elemental composition of the lunar surface on a global scale. XSM instrument will measure the solar X-rays in the energy range of 1-15 keV using state-of-the-art Silicon Drift Detector (SDD). The Flight Model (FM) of the XSM payload has been designed, realized and characterized for various operating parameters. XSM provides energy resolution of 180 eV at 5.9 keV with high time cadence of one second. The X-ray spectra of the Sun observed with XSM will also contribute to the study of solar corona. The detailed description and the performance characteristics of the XSM instrument are presented in this paper.',\n",
       " 'Students of software engineering struggle to develop a systems perspective because most of the software engineering methodologies focus on developing a particular aspect of a system. Lack of unified coverage to the topic of systems modelling is identified as the root cause behind this problem. The paper explains the role of ontology in building systems perspective. A case for the necessity of ontology training as a means to overcome this problem is presented. The course content for a typical course on ontology is also described in the paper.',\n",
       " 'Perovskite semiconductors have demonstrated outstanding external luminescence quantum yields, therefore potentially allowing power conversion efficiencies (PCE) close to the thermodynamic limits. However, the precise conditions that are required to advance to an efficiency regime above monocrystalline silicon cells are not well understood. In this work, we establish a simulation model that well describes efficient p-i-n type perovskite solar cells (PCE~20%) and a range of different experiments helping to quantify the efficiency-limiting processes in state-of-the-art devices. Based on these results, we studied the role of important device and material parameters with a particular focus on chemical doping, carrier mobilities, energy level alignment and the built-in potential (V_BI) across all stack layers. We demonstrate that an efficiency regime of 30% can be unlocked by optimizing the built-in potential across the perovskite layer by using either highly doped (10^19 cm-3) thick transport layers (TLs) or ultrathin undoped TLs, e.g. self-assembled monolayers. Importantly, we only consider parameters that have been already demonstrated in recent literature, that is a bulk lifetime of 10 us, interfacial recombination velocities of 100 cm/s, a perovskite bandgap (E_gap) of 1.47 eV and an EQE of 95%. A maximum efficiency of 31% is obtained for a bandgap of 1.4 eV using doped TLs. The results of this paper promise continuous PCE improvements until perovskites may become the most efficient single-junction solar cell technology in the near future.',\n",
       " 'Let $G$ be a simple graph on $n$ vertices and $J_G$ denote the corresponding binomial edge ideal in $S = K[x_1, \\\\ldots, x_n, y_1,\\\\ldots, y_n].$\\n  We classify all generalized block graphs that admit unique extremal Betti number. Also, we prove that the Castelnuovo-Mumford regularity of binomial edge ideal of generalized block graph is bounded below by $m(G)+1$, where $m(G)$ is the number of minimal cut set.',\n",
       " 'Hydrodynamic collapse of a central air-cavity during the recoil phase of droplet impact on a superhydrophobic sieve leads to satellite-free generation of a single droplet through the sieve. Two modes of cavity formation and droplet ejection was observed and explained. The volume of the generated droplet scales with the pore size. Based on this phenomenon, we propose a new drop-on-demand printing technique. Despite significant advancements in inkjet technology, enhancement in mass-loading and particle-size have been limited due to clogging of the printhead nozzle. By replacing the nozzle with a sieve, we demonstrate printing of nanoparticle suspension with 71% mass-loading. Comparatively large particles of 20 micrometer diameter were dispensed in droplets of 80 micrometer diameter. Printing was performed for surface tension as low as 32 mNm-1 and viscosity as high as 33 mPa-s. In comparison to existing techniques, this new way of printing is widely accessible as it is significantly simple and economical.',\n",
       " 'Quantum Annealing (QA) can be used to quickly obtain near-optimal solutions for Quadratic Unconstrained Binary Optimization (QUBO) problems. In QA hardware, each decision variable of a QUBO should be mapped to one or more adjacent qubits in such a way that pairs of variables defining a quadratic term in the objective function are mapped to some pair of adjacent qubits. However, qubits have limited connectivity in existing QA hardware. This has spurred work on preprocessing algorithms for embedding the graph representing problem variables with quadratic terms in the hardware graph representing qubits adjacencies, such as the Chimera graph in hardware produced by D-Wave Systems.In this paper, we use integer linear programming to search for an embedding of the problem graph into certain classes of minors of the Chimera graph, which we call template embeddings. One of these classes corresponds to complete bipartite graphs, for which we show the limitation of the existing approach based on minimum Odd Cycle Transversals (OCTs). Some of the formulations presented are exact, and thus can be used to certify the absence of a minor embedding. On an extensive test set consisting of random graphs from five different classes of varying size and sparsity, we can embed 38% more graphs than a state-of-the-art OCT-based approach.',\n",
       " 'In this article we obtain an upper bound for the regularity of powers of ideals generated by a homogeneous quadratic sequence in a polynomial ring in terms of regularity of its related ideals and degrees of its generators. As a consequence we compute upper bounds for $reg(J_G^s)$ for some classes of graphs $G$. We generalize a result of Matsuda and Murai to show that the Castelnuovo-Mumford regularity of $J^s_G$ is bounded below by $2s+\\\\ell(G)-1$, where $\\\\ell(G)$ is the longest induced path in any graph $G$. Using these two bounds we compute explicitly the regularity of powers of binomial edge ideals of cycle graphs, star graphs and balloon graphs. Also we give a sharp upper bound for the regularity of powers of almost complete intersection binomial edge ideals.',\n",
       " 'The logarithmic mean-velocity profile is a key experimental and theoretical result in wall-bounded turbulence. Similarly, here we show that the topographic surface emerging between parallel zero-elevation boundaries presents an intermediate region with a logarithmic mean-elevation profile. We use model simulations, which account for growth, erosion, and smoothing processes and give rise to complex topography with channel branching and fractal river networks, as well as data from a physical landscape-evolution experiment. Dimensional and self-similarity arguments are used to corroborate this finding. Our results suggest a universality of the logarithmic scaling in bounded complex systems out of equilibrium, of which landscape topography and turbulence are quintessential examples.',\n",
       " 'Simulations of biological macromolecules play an important role in understanding the physical basis of a number of complex processes such as protein folding. Even with increasing computational power and evolution of specialized architectures, the ability to simulate protein folding at atomistic scales still remains challenging. This stems from the dual aspects of high dimensionality of protein conformational landscapes, and the inability of atomistic molecular dynamics (MD) simulations to sufficiently sample these landscapes to observe folding events. Machine learning/deep learning (ML/DL) techniques, when combined with atomistic MD simulations offer the opportunity to potentially overcome these limitations by: (1) effectively reducing the dimensionality of MD simulations to automatically build latent representations that correspond to biophysically relevant reaction coordinates (RCs), and (2) driving MD simulations to automatically sample potentially novel conformational states based on these RCs. We examine how coupling DL approaches with MD simulations can fold small proteins effectively on supercomputers. In particular, we study the computational costs and effectiveness of scaling DL-coupled MD workflows by folding two prototypical systems, viz., Fs-peptide and the fast-folding variant of the villin head piece protein. We demonstrate that a DL driven MD workflow is able to effectively learn latent representations and drive adaptive simulations. Compared to traditional MD-based approaches, our approach achieves an effective performance gain in sampling the folded states by at least 2.3x. Our study provides a quantitative basis to understand how DL driven MD simulations, can lead to effective performance gains and reduced times to solution on supercomputing resources.',\n",
       " 'We conduct to our knowledge a first measurement study of commercial 5G performance on smartphones by closely examining 5G networks of three carriers (two mmWave carriers, one mid-band carrier) in three U.S. cities. We conduct extensive field tests on 5G performance in diverse urban environments. We systematically analyze the handoff mechanisms in 5G and their impact on network performance. We explore the feasibility of using location and possibly other environmental information to predict the network performance. We also study the app performance (web browsing and HTTP download) over 5G. Our study consumes more than 15 TB of cellular data. Conducted when 5G just made its debut, it provides a \"baseline\" for studying how 5G performance evolves, and identifies key research directions on improving 5G users\\' experience in a cross-layer manner. We have released the data collected from our study (referred to as 5Gophers) at https://fivegophers.umn.edu/www20.',\n",
       " 'A significant barrier to progress in data-driven approaches to building dialog systems is the lack of high quality, goal-oriented conversational data. To help satisfy this elementary requirement, we introduce the initial release of the Taskmaster-1 dataset which includes 13,215 task-based dialogs comprising six domains. Two procedures were used to create this collection, each with unique advantages. The first involves a two-person, spoken \"Wizard of Oz\" (WOz) approach in which trained agents and crowdsourced workers interact to complete the task while the second is \"self-dialog\" in which crowdsourced workers write the entire dialog themselves. We do not restrict the workers to detailed scripts or to a small knowledge base and hence we observe that our dataset contains more realistic and diverse conversations in comparison to existing datasets. We offer several baseline models including state of the art neural seq2seq architectures with benchmark performance as well as qualitative human evaluations. Dialogs are labeled with API calls and arguments, a simple and cost effective approach which avoids the requirement of complex annotation schema. The layer of abstraction between the dialog model and the service provider API allows for a given model to interact with multiple services that provide similar functionally. Finally, the dataset will evoke interest in written vs. spoken language, discourse patterns, error handling and other linguistic phenomena related to dialog system research, development and design.',\n",
       " 'Accessibility--the process of designing for people with disabilities (PWD)--is an important but under-explored challenge in the visualization research community. Without careful attention, and if PWD are not included as equal participants throughout the process, there is a danger of perpetuating a vision-first approach to accessible design that marginalizes the lived experience of disability (e.g., by creating overly simplistic \"sensory translations\" that map visual to non-visual modalities in a one-to-one fashion). In this paper, we present a set of sociotechnical considerations for research in accessible visualization design, drawing on literature in disability studies, tactile information systems, and participatory methods. We identify that using state-of-the-art technologies may introduce more barriers to access than they remove, and that expectations of research novelty may not produce outcomes well-aligned with the needs of disability communities. Instead, to promote a more inclusive design process, we emphasize the importance of clearly communicating goals, following existing accessibility guidelines, and treating PWD as equal participants who are compensated for their specialized skills. To illustrate how these considerations can be applied in practice, we discuss a case study of an inclusive design workshop held in collaboration with the Perkins School for the Blind.',\n",
       " 'Machine learning (ML) techniques are increasingly applied to decision-making and control problems in Cyber-Physical Systems among which many are safety-critical, e.g., chemical plants, robotics, autonomous vehicles. Despite the significant benefits brought by ML techniques, they also raise additional safety issues because 1) most expressive and powerful ML models are not transparent and behave as a black box and 2) the training data which plays a crucial role in ML safety is usually incomplete. An important technique to achieve safety for ML models is \"Safe Fail\", i.e., a model selects a reject option and applies the backup solution, a traditional controller or a human operator for example, when it has low confidence in a prediction.\\n  Data-driven models produced by ML algorithms learn from training data, and hence they are only as good as the examples they have learnt. As pointed in [17], ML models work well in the \"training space\" (i.e., feature space with sufficient training data), but they could not extrapolate beyond the training space. As observed in many previous studies, a feature space that lacks training data generally has a much higher error rate than the one that contains sufficient training samples [31]. Therefore, it is essential to identify the training space and avoid extrapolating beyond the training space. In this paper, we propose an efficient Feature Space Partitioning Tree (FSPT) to address this problem. Using experiments, we also show that, a strong relationship exists between model performance and FSPT score.',\n",
       " 'Point-like motile topological defects control the universal dynamics of diverse two-dimensional active nematics ranging from shaken granular rods to cellular monolayers. A comparable understanding in higher dimensions has yet to emerge. We report the creation of three-dimensional active nematics by dispersing extensile microtubule bundles in a passive colloidal liquid crystal. Light-sheet microscopy reveals the millimeter-scale structure of active nematics with a single bundle resolution and the temporal evolution of the associated nematic director field. The dominant excitations of three-dimensional active nematics are extended charge-neutral disclination loops that undergo complex dynamics and recombination events. These studies introduce a new class of non-equilibrium systems whose turbulent-like dynamics arises from the interplay between internally generated active stresses, the chaotic flows and the topological structure of the constituent defects.',\n",
       " 'This article aims to introduced a new distribution named as extended xgamma (EXg) distribution. This generalization is derived from xgamma distribution (Xg), a special finite mixture of exponential and gamma distributions [see, Sen et al. ($2016$)]. Some important statistical properties, viz., survival characteristics, moments, mean deviation and random number generation have been derived. Further, maximum likelihood estimation for the estimation of the unknown parameters have also been discussed for the complete sample. The application of the proposed model has been illustrated through a real data set and observed that the proposed model might be taken as an better alternative to some well known lifetime distributions.',\n",
       " 'In this article, we calculate the mass shift and decay constant of isospin averaged pseudoscalar ($D^+$,$D^0$) and scalar ($D^+_0$,$D^0_0$) mesons by the magnetic field induced quark and gluon condensates at finite density and temperature of asymmetric nuclear matter. We have calculated the in-medium chiral condensates from the chiral SU(3) mean field model and subsequently used these condensates in QCD Sum Rules (QCDSR) to calculate the effective mass and decay constant of $D$ mesons. Consideration of external magnetic field effects in hot and dense nuclear matter lead to appreciable modification in the masses and decay constants of $D$ mesons. Furthermore, we also studied the effective decay width of higher charmonium states ($Ïˆ(3686),Ïˆ(3770),{{Ï‡_c}_0}(3414),{{Ï‡_c}_2}(3556)$) as a by-product by using $^3P_0$ model which can have an important impact on the yield of $J/Ïˆ$ mesons. The results of present work will be helpful to understand the experimental observables of the heavy ion colliders which aim to produce matter at finite density and moderate temperature.',\n",
       " 'We study the arithmetic circuit complexity of some well-known family of polynomials through the lens of parameterized complexity. Our main focus is on the construction of explicit algebraic branching programs (ABP) for determinant and permanent polynomials of the \\\\emph{rectangular} symbolic matrix in both commutative and noncommutative settings. The main results are:\\n  1. We show an explicit $O^{*}({n\\\\choose {\\\\downarrow k/2}})$-size ABP construction for noncommutative permanent polynomial of $k\\\\times n$ symbolic matrix. We obtain this via an explicit ABP construction of size $O^{*}({n\\\\choose {\\\\downarrow k/2}})$ for $S_{n,k}^*$, noncommutative symmetrized version of the elementary symmetric polynomial $S_{n,k}$.\\n  2. We obtain an explicit $O^{*}(2^k)$-size ABP construction for the commutative rectangular determinant polynomial of the $k\\\\times n$ symbolic matrix.\\n  3. In contrast, we show that evaluating the rectangular noncommutative determinant over rational matrices is $W[1]$-hard.',\n",
       " \"Regulatory compliance is an organization's adherence to laws, regulations, guidelines and specifications relevant to its business. Compliance officers responsible for maintaining adherence constantly struggle to keep up with the large amount of changes in regulatory requirements. Keeping up with the changes entail two main tasks: fetching the regulatory announcements that actually contain changes of interest, and incorporating those changes in the business process. In this paper we focus on the first task, and present a Compliance Change Tracking System, that gathers regulatory announcements from government sites, news sites, email subscriptions; classifies their importance i.e Actionability through a hierarchical classifier, and business process applicability through a multi-class classifier. For these classifiers, we experiment with several approaches such as vanilla classification methods (e.g. Naive Bayes, logistic regression etc.), hierarchical classification methods, rule based approach, hybrid approach with various preprocessing and feature selection methods; and show that despite the richness of other models, a simple hierarchical classification with bag-of-words features works the best for Actionability classifier and multi-class logistic regression works the best for Applicability classifier. The system has been deployed in global delivery centers, and has received positive feedback from payroll compliance officers.\",\n",
       " 'We study here the microscopic deformations of elastic lamellae constituting a superhydrophobic substrate under different wetting conditions of a sessile droplet using electrowetting. The deformation profiles of the lamellae are experimentally evaluated using confocal microscopy. These experimental results are then explained using a variational principle formalism within the framework of linear elasticity. We show that the local deformation profile of a lamella is mainly controlled by the net horizontal component of the capillary forces acting on its top due to the pinned droplet contact line. We also discuss the indirect role of electrowetting in dictating the deformation characteristics of the elastic lamellae. One important conclusion is that the small deflection assumption, which is frequently used in the literature, fails to provide a quantitative description of the experimental results; a full solution of the non-linear governing equation is necessary to describe the experimentally obtained deflection profiles.',\n",
       " 'In the present work, we study the recent collision energy and multiplicity dependence of the charged particle transverse momentum spectra as measured by the ALICE collaboration in $pp$ collisions at $\\\\sqrt{s}$ = 5.02 and 13 TeV using the non-extensive Tsallis distribution and the Boltzmann-Gibbs Blast Wave (BGBW) model. A thermodynamically consistent form of the Tsallis distribution is used to extract the kinetic freeze-out parameters from the transverse momentum spectra of charged particles at mid-rapidity. In addition, a comprehensive study of fitting range dependence of transverse momentum spectra on the freeze-out parameters is done using Tsallis statistics. The applicability of BGBW model is verified by fitting the transverse momentum spectra of the bulk part ($\\\\sim 2.5~ {\\\\rm GeV}/c$)for both 5.02 and 13 TeV energies and also in different multiplicity classes. The radial flow, $<Î²>$ is almost independent of collision energy and multiplicity whereas the behavior of kinetic freeze-out temperature significantly depends on multiplicity classes. It is found that the Tsallis distribution generally leads to a better description for the complete transverse momentum spectra whereas the BGBW model explains the bulk part of the system.',\n",
       " \"We present a study of the properties of Bargmann Invariants (BI) and Null Phase Curves (NPC) in the theory of the geometric phase for finite dimensional systems. A recent suggestion to exploit the Majorana theorem on symmetric SU(2) multispinors is combined with the Schwinger oscillator operator construction to develop efficient operator based methods to handle these problems. The BI is described using intrinsic unitary invariant angle parameters, whose algebraic properties as functions of Hilbert space dimension are analysed using elegant group theoretic methods. The BI-geometric phase connection, extended by the use of NPC's, is explored in detail, and interesting new experiments in this subject are pointed out.\",\n",
       " 'Significant progress in computer hardware and software have enabled molecular dynamics (MD) simulations to model complex biological phenomena such as protein folding. However, enabling MD simulations to access biologically relevant timescales (e.g., beyond milliseconds) still remains challenging. These limitations include (1) quantifying which set of states have already been (sufficiently) sampled in an ensemble of MD runs, and (2) identifying novel states from which simulations can be initiated to sample rare events (e.g., sampling folding events). With the recent success of deep learning and artificial intelligence techniques in analyzing large datasets, we posit that these techniques can also be used to adaptively guide MD simulations to model such complex biological phenomena. Leveraging our recently developed unsupervised deep learning technique to cluster protein folding trajectories into partially folded intermediates, we build an iterative workflow that enables our generative model to be coupled with all-atom MD simulations to fold small protein systems on emerging high performance computing platforms. We demonstrate our approach in folding Fs-peptide and the $Î²Î²Î±$ (BBA) fold, FSD-EY. Our adaptive workflow enables us to achieve an overall root-mean squared deviation (RMSD) to the native state of 1.6$~Ã…$ and 4.4~$Ã…$ respectively for Fs-peptide and FSD-EY. We also highlight some emerging challenges in the context of designing scalable workflows when data intensive deep learning techniques are coupled to compute intensive MD simulations.',\n",
       " 'An emerging generation of visualization authoring systems support expressive information visualization without textual programming. As they vary in their visualization models, system architectures, and user interfaces, it is challenging to directly compare these systems using traditional evaluative methods. Recognizing the value of contextualizing our decisions in the broader design space, we present critical reflections on three systems we developed -- Lyra, Data Illustrator, and Charticulator. This paper surfaces knowledge that would have been daunting within the constituent papers of these three systems. We compare and contrast their (previously unmentioned) limitations and trade-offs between expressivity and learnability. We also reflect on common assumptions that we made during the development of our systems, thereby informing future research directions in visualization authoring systems.',\n",
       " 'Dark patterns are user interface design choices that benefit an online service by coercing, steering, or deceiving users into making unintended and potentially harmful decisions. We present automated techniques that enable experts to identify dark patterns on a large set of websites. Using these techniques, we study shopping websites, which often use dark patterns to influence users into making more purchases or disclosing more information than they would otherwise. Analyzing ~53K product pages from ~11K shopping websites, we discover 1,818 dark pattern instances, together representing 15 types and 7 broader categories. We examine these dark patterns for deceptive practices, and find 183 websites that engage in such practices. We also uncover 22 third-party entities that offer dark patterns as a turnkey solution. Finally, we develop a taxonomy of dark pattern characteristics that describes the underlying influence of the dark patterns and their potential harm on user decision-making. Based on our findings, we make recommendations for stakeholders including researchers and regulators to study, mitigate, and minimize the use of these patterns.',\n",
       " 'Let $G$ be a finite simple graph and $I(G)$ denote the corresponding edge ideal in a polynomial ring over a field $\\\\mathbb{K}$. In this paper, we obtain upper bounds for the Castelnuovo-Mumford regularity of symbolic powers of certain classes of edge ideals. We also prove that for several classes of graphs, the regularity of symbolic powers of their edge ideals coincides with that of their ordinary powers.',\n",
       " 'This paper presents the first hardware implementation of the Datagram Transport Layer Security (DTLS) protocol to enable end-to-end security for the Internet of Things (IoT). A key component of this design is a reconfigurable prime field elliptic curve cryptography (ECC) accelerator, which is 238x and 9x more energy-efficient compared to software and state-of-the-art hardware respectively. Our full hardware implementation of the DTLS 1.3 protocol provides 438x improvement in energy-efficiency over software, along with code size and data memory usage as low as 8 KB and 3 KB respectively. The cryptographic accelerators are coupled with an on-chip low-power RISC-V processor to benchmark applications beyond DTLS with up to two orders of magnitude energy savings. The test chip, fabricated in 65 nm CMOS, demonstrates hardware-accelerated DTLS sessions while consuming 44.08 uJ per handshake, and 0.89 nJ per byte of encrypted data at 16 MHz and 0.8 V.',\n",
       " 'The proliferation of ubiquitous computing requires energy-efficient as well as secure operation of modern processors. Side channel attacks are becoming a critical threat to security and privacy of devices embedded in modern computing infrastructures. Unintended information leakage via physical signatures such as power consumption, electromagnetic emission (EM) and execution time have emerged as a key security consideration for SoCs. Also, information published on purpose at user privilege level accessible through software interfaces results in software only attacks. In this paper, we used a supervised learning based approach for inferring applications executing on android platform based on features extracted from EM side-channel emissions and software exposed dynamic voltage frequency scaling(DVFS) states. We highlight the importance of machine learning based approach in utilizing these multi-dimensional features on a complex SoC, against profiling-based approaches. We also show that learning the instantaneous frequency states polled from onboard frequency driver (cpufreq) is adequate to identify a known application and flag potentially malicious unknown application. The experimental results on benchmarking applications running on ARMv8 processor in Snapdragon 820 board demonstrates early detection of these apps, and atleast 85% accuracy in detecting unknown applications. Overall, the highlight is to utilize a low-complexity path to application inference attacks through learning instantaneous frequency states pattern of CPU core.',\n",
       " 'We study the visible photon production from the viscous dissipation of the dark matter (DM) fluid. The visible photon production depends on the magnitude of the dark matter viscosity and becomes important at the late times. We argue that for sufficiently large dark matter viscosity, the number of the resonantly converted visible photons becomes large which populates the Rayleigh-Jeans (RJ) tails of the Cosmic Microwave Background (CMB) radiation. Consequently, these excess visible photons possibly can explain the reported EDGES anomaly in the 21 cm signal. Further, we explore the parameter space for which the 21 cm signal can provide the region to probe the dark radiation and the DM viscosity.',\n",
       " 'Slender filaments clamped at both ends and pre-stressed into a buckled planar shape are known to exhibit stable flapping oscillations in a dissipative medium under nonconservative follower forces. However, beyond a critical pre-stress value clamped rods transition to an out-of-plane (bent and twisted) equilibrium shape in the absence of follower forces. We analyze the nonlinear three-dimensional spatiotemporal dynamics of these three-dimensional pre-stressed shapes under the action of follower forces. In this regime, we find emergence of swirling oscillations with two characteristic time-scales with the first time scale characterizing a purely rotational (swirling) motion around the end-to-end axis of the filament and the second time scale capturing the rate at which the direction of swirling oscillations are reversed. This reversal of swirling oscillations resembles relaxation oscillations where a sudden jump in torsional deformation is followed by a period of gradual decrease in net torsion leading to the next cycle of variations. Our work suggests means by which mechanical deformation mediated by non-conservative follower force may be used to generate oscillatory behavior that can pump or mix fluid at macroscales.',\n",
       " \"Natural environments can present diverse challenges, but some genotypes remain fit across many environments. Such `generalists' can be hard to evolve, out-competed by specialists fitter in any particular environment. Here, inspired by the search for broadly-neutralising antibodies during B-cell affinity maturation, we demonstrate that environmental changes on an intermediate timescale can reliably evolve generalists, even when faster or slower environmental changes are unable to do so. We find that changing environments on timescales comparable to evolutionary transients in a population enhances the rate of evolving generalists from specialists, without enhancing the reverse process. The yield of generalists is further increased in more complex dynamic environments, such as a `chirp' of increasing frequency. Our work offers design principles for how non-equilibrium fitness `seascapes' can dynamically funnel populations to genotypes unobtainable in static environments.\",\n",
       " 'Non-Gaussian operations on two mode squeezed vacuum states (TMSV) in continuous variable measurement device independent quantum key distribution (CV-MDI-QKD) protocols have been shown to effectively increase the total transmission distances drastically. In this paper we show that photon subtraction on a two mode squeezed coherent (PSTMSC) state can further improve the transmission distances remarkably. To that end we also provide a generalized covariance matrix corresponding to PSTMSC, which has not been attempted before. We show that coherence, defined as the amount of displacement of vacuum state, along with non-Gaussianity can help improve the performance of prevalent CV-MDI-QKD protocols. Furthermore, since we use realistic parameters, our technique is experimentally feasible and can be readily implemented',\n",
       " 'Let $p\\\\ge 5$ be a prime, and let $f$ be a cuspidal eigenform of weight at least $2$ and level coprime to $p$ of finite slope $Î±$. Let $\\\\barÏ_f$ denote the mod $p$ Galois representation associated with $f$ and $Ï‰$ the mod $p$ cyclotomic character. Under an assumption on the weight of $f$, we prove that there exists a cuspidal eigenform $g$ of weight at least $2$ and level coprime to $p$ of slope $Î±+1$ such that $$\\\\barÏ_f \\\\otimes Ï‰\\\\simeq \\\\barÏ_g,$$ up to semisimplification. The proof uses Hida-Coleman families and the theta operator acting on overconvergent forms. The structure of the reductions of the local Galois representations associated to cusp forms with slopes in the interval $[0,1)$ were determined by Deligne, Buzzard and Gee and for slopes in $[1,2)$ by Bhattacharya, Ganguli, Ghate, Rai and Rozensztajn. We show that these reductions, in spite of their somewhat complicated behavior, are compatible with the displayed equation above. Moreover, the displayed equation above allows us to predict the shape of the reductions of a class of Galois representations attached to eigenforms of slope larger than $2$. Finally, the methods of this paper allow us to obtain upper bounds on the radii of certain Coleman families.',\n",
       " 'We study the performance of long short-term memory networks (LSTMs) and neural ordinary differential equations (NODEs) in learning latent-space representations of dynamical equations for an advection-dominated problem given by the viscous Burgers equation. Our formulation is devised in a non-intrusive manner with an equation-free evolution of dynamics in a reduced space with the latter being obtained through a proper orthogonal decomposition. In addition, we leverage the sequential nature of learning for both LSTMs and NODEs to demonstrate their capability for closure in systems which are not completely resolved in the reduced space. We assess our hypothesis for two advection-dominated problems given by the viscous Burgers equation. It is observed that both LSTMs and NODEs are able to reproduce the effects of the absent scales for our test cases more effectively than intrusive dynamics evolution through a Galerkin projection. This result empirically suggests that time-series learning techniques implicitly leverage a memory kernel for coarse-grained system closure as is suggested through the Mori-Zwanzig formalism.',\n",
       " 'LTE Narrowband Internet of Things (NB-IoT) is a 3GPP defined cellular technology that is designed to enable connectivity to many low-cost and low power/throughput IoT devices running delay-tolerant applications. NB-IoT can coexist within LTE spectrum either in a standalone mode, in-band with LTE or in guard-band of LTE. With NB-IoT designed to operate in very low signal to noise power ratios, the uplink receiver design presents several challenges. In this paper the design and performance of a NB-IoT uplink receiver is studied in detail. First, receiver design for NB-IoT uplink channels, with corresponding mathematical analysis, is presented. Specifically, it is shown how the time/frequency structure of signals can be exploited to enhance the receiver performance. Second, the performance of each channel is characterized with both link level simulations and implementation on a commercially deployed Qualcomm(registered) FSM(Trade Mark) platform [1]. Comparisons against the 3GPP defined Radio Performance and Protocol aspect requirements are also provided. Finally, implementation details are addressed and discussions on proposed enhancements to NB-IoT in 3GPP Release 15 are provided. It is shown how the proposed receiver algorithms can be adopted to Release 15 enhancements with minor or no modifications. The work in this paper is of significance to system designers looking to implement efficient NB-IoT uplink receiver to coexist with legacy LTE systems.',\n",
       " \"Auto-regressive models are widely used in sequence generation problems. The output sequence is typically generated in a predetermined order, one discrete unit (pixel or word or character) at a time. The models are trained by teacher-forcing where ground-truth history is fed to the model as input, which at test time is replaced by the model prediction. Scheduled Sampling aims to mitigate this discrepancy between train and test time by randomly replacing some discrete units in the history with the model's prediction. While teacher-forced training works well with ML accelerators as the computation can be parallelized across time, Scheduled Sampling involves undesirable sequential processing. In this paper, we introduce a simple technique to parallelize Scheduled Sampling across time. Experimentally, we find the proposed technique leads to equivalent or better performance on image generation, summarization, dialog generation, and translation compared to teacher-forced training. In dialog response generation task, Parallel Scheduled Sampling achieves 1.6 BLEU score (11.5%) improvement over teacher-forcing while in image generation it achieves 20% and 13.8% improvement in Frechet Inception Distance (FID) and Inception Score (IS) respectively. Further, we discuss the effects of different hyper-parameters associated with Scheduled Sampling on the model performance.\",\n",
       " 'The event-shape and multiplicity dependence of chemical freeze-out temperature ($T_{ch}$), freeze-out radius ($R$) and strangeness saturation factor ($Î³_{s}$) are obtained by studying the particle yields obtained from PYTHIA8 event generator in proton+proton (pp) collisions at $\\\\sqrt{s}$ = 13 TeV. Spherocity is one of the transverse event-shape techniques to isolate jetty and isotropic events in high-energy collisions and helps in looking into various observables in more differential manner. In this study, spherocity classes are divided into three categories, namely (i) Spherocity integrated, (ii) Isotropic, and (iii) Jetty. The chemical freeze-out parameters are extracted using a statistical thermal model as a function of spherocity classes and charged particle multiplicity in canonical, strangeness canonical and grand canonical ensembles. A clear observation of multiplicity and spherocity class dependence of $T_{ch}$, $R$ and $Î³_{s}$ is observed. This study plays an important role in understanding the particle production mechanism in high-multiplicity pp collisions at the Large Hadron Collider energies.',\n",
       " 'We consider optimal design of PDE-based Bayesian linear inverse problems with infinite-dimensional parameters. We focus on the A-optimal design criterion, defined as the average posterior variance and quantified by the trace of the posterior covariance operator. We propose using structure exploiting randomized methods to compute the A-optimal objective function and its gradient, and provide a detailed analysis of the error for the proposed estimators. To ensure sparse and binary design vectors, we develop a novel reweighted $\\\\ell_1$-minimization algorithm. We also introduce a modified A-optimal criterion and present randomized estimators for its efficient computation. We present numerical results illustrating the proposed methods on a model contaminant source identification problem, where the inverse problem seeks to recover the initial state of a contaminant plume, using discrete measurements of the contaminant in space and time.',\n",
       " 'We show how to produce antihelium and other antinuclei in large fractions from the decays of a new particle $Ï†$ that carries baryon number. Close to threshold, the production of nuclear bound states is preferred over the decay into individual nucleons, effectively decoupling antihelium and antiproton fluxes and allowing the former to dominate. $Ï†$ can either form dark matter itself or be produced by it, and can give rise to a flux of antihelium that is large enough to explain the preliminary antihelium events reported by AMS-02.',\n",
       " 'In the present work the conjunction of chiral SU(3) model with QCD sum rules is employed to explore the possibility of $Ï$ meson condensation in neutron stars. The quark and gluon condensates in terms of which the in-medium masses of $Ï$ mesons can be expressed are calculated using the chiral SU(3) model in the charge neutral matter which is relevant for neutron stars. It is observed that condition of $Ï$ meson condensation is satisfied for the density of about 7$Ï_{0}$, where $Ï_{0}$ is the nuclear saturation density. In the end, a brief qualitative discussion of the magnetic field is also involved to check out for the further possibility of $Ï$ meson condensation.',\n",
       " 'Quantitative assessment of Tumor-TIL spatial relationships is increasingly important in both basic science and clinical aspects of breast cancer research. We have developed and evaluated convolutional neural network (CNN) analysis pipelines to generate combined maps of cancer regions and tumor infiltrating lymphocytes (TILs) in routine diagnostic breast cancer whole slide tissue images (WSIs). We produce interactive whole slide maps that provide 1) insight about the structural patterns and spatial distribution of lymphocytic infiltrates and 2) facilitate improved quantification of TILs. We evaluated both tumor and TIL analyses using three CNN networks - Resnet-34, VGG16 and Inception v4, and demonstrated that the results compared favorably to those obtained by what believe are the best published methods. We have produced open-source tools and generated a public dataset consisting of tumor/TIL maps for 1,015 TCGA breast cancer images. We also present a customized web-based interface that enables easy visualization and interactive exploration of high-resolution combined Tumor-TIL maps for 1,015TCGA invasive breast cancer cases that can be downloaded for further downstream analyses.',\n",
       " 'Correctly detecting the semantic type of data columns is crucial for data science tasks such as automated data cleaning, schema matching, and data discovery. Existing data preparation and analysis systems rely on dictionary lookups and regular expression matching to detect semantic types. However, these matching-based approaches often are not robust to dirty data and only detect a limited number of types. We introduce Sherlock, a multi-input deep neural network for detecting semantic types. We train Sherlock on $686,765$ data columns retrieved from the VizNet corpus by matching $78$ semantic types from DBpedia to column headers. We characterize each matched column with $1,588$ features describing the statistical properties, character distributions, word embeddings, and paragraph vectors of column values. Sherlock achieves a support-weighted F$_1$ score of $0.89$, exceeding that of machine learning baselines, dictionary and regular expression benchmarks, and the consensus of crowdsourced annotations.',\n",
       " 'Singular value decomposition (SVD) based principal component analysis (PCA) breaks down in the high-dimensional and limited sample size regime below a certain critical eigen-SNR that depends on the dimensionality of the system and the number of samples. Below this critical eigen-SNR, the estimates returned by the SVD are asymptotically uncorrelated with the latent principal components. We consider a setting where the left singular vector of the underlying rank one signal matrix is assumed to be sparse and the right singular vector is assumed to be equisigned, that is, having either only nonnegative or only nonpositive entries. We consider six different algorithms for estimating the sparse principal component based on different statistical criteria and prove that by exploiting sparsity, we recover consistent estimates in the low eigen-SNR regime where the SVD fails. Our analysis reveals conditions under which a coordinate selection scheme based on a \\\\textit{sum-type decision statistic} outperforms schemes that utilize the $\\\\ell_1$ and $\\\\ell_2$ norm-based statistics. We derive lower bounds on the size of detectable coordinates of the principal left singular vector and utilize these lower bounds to derive lower bounds on the worst-case risk. Finally, we verify our findings with numerical simulations and illustrate the performance with a video data example, where the interest is in identifying objects.',\n",
       " 'We construct a theory in which the solution to the strong CP problem is an emergent property of the background of the dark matter in the Universe. The role of the axion degree of freedom is played by multi-body collective excitations similar to spin-waves in the medium of the dark matter of the Galactic halo. The dark matter is a vector particle whose low energy interactions with the Standard Model take the form of its spin density coupled to $G \\\\widetilde{G}$, which induces a potential on the average spin density inducing it to compensate $\\\\overlineÎ¸$, effectively removing CP violation in the strong sector in regions of the Universe with sufficient dark matter density. We discuss the viable parameter space, finding that light dark matter masses within a few orders of magnitude of the fuzzy limit are preferred, and discuss the associated signals with this type of solution to the strong CP problem.',\n",
       " 'Elastically driven filaments subjected to animating compressive follower forces provide a synthetic way to mimic the oscillatory beating of active biological filaments such as eukaryotic cilia. The dynamics of such active filaments can, under favorable conditions, exhibit stable time-periodic responses that result due to the interplay of elastic buckling instabilities, geometric constraints, boundary conditions, and dissipation due to fluid drag. In this paper, we use a continuum elastic rod model to estimate the critical follower force required for the onset of the stable time-periodic flapping oscillations in pre-stressed rods subjected to fluid drag. The pre-stress is generated by imposing either clamped-clamped or clamped-pinned boundary constraints and the results are compared with those of clamped-free case, which is without pre-stress. We find that the critical value increases with the initial slack--that quantifies the pre-stress, and strongly depends on the type of the constraints at the boundaries. The frequency of oscillations far from the onset, however, depends primarily on the magnitude of the follower force, not on the boundary constraints. Interestingly, oscillations for the clamped-pinned case are observed only when the follower forces are directed towards the clamped end. This finding can be exploited to design a mechanical switch to initiate or quench the oscillations by reversing the direction of the follower force or altering the boundary conditions.',\n",
       " 'The collision system and multiplicity dependence of chemical freeze-out temperature ($T_{\\\\rm ch}$) and strangeness saturation factor ($Î³_{s}$) are obtained by studying the particle ratios at the Large Hadron Collider (LHC) energies. Here, we consider the new results in pp at 13 TeV, p+Pb at $\\\\sqrt{s_{\\\\rm NN}}$ = 5.02 TeV, Xe+Xe at $\\\\sqrt{s_{\\\\rm NN}}$ = 5.44 TeV and Pb+Pb at $\\\\sqrt{s_{\\\\rm NN}}$ = 5.02 TeV along with the earlier results in pp at $\\\\sqrt{s}$ = 7 TeV and Pb+Pb at $\\\\sqrt{s_{\\\\rm NN}}$ = 2.76 TeV. A statistical thermal model is used to extract the chemical freeze-out parameters in different multiplicity classes. To understand the particle production from small to large collision systems two ensembles namely, canonical and grand canonical have been considered in this study. A clear observation of multiplicity dependence of $T_{\\\\rm ch}$ and $Î³_{s}$ is observed. The values obtained in high-multiplicity pp collisions are found to be similar to the peripheral Pb+Pb collisions. A final state midrapidity charged particle multiplicity density of around 20-30 appears to be a threshold below which, the chemical freeze-out temperature is lower than the kinetic freeze-out temperature.',\n",
       " 'Recent observations of QGP-like conditions in high-multiplicity pp collisions from ALICE experiment at the LHC warrants an introspection whether to use pp collisions as a baseline measurement to characterize heavy-ion collisions for possible formation of a Quark-Gluon Plasma. A double differential study of the particle spectra and thermodynamics of the produced system as a function of charged-particle multiplicity and transverse spherocity in pp collisions would shed light into the underlying event dynamics. Transverse spherocity, one of the event shape observables, allows to separate the events in terms of jetty and isotropic events. We analyse the identified particle transverse momentum ($p_{\\\\rm T}$) spectra as a function of charged-particle multiplicity and transverse spherocity using Tsallis non-extensive statistics and Boltzmann-Gibbs Blastwave (BGBW) model in pp collisions at $\\\\sqrt{s}$ = 13 TeV using PYTHIA8 event generator. The extracted parameters such as temperature ($T$), radial flow ($Î²$) and non-extensive parameter ($q$) are shown as a function of charged-particle multiplicity for different spherocity classes. We observe that the isotropic events approach to thermal equilibrium while the jetty ones remain far from equilibrium. We argue that, while studying the QGP-like conditions in small systems, one should separate the isotropic events from the spherocity-integrated events, as the production dynamics are different.',\n",
       " 'Many applications in data science and scientific computing involve large-scale datasets that are expensive to store and compute with, but can be efficiently compressed and stored in an appropriate tensor format. In recent years, randomized matrix methods have been used to efficiently and accurately compute low-rank matrix decompositions. Motivated by this success, we focus on developing randomized algorithms for tensor decompositions in the Tucker representation. Specifically, we present randomized versions of two well-known compression algorithms, namely, HOSVD and STHOSVD. We present a detailed probabilistic analysis of the error of the randomized tensor algorithms. We also develop variants of these algorithms that tackle specific challenges posed by large-scale datasets. The first variant adaptively finds a low-rank representation satisfying a given tolerance and it is beneficial when the target-rank is not known in advance. The second variant preserves the structure of the original tensor, and is beneficial for large sparse tensors that are difficult to load in memory. We consider several different datasets for our numerical experiments: synthetic test tensors and realistic applications such as the compression of facial image samples in the Olivetti database and word counts in the Enron email dataset.',\n",
       " \"Computing Nash equilibrium (NE) of multi-player games has witnessed renewed interest due to recent advances in generative adversarial networks. However, computing equilibrium efficiently is challenging. To this end, we introduce the Gradient-based Nikaido-Isoda (GNI) function which serves: (i) as a merit function, vanishing only at the first-order stationary points of each player's optimization problem, and (ii) provides error bounds to a stationary Nash point. Gradient descent is shown to converge sublinearly to a first-order stationary point of the GNI function. For the particular case of bilinear min-max games and multi-player quadratic games, the GNI function is convex. Hence, the application of gradient descent in this case yields linear convergence to an NE (when one exists). In our numerical experiments, we observe that the GNI formulation always converges to the first-order stationary point of each player's optimization problem.\",\n",
       " 'In a recent paper, Ayyer and Behrend present for a wide class of partitions factorizations of Schur polynomials with an even number of variables where half of the variables are the reciprocals of the others into symplectic and/or orthogonal group characters, thereby generalizing results of Ciucu and Krattenthaler for rectangular shapes. Their proofs proceed by manipulations of determinants underlying the characters. The purpose of the current paper is to provide bijective proofs of such factorizations. The quantities involved have known combinatorial interpretations in terms of Gelfand-Tsetlin patterns of various types or half Gelfand-Tsetlin patterns, which can in turn be transformed into perfect matchings of weighted trapezoidal honeycomb graphs. An important ingredient is then Ciucu\\'s theorem for graphs with reflective symmetry. However, before being able to apply it, we need to employ a certain averaging procedure in order to achieve symmetric edge weights. This procedure is based on a \"randomized\" bijection, which can however also be turned into a classical bijection. For one type of Schur polynomial factorization, we also need an additional graph operation that almost doubles the underlying graph. Finally, our combinatorial proofs reveal that the factorizations under consideration can in fact also be generalized to skew shapes as discussed at the end of the paper.',\n",
       " \"Researchers currently rely on ad hoc datasets to train automated visualization tools and evaluate the effectiveness of visualization designs. These exemplars often lack the characteristics of real-world datasets, and their one-off nature makes it difficult to compare different techniques. In this paper, we present VizNet: a large-scale corpus of over 31 million datasets compiled from open data repositories and online visualization galleries. On average, these datasets comprise 17 records over 3 dimensions and across the corpus, we find 51% of the dimensions record categorical data, 44% quantitative, and only 5% temporal. VizNet provides the necessary common baseline for comparing visualization design techniques, and developing benchmark models and algorithms for automating visual analysis. To demonstrate VizNet's utility as a platform for conducting online crowdsourced experiments at scale, we replicate a prior study assessing the influence of user task and data distribution on visual encoding effectiveness, and extend it by considering an additional task: outlier detection. To contend with running such studies at scale, we demonstrate how a metric of perceptual effectiveness can be learned from experimental results, and show its predictive power across test datasets.\",\n",
       " 'We propose a framework to model ferroelectric negative capacitance: electrostatic Micro Electro Mechanical Systems (MEMS) hybrid actuators and analyze their dynamic (step input) response. Using this framework, we report the first proposal for reduction in the dynamic pull-in and pull-out voltages of the hybrid actuators due to the negative capacitance of the ferroelectric. The proposed model also reveals the effect of ferroelectric thickness on the dynamic pull-in and pull-out voltages and the effect of ferroelectric damping on the energy dissipated during actuation. We infer from our analysis that the hybrid actuators are better than the standalone MEMS actuators in terms of operating voltage and energy dissipation. Further, we show that one can trade-off a small part of the reduction in actuation voltage to achieve identical pull-in times in the hybrid and standalone MEMS actuators, while still consuming substantially lower energy in the former as compared to the latter. The circuit compatibility of the proposed hybrid actuator model makes it suitable for analysis and evaluation of various heterogeneous systems consisting of hybrid MEMS actuators and other electronic devices.',\n",
       " 'HrubeÅ¡ and Wigderson [HW14] initiated the study of noncommutative arithmetic circuits with division computing a noncommutative rational function in the free skew field, and raised the question of rational identity testing. It is now known that the problem can be solved in deterministic polynomial time in the white-box model for noncommutative formulas with inverses, and in randomized polynomial time in the black-box model [GGOW16, IQS18, DM18], where the running time is polynomial in the size of the formula. The complexity of identity testing of noncommutative rational functions remains open in general (when the formula size is not polynomially bounded). We solve the problem for a natural special case. We consider polynomial expressions in the free group algebra $\\\\mathbb{F}\\\\langle X, X^{-1}\\\\rangle$ where $X=\\\\{x_1, x_2, \\\\ldots, x_n\\\\}$, a subclass of rational expressions of inversion height one. Our main results are the following. 1. Given a degree $d$ expression $f$ in $\\\\mathbb{F}\\\\langle X, X^{-1}\\\\rangle$ as a black-box, we obtain a randomized $\\\\text{poly}(n,d)$ algorithm to check whether $f$ is an identically zero expression or not. We obtain this by generalizing the Amitsur-Levitzki theorem [AL50] to $\\\\mathbb{F}\\\\langle X, X^{-1}\\\\rangle$. This also yields a deterministic identity testing algorithm (and even an expression reconstruction algorithm) that is polynomial time in the sparsity of the input expression. 2. Given an expression $f$ in $\\\\mathbb{F}\\\\langle X, X^{-1}\\\\rangle$ of degree at most $D$, and sparsity $s$, as black-box, we can check whether $f$ is identically zero or not in randomized $\\\\text{poly}(n,\\\\log s, \\\\log D)$ time.',\n",
       " \"Let $G$ be a simple graph and $I(G)$ be its edge ideal. In this article, we study the Castelnuovo-Mumford regularity of symbolic powers of edge ideals of join of graphs. As a consequence, we prove Minh's conjecture for wheel graphs, complete multipartite graphs and a subclass of co-chordal graphs. We obtain a class of graphs of regularity $3$. By constructing graphs, we prove that multiplicity of an edge ideal is independent with depth, dimension, regularity and degree of $h$-polynomial. Also, we prove that depth of an edge ideal is independent with regularity and degree of $h$-polynomial, by constructing graphs.\",\n",
       " 'The last few years have seen the proliferation of low-power wide area networks like LoRa, Sigfox and 802.11ah, each of which use a different and sometimes proprietary coding and modulation scheme, work below the noise floor and operate on the same frequency band.\\n  We introduce DeepSense, which is the first carrier sense mechanism that enables random access and coexistence for low-power wide area networks even when signals are below the noise floor. Our key insight is that any communication protocol that operates below the noise floor has to use coding at the physical layer. We show that neural networks can be used as a general algorithmic framework that can learn the coding mechanisms being employed by such protocols to identify signals that are hidden within noise. Our evaluation shows that DeepSense performs carrier sense across 26 different LPWAN protocols and configurations below the noise floor and can operate in the presence of frequency shifts as well as concurrent transmissions. Beyond carrier sense, we also show that DeepSense can support multi bit-rate LoRa networks by classifying between 21 different LoRa configurations and flexibly adapting bitrates based on signal strength. In a deployment of a multi-rate LoRa network, DeepSense improves bit rate by 4x for nearby devices and provides a 1.7x increase in the number of locations that can connect to the campus-wide network.',\n",
       " 'The Trigger Data Serializer (TDS) is a custom ASIC designed for the upgrade of the innermost station of the endcap ATLAS Muon Spectrometer. It is a mixed-signal chip with two working modes that can handle up to 128 detector channels. A total of 6,000 TDS ASICs have been produced for detector operation. This paper discusses a custom automatic test platform we developed to provide quality control of the TDS ASICs. We introduce the design, test procedures, and results obtained from this TDS testing platform.',\n",
       " \"It is crucial to reduce natural gas methane emissions, which can potentially offset the climate benefits of replacing coal with gas. Optical gas imaging (OGI) is a widely-used method to detect methane leaks, but is labor-intensive and cannot provide leak detection results without operators' judgment. In this paper, we develop a computer vision approach to OGI-based leak detection using convolutional neural networks (CNN) trained on methane leak images to enable automatic detection. First, we collect ~1 M frames of labeled video of methane leaks from different leaking equipment for building CNN model, covering a wide range of leak sizes (5.3-2051.6 gCH4/h) and imaging distances (4.6-15.6 m). Second, we examine different background subtraction methods to extract the methane plume in the foreground. Third, we then test three CNN model variants, collectively called GasNet, to detect plumes in videos taken at other pieces of leaking equipment. We assess the ability of GasNet to perform leak detection by comparing it to a baseline method that uses optical-flow based change detection algorithm. We explore the sensitivity of results to the CNN structure, with a moderate-complexity variant performing best across distances. We find that the detection accuracy can reach as high as 99%, the overall detection accuracy can exceed 95% for a case across all leak sizes and imaging distances. Binary detection accuracy exceeds 97% for large leaks (~710 gCH4/h) imaged closely (~5-7 m). At closer imaging distances (~5-10 m), CNN-based models have greater than 94% accuracy across all leak sizes. At farthest distances (~13-16 m), performance degrades rapidly, but it can achieve above 95% accuracy to detect large leaks (>950 gCH4/h). The GasNet-based computer vision approach could be deployed in OGI surveys to allow automatic vigilance of methane leak detection with high detection accuracy in the real world.\",\n",
       " 'The elimination of the off-chip frequency reference, typically a crystal oscillator, would bring important benefits in terms of size, price and energy efficiency to IEEE802.15.4 compliant radios and systems-on-chip. The stability of on-chip oscillators is orders of magnitude worse than that of a crystal. It is known that as the temperature changes, they can drift more than 50 ppm/Â°C. This paper presents the result of an extensive experimental study. First, we propose mechanisms for crystal-free radios to be able to track an IEEE802.15.4 join proxy, calibrate the on-chip oscillators and maintain calibration against temperature changes. Then, we implement the resulting algorithms on a crystal-free platform and present the results of an experimental validation. We show that our approach is able to track a crystal-based IEEE802.15.4-compliant join proxy and maintain the requested radio frequency stability of +/-40 ppm, even when subject to temperature variation of 2Â°C/min.',\n",
       " \"SREC markets are a relatively novel market-based system to incentivize the production of energy from solar means. A regulator imposes a floor on the amount of energy each regulated firm must generate from solar power in a given period and provides them with certificates for each generated MWh. Firms offset these certificates against the floor and pay a penalty for any lacking certificates. Certificates are tradable assets, allowing firms to purchase/sell them freely. In this work, we formulate a stochastic control problem for generating and trading in SREC markets from a regulated firm's perspective. We account for generation and trading costs, the impact both have on SREC prices, provide a characterization of the optimal strategy, and develop a numerical algorithm to solve this control problem. Through numerical experiments, we explore how a firm who acts optimally behaves under various conditions. We find that an optimal firm's generation and trading behaviour can be separated into various regimes, based on the marginal benefit of obtaining an additional SREC, and validate our theoretical characterization of the optimal strategy. We also conduct parameter sensitivity experiments and conduct comparisons of the optimal strategy to other candidate strategies.\",\n",
       " 'We investigate the mass-shift of $P$-wave charmonium (${Ï‡_c}_0$, ${Ï‡_c}_1$) and $S$ and $P$-wave bottomonium ($Î·_b$, $Î¥$, ${Ï‡_b}_0$ and ${Ï‡_b}_1$) states in magnetized hot asymmetric nuclear matter using the unification of QCD sum rules (QCDSR) and chiral $SU(3)$ model. Within QCDSR, we use two approaches, $i.e.$, moment sum rule and Borel sum rule. The magnetic field induced scalar gluon condensate $\\\\left\\\\langle \\\\frac{Î±_{s}}Ï€ G^a_{Î¼Î½} {G^a}^{Î¼Î½} \\\\right\\\\rangle$ and the twist-2 gluon operator $\\\\left\\\\langle \\\\frac{Î±_{s}}Ï€ G^a_{Î¼Ïƒ} {{G^a}_Î½}^Ïƒ \\\\right\\\\rangle $ calculated in chiral $SU(3$) model are utilised in QCD sum rules to calculate the in-medium mass-shift of above mesons. The attractive mass-shift of these mesons is observed which is more sensitive to magnetic field in high density regime for charmonium, but less for bottomonium. These results may be helpful to understand the decay of higher quarkonium states to the lower quarkonium states in asymmetric heavy ion collision experiments.',\n",
       " 'Let $G$ be a simple graph on $n$ vertices and $J_G$ denote the corresponding binomial edge ideal in the polynomial ring $S = K[x_1, \\\\ldots, x_n, y_1, \\\\ldots, y_n].$ In this article, we compute the second Betti number and obtain a minimal presentation of trees and unicyclic graphs. We also classify all graphs whose binomial edge ideals are almost complete intersection and we prove that the Rees algebra of their binomial edge ideal is Cohen-Macaulay. We also obtain an explicit description of the defining ideal of the Rees algebra of those binomial edge ideals.',\n",
       " 'Metal-mediated exfoliation has been demonstrated as a promising approach for obtaining large-area flakes of 2D materials to fabricate prototypical nanoelectronics. However, several processing challenges related to organic contamination at the interfaces of the 2D material and the gate oxide must be overcome to realize robust devices with high yield. Here, we demonstrate an optimized process to realize high-performance field-effect transistor (FET) arrays from large-area (~5000 um2) monolayer MoS2 with a yield of 85 %. A central element of this process is an exposed material forming gas anneal (EM-FGA) that results in uniform FET performance metrics (i.e., field-effect mobilities, threshold voltages, and contact performance). Complementary analytical measurements show that the EM-FGA process reduces deleterious channel doping effects by decreasing organic contamination, while also reducing the prevalence of insulating molybdenum oxide, effectively improving the MoS2-gate oxide interface. The uniform FET performance metrics and high device yield achieved by applying the EM-FGA technique on large-area 2D material flakes will help advance the fabrication of complex 2D nanoelectronics devices and demonstrates the need for improved engineering of the 2D material-gate oxide interface.',\n",
       " 'Let $G$ be a simple graph on the vertex set $[n]$ and $J_G$ be the corresponding binomial edge ideal. Let $G=v*H$ be the cone of $v$ on $H$. In this article, we compute all the Betti numbers of $J_G$ in terms of Betti number of $J_H$ and as a consequence, we get the Betti diagram of wheel graph. Also, we study Cohen-Macaulay defect of $S/J_G$ in terms of Cohen-Macaulay defect of $S_H/J_H$ and using this we construct a graph with Cohen-Macaulay defect $q$ for any $q\\\\geq 1$. We obtain the depth of binomial edge ideal of join of graphs. Also, we prove that for any pair $(r,b)$ of positive integers with $1\\\\leq b< r$, there exists a connected graph $G$ such that $reg(S/J_G)=r$ and the number of extremal Betti number of $S/J_G$ is $b$.',\n",
       " 'For real reductive groups, we prove that a morphism of $L$-groups which preserves regular unipotent elements respects cohomological $A$-parameters. This allows us to give a complete understanding of cohomological $A$-parameters for all real classical groups. Along the way we elucidate cohomological $A$-parameters for all real reductive groups. We construct Langlands parameters of tempered cohomological representations of all real reductive groups, and give a complete list of cohomological unitary representations of $GL_n({\\\\mathbb R})$, a result which is due to B. Speh when the coefficient system is trivial. The latter result is crucial for us as we go from cohomological representations of $GL_n({\\\\mathbb R})$ to cohomological representations of classical groups. The paper ends with a section where we prove that the sum of the ranks of cohomology groups in an $A$-packet on any real group (and any infinitesimal character) is independent of the $A$-packet under consideration, and can be explicitly calculated. This result when summed over all pure innerforms has a particularly nice form.',\n",
       " 'We consider models of light dark matter coupled to quarks through a vector current interaction. For low energies, these models must be treated through the effective couplings to mesons, which are implemented here through the chiral Lagrangian. We find the rates of dark matter annihilation and decay to the light mesons, and find the expected photon spectrum from the decay of the hadrons. We compare to current and future observations, and show that there is a significant discovery reach for these models.',\n",
       " 'The existence of contextuality in quantum mechanics is a fundamental departure from the classical description of the world. Currently, the quest to identify scenarios which cannot be more contextual than quantum theory is at the forefront of research in quantum contextuality. In this work, we experimentally test two inequalities, which are capable of revealing fully contextual quantum correlations, on a Hilbert space of dimension 8 and 4 respectively, on an NMR quantum information processor. The projectors associated with the contextuality inequalities are first reformulated in terms of Pauli operators, which can be determined in an NMR experiment. We also analyze the behavior of each inequality under rotation of the underlying quantum state, which unitarily transforms it to another pure state.',\n",
       " 'A Superoscillatory lens (SOL) is known to produce a sub-diffraction hotspot which is useful for high-resolution imaging. However, high-energy rings called sidelobes coexist with the central hotspot. Additionally, SOLs have not yet been directly used to image reflective objects due to low efficiency and poor imaging properties. We propose a novel reflection confocal nanoscope which mitigates these issues by relaying the SOL intensity pattern onto the object and use conventional optics for detection. We experimentally demonstrate super-resolution by imaging double bars with 330 nm separation using a 632.8 nm excitation and a 0.95 NA objective. We also discuss the enhanced contrast properties of the SOL nanoscope against a laser confocal microscope, and the degradation of performance while imaging large objects.',\n",
       " \"Living organisms need to be sensitive to a changing environment while also ignoring uninformative environmental fluctuations. Here, we show that the circadian clock in \\\\textit{Synechococcus elongatus} can naturally tune its environmental sensitivity, through a clock-metabolism coupling quantified in recent experiments. The metabolic coupling can detect mismatch between clock predictions and the day-night light cycle, and temporarily raise the clock's sensitivity to light changes and thus entrain faster. We also analyze analogous behavior in recent experiments on switching between slow and fast osmotic stress response pathways in yeast. In both cases, cells can raise their sensitivity to new external information in epochs of frequent challenging stress, much like a Kalman filter with adaptive gain in signal processing. Our work suggests a new class of experiments that probe the history-dependence of environmental sensitivity in biophysical sensing mechanisms.\",\n",
       " 'Training complex machine learning models in parallel is an increasingly important workload. We accelerate distributed parallel training by designing a communication primitive that uses a programmable switch dataplane to execute a key step of the training process. Our approach, SwitchML, reduces the volume of exchanged data by aggregating the model updates from multiple workers in the network. We co-design the switch processing with the end-host protocols and ML frameworks to provide a robust, efficient solution that speeds up training by up to 300%, and at least by 20% for a number of real-world benchmark models.',\n",
       " 'This paper presents a reconfigurable cryptographic engine that implements the DTLS protocol to enable end-to-end security for IoT. This implementation of the DTLS engine demonstrates 10x reduction in code size and 438x improvement in energy-efficiency over software. Our ECC primitive is 237x and 9x more energy-efficient compared to software and state-of-the-art hardware respectively. Pairing the DTLS engine with an on-chip RISC-V allows us to demonstrate applications beyond DTLS with up to 2 orders of magnitude energy savings.',\n",
       " \"Geometric deep learning provides a principled and versatile manner for the integration of imaging and non-imaging modalities in the medical domain. Graph Convolutional Networks (GCNs) in particular have been explored on a wide variety of problems such as disease prediction, segmentation, and matrix completion by leveraging large, multimodal datasets. In this paper, we introduce a new spectral domain architecture for deep learning on graphs for disease prediction. The novelty lies in defining geometric 'inception modules' which are capable of capturing intra- and inter-graph structural heterogeneity during convolutions. We design filters with different kernel sizes to build our architecture. We show our disease prediction results on two publicly available datasets. Further, we provide insights on the behaviour of regular GCNs and our proposed model under varying input scenarios on simulated data.\",\n",
       " 'The Dynamic Mode Decomposition (DMD) extracted dynamic modes are the non-orthogonal eigenvectors of the matrix that best approximates the one-step temporal evolution of the multivariate samples. In the context of dynamical system analysis, the extracted dynamic modes are a generalization of global stability modes. We apply DMD to a data matrix whose rows are linearly independent, additive mixtures of latent time series. We show that when the latent time series are uncorrelated at a lag of one time-step then, in the large sample limit, the recovered dynamic modes will approximate, up to a column-wise normalization, the columns of the mixing matrix. Thus, DMD is a time series blind source separation algorithm in disguise, but is different from closely related second order algorithms such as the Second-Order Blind Identification (SOBI) method and the Algorithm for Multiple Unknown Signals Extraction (AMUSE). All can unmix mixed stationary, ergodic Gaussian time series in a way that kurtosis-based Independent Components Analysis (ICA) fundamentally cannot. We use our insights on single lag DMD to develop a higher-lag extension, analyze the finite sample performance with and without randomly missing data, and identify settings where the higher lag variant can outperform the conventional single lag variant. We validate our results with numerical simulations, and highlight how DMD can be used in change point detection.',\n",
       " 'Discrete empirical interpolation method (DEIM) is a popular technique for nonlinear model reduction and it has two main ingredients: an interpolating basis that is computed from a collection of snapshots of the solution and a set of indices which determine the nonlinear components to be simulated. The computation of these two ingredients dominates the overall cost of the DEIM algorithm. To specifically address these two issues, we present randomized versions of the DEIM algorithm. There are three main contributions of this paper. First, we use randomized range finding algorithms to efficiently find an approximate DEIM basis. Second, we develop randomized subset selection tools, based on leverage scores, to efficiently select the nonlinear components. Third, we develop several theoretical results that quantify the accuracy of the randomization on the DEIM approximation. We also present numerical experiments that demonstrate the benefits of the proposed algorithms.',\n",
       " 'In this paper, we compute the regularity and Hilbert series of symbolic powers of cover ideal of a graph $G$ when $G$ is either a crown graph or a complete multipartite graph. We also compute the multiplicity of symbolic powers of cover ideals in terms of the number of edges.',\n",
       " 'High-fidelity modeling of turbulent flows is one of the major challenges in computational physics, with diverse applications in engineering, earth sciences and astrophysics, among many others. The rising popularity of high-fidelity computational fluid dynamics (CFD) techniques like direct numerical simulation (DNS) and large eddy simulation (LES) have made significant inroads into the problem. However, they remain out of reach for many practical three-dimensional flows characterized by extremely large domains and transient phenomena. Therefore designing efficient and accurate data-driven generative approaches to model turbulence is a necessity. We propose a novel training approach for dimensionality reduction and spatio-temporal modeling of the three-dimensional dynamics of turbulence using a combination of Convolutional autoencoder and the Convolutional LSTM neural networks. The quality of the emulated turbulent fields is assessed with rigorous physics-based statistical tests, instead of visual assessments. The results show significant promise in the training methodology to generate physically consistent turbulent flows at a small fraction of the computing resources required for DNS.',\n",
       " \"We have demonstrated atomically thin, quantum capacitance-limited, field-effect transistors (FETs) that enable the detection of pH changes with ~75-fold higher sensitivity (4.4 V/pH) over the Nernst value of 59 mV/pH at room temperature when used as a biosensor. The transistors, which are fabricated from a monolayer of MoS2 with a room temperature ionic liquid (RTIL) in place of a conventional oxide gate dielectric, exhibit very low intrinsic noise resulting in a pH limit of detection (LOD) of 92x10^-6 at 10 Hz. This high device performance, which is a function of the structure of our device, is achieved by remotely connecting the gate to a pH sensing element allowing the FETs to be reused. Because pH measurements are fundamentally important in biotechnology, the low limit of detection demonstrated here will benefit numerous applications ranging from pharmaceutical manufacturing to clinical diagnostics. As an example, we experimentally quantified the function of the kinase Cdk5, an enzyme implicated in Alzheimer's disease, at concentrations that are 5-fold lower than physiological values, and with sufficient time-resolution to allow the estimation of both steady-state and kinetic parameters in a single experiment. The high sensitivity, low LOD and fast turnaround time of the measurements will allow the development of early diagnostic tools and novel therapeutics to detect and treat neurological conditions years before currently possible.\",\n",
       " \"This paper addresses the problem of constructing secure exact-repair regenerating codes at the MSR point for all feasible values of the parameters. The setting involves a passive eavesdropper who is allowed to observe the stored contents of, and the downloads into, an $l$-subset of the $n$ nodes of a distributed storage system (DSS). The objective is to achieve perfect secrecy between the eavesdropped symbols and the file stored on the DSS. Previous secure code constructions (most notably that by Rawat et al.) tackle the problem only for the restricted case wherein the number, $d$, of helper nodes aiding in the recovery of a failed node is equal to $n-1$. This paper builds on Rawat's work, by combining Gabidulin pre-coding and an MSR construction by Ye and Barg to prove the achievability of secrecy capacity at the MSR point for all allowed values of $d$.\",\n",
       " 'We contrast the distinct frameworks of materials design and physical learning in creating elastic networks with desired stable states. In design, the desired states are specified in advance and material parameters can be optimized on a computer with this knowledge. In learning, the material physically experiences the desired stable states in sequence, changing the material so as to stabilize each additional state. We show that while designed states are stable in networks of linear Hookean springs, sequential learning requires specific non-linear elasticity. We find that such non-linearity stabilizes states in which strain is zero in some springs and large in others, thus playing the role of Bayesian priors used in sparse statistical regression. Our model shows how specific material properties allow continuous learning of new functions through deployment of the material itself.',\n",
       " 'Motor proteins drive persistent motion and self-organisation of cytoskeletal filaments. However, state-of-the-art microscopy techniques and continuum modelling approaches focus on large length and time scales. Here, we perform component-based computer simulations of polar filaments and molecular motors linking microscopic interactions and activity to self-organisation and dynamics from the two-filament level up to the mesoscopic domain level. Dynamic filament crosslinking and sliding, and excluded-volume interactions promote formation of bundles at small densities, and of active polar nematics at high densities. A buckling-type instability sets the size of polar domains and the density of topological defects. We predict a universal scaling of the active diffusion coefficient and the domain size with activity, and its dependence on parameters like motor concentration and filament persistence length. Our results provide a microscopic understanding of cytoplasmic streaming in cells and help to develop design strategies for novel engineered active materials.',\n",
       " 'We consider a finite one-dimensional totally asymmetric simple exclusion process (TASEP) with four types of particles, $\\\\{1,0,\\\\bar{1},*\\\\}$, in contact with reservoirs. Particles of species $0$ can neither enter nor exit the lattice, and those of species $*$ are constrained to lie at the first and last site. Particles of species $1$ enter from the left reservoir into either the first or second site, move rightwards, and leave from either the last or penultimate site. Conversely, particles of species $\\\\bar{1}$ enter from the right reservoir into either the last or penultimate site, move leftwards, and leave from either the first or last site. This dynamics is motivated by a natural random walk on the Weyl group of type D. We compute the exact nonequilibrium steady state distribution using a matrix ansatz building on earlier work of Arita. We then give explicit formulas for the nonequilibrium partition function as well as densities and currents of all species in the steady state, and derive the phase diagram.',\n",
       " 'The EDGES experiment has detected the global absorption signal of 21 cm line at $z\\\\sim17$ in cosmic dawn era and reported its amplitude larger than the standard cosmological prediction. One of the possible explanation requires that the baryons were much cooler than the standard scenario. This requires an interaction between the dark and baryonic sectors with some appropriate cross-section, $ \\\\hatÏƒ $. In this work, we examine the role that dissipative effects of cosmic fluid might play in influencing the 21 cm signal. We show that the presence of viscous dissipation of dark matter can significantly affect the energy transfer between the baryonic and dark matter fluids. It is demonstrated that the inclusion of the dissipative mechanism in the dark sector, strongly modify the earlier constraints on dark matter mass and $ \\\\hatÏƒ $ obtain from EDGES observation. Further, we argue that EDGES absorption signal can put an independent bound on dark matter viscosity which is many order of magnitude larger than the maximum viscosity allowed by the structure formation.',\n",
       " 'In this paper we advance a previously developed bistatic scattering forward model to include the circularly polarized incident and scattered waves, which is the case for Global Navigation Satellite System (GNSS) reflectometry. This model development enables retrieval of soil moisture from Signals of Opportunity (SoOp) bistatic observations, e.g., from the Cyclone Global Navigation Satellite System (CYGNSS) observations and GNSS Reflectometer Instrument for Bistatic Synthetic Aperture Radar (GRIBSAR). In order to validate the forward model with measured data, we present a method to construct the Delay Doppler Map (DDM) from simulations of the forward model. The forward model Radar Cross Section (RCS) predictions will be compared with actual measurements. The validated model is intended for use in soil moisture retrievals.',\n",
       " 'In this work, we experimentally created and characterized a class of qubit-ququart PPT (positive under partial transpose) entangled states using three nuclear spins on an nuclear magnetic resonance (NMR) quantum information processor. Entanglement detection and characterization for systems with a Hilbert space dimension $\\\\ge 2 \\\\otimes 3$ is nontrivial since there are states in such systems which are both PPT as well as entangled. The experimental detection scheme that we devised for the detection of qubit-ququart PPT entanglement was based on the measurement of three Pauli operators with high precision, and is a key ingredient of the protocol in detecting entanglement. The family of PPT-entangled states considered in the current study are incoherent mixtures of five pure states. All the five states were prepared with high fidelities and the resulting PPT entangled states were prepared with mean fidelity $\\\\ge 0.95$. The entanglement thus detected was validated by carrying out full quantum state tomography (QST).',\n",
       " \"We introduce a new one-dimensional discrete dynamical system reminiscent of mathematical billiards that arises in the study of two-move riders, a type of fairy chess piece. In this model, particles travel through a bounded convex region along line segments of one of two fixed slopes.\\n  This dynamical system applies to characterize the vertices of the inside-out polytope arising from counting placements of nonattacking chess pieces and also to give a bound for the period of the counting quasipolynomial. The analysis focuses on points of the region that are on trajectories that contain a corner or on cycles of full rank, or are crossing points thereof.\\n  As a consequence, we give a simple proof that the period of the bishops' counting quasipolynomial is 2, and provide formulas bounding periods of counting quasipolynomials for many two-move riders including all partial nightriders. We draw parallels to the theory of mathematical billiards and pose many new open questions.\",\n",
       " 'This report describes eighteen projects that explored how commercial cloud computing services can be utilized for scientific computation at national laboratories. These demonstrations ranged from deploying proprietary software in a cloud environment to leveraging established cloud-based analytics workflows for processing scientific datasets. By and large, the projects were successful and collectively they suggest that cloud computing can be a valuable computational resource for scientific computation at national laboratories.',\n",
       " 'Remote detection of the cardiac pulse has a number of applications in sports and medicine, and can be used to determine the physiological state of the subject. Previous approaches to estimate Heart Rate from video require the subject to remain stationary and employ background information to eliminate illumination interferences. The present research proposes a spectral reflectance based novel illumination rectification method to eliminate illumination variations in the video. Our method does not rely on the background of the video and is robust to extreme motion interferences (head movements). Furthermore, in order to tackle extreme motion artifacts, the present framework introduces a novel feature point recovery system which recovers the feature tracking points lost during extreme head movements of the subject. Finally, the individual HR estimates from multiple feature points are combined to produce an average HR. We evaluate the efficacy of our framework on the MAHNOB HCI dataset, a publicly available dataset employed by previous methods. Our HR measurement framework outperformed previous methods and had a root mean square error of 5.21%.',\n",
       " 'In this work, we present expressions for radiative heat transfer between pairs of spheres in a linear chain and between individual spheres and their environment. The expressions are valid for coated spheres of arbitrary size, spacing, and isotropic optical properties. The spheres may be small and closely-spaced, which violates the assumptions foundational to classical radiative transfer. We validate our results against existing formulations of radiative heat transfer, namely the thermal discrete dipole and boundary element methods. Our results have important implications for the modeling and interpretation of near-field radiative heat transfer experiments between spherical bodies.',\n",
       " 'Multi-modal data comprising imaging (MRI, fMRI, PET, etc.) and non-imaging (clinical test, demographics, etc.) data can be collected together and used for disease prediction. Such diverse data gives complementary information about the patientÅ› condition to make an informed diagnosis. A model capable of leveraging the individuality of each multi-modal data is required for better disease prediction. We propose a graph convolution based deep model which takes into account the distinctiveness of each element of the multi-modal data. We incorporate a novel self-attention layer, which weights every element of the demographic data by exploring its relation to the underlying disease. We demonstrate the superiority of our developed technique in terms of computational speed and performance when compared to state-of-the-art methods. Our method outperforms other methods with a significant margin.',\n",
       " 'Recent attacks have broken process isolation by exploiting microarchitectural side channels that allow indirect access to shared microarchitectural state. Enclaves strengthen the process abstraction to restore isolation guarantees.\\n  We propose MI6, an aggressive, speculative out-of-order processor capable of providing secure enclaves under a threat model that includes an untrusted OS and an attacker capable of mounting any software attack currently considered practical, including control flow speculation attacks. MI6 is inspired by Sanctum [16] and extends its isolation guarantee to more realistic memory hierarchies. It also introduces a purge instruction, which is used only when a secure process is scheduled, and implements it for a complex processor microarchitecture. We model the performance impact of enclaves in MI6 through FPGA emulation on AWS F1 FPGAs by running SPEC CINT2006 benchmarks on top of an untrusted Linux OS. Security comes at the cost of approximately 16.4% average slowdown for protected programs.',\n",
       " 'Virtual execution environments allow for consolidation of multiple applications onto the same physical server, thereby enabling more efficient use of server resources. However, users often statically configure the resources of virtual machines through guesswork, resulting in either insufficient resource allocations that hinder VM performance, or excessive allocations that waste precious data center resources. In this paper, we first characterize real-world resource allocation and utilization of VMs through the analysis of an extensive dataset, consisting of more than 250k VMs from over 3.6k private enterprise clusters. Our large-scale analysis confirms that VMs are often misconfigured, either overprovisioned or underprovisioned, and that this problem is pervasive across a wide range of private clusters. We then propose ADARES, an adaptive system that dynamically adjusts VM resources using machine learning techniques. In particular, ADARES leverages the contextual bandits framework to effectively manage the adaptations. Our system exploits easily collectible data, at the cluster, node, and VM levels, to make more sensible allocation decisions, and uses transfer learning to safely explore the configurations space and speed up training. Our empirical evaluation shows that ADARES can significantly improve system utilization without sacrificing performance. For instance, when compared to threshold and prediction-based baselines, it achieves more predictable VM-level performance and also reduces the amount of virtual CPUs and memory provisioned by up to 35% and 60% respectively for synthetic workloads on real clusters.',\n",
       " 'The proliferation of smart home Internet of Things (IoT) devices presents unprecedented challenges for preserving privacy within the home. In this paper, we demonstrate that a passive network observer (e.g., an Internet service provider) can infer private in-home activities by analyzing Internet traffic from commercially available smart home devices even when the devices use end-to-end transport-layer encryption. We evaluate common approaches for defending against these types of traffic analysis attacks, including firewalls, virtual private networks, and independent link padding, and find that none sufficiently conceal user activities with reasonable data overhead. We develop a new defense, \"stochastic traffic padding\" (STP), that makes it difficult for a passive network adversary to reliably distinguish genuine user activities from generated traffic patterns designed to look like user interactions. Our analysis provides a theoretical bound on an adversary\\'s ability to accurately detect genuine user activities as a function of the amount of additional cover traffic generated by the defense technique.',\n",
       " \"Every irreducible odd dimensional representation of the $n$'th symmetric or hyperoctahedral group, when restricted to the $(n-1)$'th, has a unique irreducible odd-dimensional constituent. Furthermore, the subgraph induced by odd-dimensional representations in the Bratteli diagram of symmetric and hyperoctahedral groups is a binary tree with a simple recursive description. We survey the description of this tree, known as the Macdonald tree, for symmetric groups, from our earlier work. We describe analogous results for hyperoctahedral groups.\\n  A partition $Î»$ of $n$ is said to be chiral if the corresponding irreducible representation $V_Î»$ of $S_n$ has non-trivial determinant. We review our previous results on the structure and enumeration of chiral partitions, and subsequent extension to all Coxeter groups by Ghosh and Spallone. Finally we show that the numbers of odd and chiral partitions track each other closely.\",\n",
       " 'We examine the phenomenology of the majoron portal: a simplified model of fermionic dark matter coupled to a light scalar mediator carrying lepton number 2. We find that the mediator can be very light and still consistent with laboratory and cosmological bounds. This model satisfies the thermal relic condition for natural values of dimensionless coupling constants and admits a mediator in the $10 - 100 ~\\\\text{MeV}$ mass range favored by small scale structure observations. As such, this model provides an excellent candidate for self-interacting dark matter.',\n",
       " 'The magnetic moments of baryon decuplet are studied in vacuum as well as in the symmetric nuclear matter at finite temperature using a chiral SU(3) quark mean field model approach. The contributions coming from the valence quarks, quark sea and the orbital angular momentum of the quark sea have been considered to calculate magnetic moment of decuplet baryons. The decuplet baryon masses decrease, whereas, the magnetic moments increase significantly with the rise of baryonic density of the symmetric nuclear medium. This is because of the reason that constituent quark magnetic moment and the quark spin polarizations show considerable variation in the nuclear medium especially in the low temperature and baryonic density regime.\\n  The increase is however quantitatively less as compared to the case of octet baryon members.',\n",
       " 'The $k$-dimensional Weisfeiler-Leman algorithm ($k$-WL) is a fruitful approach to the Graph Isomorphism problem. 2-WL corresponds to the original algorithm suggested by Weisfeiler and Leman over 50 years ago. 1-WL is the classical color refinement routine. Indistinguishability by $k$-WL is an equivalence relation on graphs that is of fundamental importance for isomorphism testing, descriptive complexity theory, and graph similarity testing which is also of some relevance in artificial intelligence. Focusing on dimensions $k=1,2$, we investigate subgraph patterns whose counts are $k$-WL invariant, and whose occurrence is $k$-WL invariant. We achieve a complete description of all such patterns for dimension $k=1$ and considerably extend the previous results known for $k=2$.',\n",
       " 'Small system collectivity observed at the LHC energies along with enhancement of strangeness makes high-multiplicity proton+proton collisions very interesting in order to look for QGP-like features, usually found in heavy-ion collisions. It may be interesting to perform a double differential study of different observables in pp collisions in terms of charged particle multiplicity and event shape in order to understand the new dimensions in high-multiplicity pp physics. We study the correlation between the number of multi-partonic interactions (nMPI), event shape (transverse spherocity) and charged particle multiplicity classes. We report the simulation results on the spherocity and multiplicity dependent study of identified particle production in pp collisions at $\\\\sqrt{s}$= 13 TeV using PYTHIA 8. We explore the event shape dependence of the transverse momentum ($p_{\\\\rm{T}}$) spectra, integrated yield, mean transverse momentum ($\\\\langle p_{\\\\rm{T}} \\\\rangle$) and particle ratios of the identified particles. A clear spherocity dependence of $p_{\\\\rm{T}}$-spectra is observed for all the particles. The $p_{\\\\rm T}$-crossing point of the ratios of jetty and isotropic events to the spherocity-integrated ones, depend on the multiplicity classes. The p/$Ï†$ ratio at low-$p_{\\\\rm T}$ shows a weak $p_{\\\\rm T}$ dependence for high-multiplicity isotropic events, which is a hydrodynamic-like behavior. Larger dependence of integrated yield on spherocity is observed for high multiplicity compared to the low multiplicity pp collisions. However, the $p_{\\\\rm{T}}$-integrated particle ratio shows less dependence on spherocity which suggests that, the relative increase in integrated yield for different particles as a function of spherocity are similar.',\n",
       " \"Hierarchical models in Bayesian inverse problems are characterized by an assumed prior probability distribution for the unknown state and measurement error precision, and hyper-priors for the prior parameters. Combining these probability models using Bayes' law often yields a posterior distribution that cannot be sampled from directly, even for a linear model with Gaussian measurement error and Gaussian prior. Gibbs sampling can be used to sample from the posterior, but problems arise when the dimension of the state is large. This is because the Gaussian sample required for each iteration can be prohibitively expensive to compute, and because the statistical efficiency of the Markov chain degrades as the dimension of the state increases. The latter problem can be mitigated using marginalization-based techniques, but these can be computationally prohibitive as well. In this paper, we combine the low-rank techniques of Brown, Saibaba, and Vallelian (2018) with the marginalization approach of Rue and Held (2005). We consider two variants of this approach: delayed acceptance and pseudo-marginalization. We provide a detailed analysis of the acceptance rates and computational costs associated with our proposed algorithms, and compare their performances on two numerical test cases---image deblurring and inverse heat equation.\",\n",
       " 'Context. Being the most numerous and oldest stars in the galaxy, M dwarfs are objects of great interest for exoplanet searches. The presence of molecules in their atmosphere complicates our understanding of their atmospheric properties. But great advances have recently been made in the modeling of M dwarfs due to the revision of solar abundances. Aims. We aim to determine stellar parameters of M dwarfs using high resolution spectra (R = 90 000) simultaneously in the visible and the near-infrared. The high resolution spectra and broad wavelength coverage provide an unique opportunity to understand the onset of dust and cloud formation at cool temperatures. Furthermore, this study will help in understanding the physical processes which occur in a cool atmospheres, particularly, the redistribution of energy from the optical to the near-infrared. Methods. The stellar parameters of M dwarfs in our sample have been determined by comparing the high resolution spectra both in the optical and in the near-infrared simultaneously observed by CARMENES with the synthetic spectra obtained from the BT-Settl model atmosphere. The detailed spectral synthesis of these observed spectra both in the optical and in the near-infrared helps to understand the missing continuum opacity. Results. For the first time, we derive fundamental stellar parameters of M dwarfs using the high resolution optical and near-infrared spectra simultaneously. We determine Teff , log g and [M/H] for 292 M dwarfs of spectral type M0 to M9, where the formation of dust and clouds are important. The derived Teff for the sample ranges from 2300 to 4000 K, values of log g ranges from 4.5 geq log g leq 5.5 and the resulting metallicity ranges from -0.5 geq [M/H] leq +0.5. We have also explored the possible differences in Teff , log g and [M/H] by comparing them with other studies of the same sample of M dwarfs.',\n",
       " 'The viscosity of dark matter in cosmological models may cause an accelerated expansion and when this effect is sufficiently large, it can explain the dark energy. In this work, attributing the origin of viscosity to self-interaction of dark matter, we study the viscous cosmology at small redshift $(0\\\\leq z\\\\leq2.5)$. Assuming the cluster scale to be virialized and by modeling a power law behavior of velocity gradients, we calculate the Hubble expansion rate, $H(z)$ and the deceleration parameter, $q(z)$. We then perform a $Ï‡^{2}$ analysis to estimate the best fit model parameters. By using the best fit values, we explain the cosmic chronometer and type Ia supernova data. We conclude that if the dissipative effects become prominent only at the late time of cosmic evolution and are smaller at higher redshift, we can explain the observational data without requiring any dark energy component. Our analysis is independent of any specific model of self interacting dark matter.',\n",
       " 'The magnetic moment of the $Ï„$ lepton is an interesting quantity that is potentially sensitive to physics beyond the Standard Model. Electroweak gauge invariance implies that a heavy new physics contribution to it takes the form of an operator which involves the Higgs boson, implying that rare Higgs decays are able to probe the same physics as $a_Ï„$. We examine the prospects for rare Higgs decays at future high energy lepton (electron or muon) colliders, and find that such a project collecting a few ab$^{-1}$ would be able to advance our understanding of this physics by roughly a factor of 10 compared to the expected reach of the high luminosity LHC.',\n",
       " 'We investigate the mass-shift of vector channel $J/Ïˆ$ and pseudoscalar channel $Î·_c$ in strongly magnetized asymmetric nuclear medium at finite temperature using the conjunction of chiral $SU(3)$ model with the QCD sum rules. The magnetic field dependence of scalar gluon condensate $\\\\left\\\\langle \\\\frac{Î±_{s}}Ï€ G^a_{Î¼Î½} {G^a}^{Î¼Î½} \\\\right\\\\rangle$ as well as the twist-2 gluon condensate $\\\\left\\\\langle \\\\frac{Î±_{s}}Ï€ G^a_{Î¼Ïƒ} {{G^a}_Î½}^Ïƒ \\\\right\\\\rangle $ calculated from chiral SU($3$) model, are implemented in QCD sum rules to calculate the magnetic field dependence of $J/Ïˆ$ and $Î·_c$ meson masses. The effects of constant external magnetic field at finite density and temperature of the medium are found to be appreciable in symmetric and asymmetric nuclear matter. The results of the present investigation may be helpful to understand the experimental observables arising from the Compressed Baryonic Matter (CBM) produced in asymmetric non-central heavy ion collision experiments.',\n",
       " 'We consider the flow network model to solve the multiprocessor real-time task scheduling problems. Using the flow network model or its generic form, linear programming (LP) formulation, for the problems is not new. However, the previous works have limitations, for example, that they are classified as offline scheduling techniques since they establish a flow network model or an LP problem considering a very long time interval. In this study, we propose how to construct the flow network model for online scheduling periodic real-time tasks on multiprocessors. Our key idea is to construct the flow network only for the active instances of tasks at the current scheduling time, while guaranteeing the existence of an optimal schedule for the future instances of the tasks. The optimal scheduling is here defined to ensure that all real-time tasks meet their deadlines when the total utilization demand of the given tasks does not exceed the total processing capacity. We then propose the flow network model-based polynomial-time scheduling algorithms. Advantageously, the flow network model allows the task workload to be collected unfairly within a certain time interval without losing the optimality. It thus leads us to designing three unfair-but-optimal scheduling algorithms on both continuous and discrete-time models. Especially, our unfair-but-optimal scheduling algorithm on a discrete-time model is, to the best of our knowledge, the first in the problem domain. We experimentally demonstrate that it significantly alleviates the scheduling overheads, i.e., the reduced number of preemptions with the comparable number of task migrations across processors.',\n",
       " 'We describe tests validating progress made toward acceleration and automation of hydrodynamic codes in the regime of developed turbulence by three Deep Learning (DL) Neural Network (NN) schemes trained on Direct Numerical Simulations of turbulence. Even the bare DL solutions, which do not take into account any physics of turbulence explicitly, are impressively good overall when it comes to qualitative description of important features of turbulence. However, the early tests have also uncovered some caveats of the DL approaches. We observe that the static DL scheme, implementing Convolutional GAN and trained on spatial snapshots of turbulence, fails to reproduce intermittency of turbulent fluctuations at small scales and details of the turbulence geometry at large scales. We show that the dynamic NN schemes, namely LAT-NET and Compressed Convolutional LSTM, trained on a temporal sequence of turbulence snapshots are capable to correct for the caveats of the static NN. We suggest a path forward towards improving reproducibility of the large-scale geometry of turbulence with NN.',\n",
       " 'Radiomics involves the study of tumor images to identify quantitative markers explaining cancer heterogeneity. The predominant approach is to extract hundreds to thousands of image features, including histogram features comprised of summaries of the marginal distribution of pixel intensities, which leads to multiple testing problems and can miss out on insights not contained in the selected features. In this paper, we present methods to model the entire marginal distribution of pixel intensities via the quantile function as functional data, regressed on a set of demographic, clinical, and genetic predictors. We call this approach quantile functional regression, regressing subject-specific marginal distributions across repeated measurements on a set of covariates, allowing us to assess which covariates are associated with the distribution in a global sense, as well as to identify distributional features characterizing these differences, including mean, variance, skewness, and various upper and lower quantiles. To account for smoothness in the quantile functions, we introduce custom basis functions we call quantlets that are sparse, regularized, near-lossless, and empirically defined, adapting to the features of a given data set. We fit this model using a Bayesian framework that uses nonlinear shrinkage of quantlet coefficients to regularize the functional regression coefficients and provides fully Bayesian inference after fitting a Markov chain Monte Carlo. We demonstrate the benefit of the basis space modeling through simulation studies, and apply the method to Magnetic resonance imaging (MRI) based radiomic dataset from Glioblastoma Multiforme to relate imaging-based quantile functions to demographic, clinical, and genetic predictors, finding specific differences in tumor pixel intensity distribution between males and females and between tumors with and without DDIT3 mutations.',\n",
       " 'Living cells communicate information about physiological conditions by producing signaling molecules in a specific timed manner. Different conditions can result in the same total amount of a signaling molecule, differing only in the pattern of the molecular concentration over time. Such temporally coded information can be completely invisible to even state-of-the-art molecular sensors with high chemical specificity that respond only to the total amount of the signaling molecule. Here, we demonstrate design principles for circuits with temporal specificity, that is, molecular circuits that respond to specific temporal patterns in a molecular concentration. We consider pulsatile patterns in a molecular concentration characterized by three fundamental temporal features - time period, duty fraction and number of pulses. We develop circuits that respond to each one of these features while being insensitive to the others. We demonstrate our design principles using abstract Chemical Reaction Networks and with explicit simulations of DNA strand displacement reactions. In this way, our work develops building blocks for temporal pattern recognition through molecular computation.',\n",
       " 'Continuous attractors have been used to understand recent neuroscience experiments where persistent activity patterns encode internal representations of external attributes like head direction or spatial location. However, the conditions under which the emergent bump of neural activity in such networks can be manipulated by space and time-dependent external sensory or motor signals are not understood. Here, we find fundamental limits on how rapidly internal representations encoded along continuous attractors can be updated by an external signal. We apply these results to place cell networks to derive a velocity-dependent non-equilibrium memory capacity in neural networks.',\n",
       " 'In recent times, Resistive RAMs (ReRAMs) have gained significant prominence due to their unique feature of supporting both non-volatile storage and logic capabilities. ReRAM is also reported to provide extremely low power consumption compared to the standard CMOS storage devices. As a result, researchers have explored the mapping and design of diverse applications, ranging from arithmetic to neuromorphic computing structures to ReRAM-based platforms. ReVAMP, a general-purpose ReRAM computing platform, has been proposed recently to leverage the parallelism exhibited in a crossbar structure. However, the technology mapping on ReVAMP remains an open challenge. Though the technology mapping with device/area-constraints have been proposed, crossbar constraints are not considered so far. In this work, we address this problem. Two technology mapping flows are proposed, considering different runtime-efficiency trade-offs. Both the mapping flows take crossbar constraints into account and generate feasible mapping for a variety of crossbar dimensions. Our proposed algorithms are highly scalable and reveal important design hints for ReRAM-based implementations.',\n",
       " 'We have applied the non-extensive statistical mechanics to free electrons in several metals to calculate the electronic specific heat at low temperature. In this case, the Fermi-Dirac (FD) function is modified from its Boltzmann-Gibbs (BG) form, with the exponential part going to a $q$-exponential, in its non-extensive form. In most cases, the non-extensive parameter, $q$, is found to be greater than unity to produce the correct thermal effective mass, $m^*$, of electrons. The ratio $m^*/m$ is found to show a nice systematic dependence on $q$. Results indicate, electrons in metals, in the presence of long range correlations are reasonably well described by Tsallis statistics.',\n",
       " \"The security of most existing cryptocurrencies is based on a concept called Proof-of-Work, in which users must solve a computationally hard cryptopuzzle to authorize transactions (`one unit of computation, one vote'). This leads to enormous expenditure on hardware and electricity in order to collect the rewards associated with transaction authorization. Proof-of-Stake is an alternative concept that instead selects users to authorize transactions proportional to their wealth (`one coin, one vote'). Some aspects of the two paradigms are the same. For instance, obtaining voting power in Proof-of-Stake has a monetary cost just as in Proof-of-Work: a coin cannot be freely duplicated any more easily than a unit of computation. However some aspects are fundamentally different. In particular, exactly because Proof-of-Stake is wasteless, there is no inherent resource cost to deviating (commonly referred to as the `Nothing-at-Stake' problem).\\n  In contrast to prior work, we focus on incentive-driven deviations (any participant will deviate if doing so yields higher revenue) instead of adversarial corruption (an adversary may take over a significant fraction of the network, but the remaining players follow the protocol). The main results of this paper are several formal barriers to designing incentive-compatible proof-of-stake cryptocurrencies (that don't apply to proof-of-work).\",\n",
       " 'In this work, we have studied the isothermal compressibility ($Îº_T$) as a function of temperature, baryon chemical potential and centre-of-mass energy ($\\\\sqrt{s_{NN}}$) using hadron resonance gas (HRG) and excluded-volume hadron resonance gas (EV-HRG) models. A mass cut-off dependence of isothermal compressibility has been studied for a physical resonance gas. Further, we study the effect of heavier resonances ($>$ 2 GeV) on the isothermal compressibility by considering the Hagedorn mass spectrum, $Ï(m)\\\\sim{\\\\exp(bm)}/{(m^2+m_0^2)^{5/4}}$. Here, the parameters, $b$ and $m_0$ are extracted after comparing the results of recent lattice QCD simulations at finite baryonic chemical potential. We find a significant difference between the results obtained in EV-HRG and HRG models at a higher temperatures and higher baryochemical potentials. The inclusion of the Hagedorn mass spectrum in the partition function for hadron gas has a large effect at a higher temperature. A higher mass cut-off in the Hagedorn mass spectrum takes the isothermal compressibility to a minimum value, which occurs near the Hagedorn temperature ($T_H$). We show explicitly that at the future low energy accelerator facilities like FAIR (CBM), Darmstadt and NICA, Dubna the created matter would be incompressible compared to the high energy facilities like RHIC and LHC.',\n",
       " 'Recent studies exploited external periodic synchronous signals to synchronize a pair of network nodes to address a threat of delaying the communications between the nodes. However, the sensing-based synchronization may yield faults due to nonmalicious signal and sensor noises. This paper considers a system of N nodes that will fuse their peer-to-peer synchronization results to correct the faults. Our analysis gives the lower bound of the number of faults that the system can tolerate when N is up to 12. If the number of faults is no greater than the lower bound, the faults can be identified and corrected. We also prove that the system cannot tolerate more than N-2 faults. Our results can guide the design of resilient sensing-based clock synchronization systems.',\n",
       " 'Encrypted databases have been studied for more than 10 years and are quickly emerging as a critical technology for the cloud. The current state of the art is to use property-preserving encrypting techniques (e.g., deterministic encryption) to protect the confidentiality of the data and support query processing at the same time. Unfortunately, these techniques have many limitations. Recently, trusted computing platforms (e.g., Intel SGX) have emerged as an alternative to implement encrypted databases. This paper demonstrates some vulnerabilities and the limitations of this technology, but it also shows how to make best use of it in order to improve on confidentiality, functionality, and performance.',\n",
       " 'Online advertisements that masquerade as non-advertising content pose numerous risks to users. Such hidden advertisements appear on social media platforms when content creators or \"influencers\" endorse products and brands in their content. While the Federal Trade Commission (FTC) requires content creators to disclose their endorsements in order to prevent deception and harm to users, we do not know whether and how content creators comply with the FTC\\'s guidelines. In this paper, we studied disclosures within affiliate marketing, an endorsement-based advertising strategy used by social media content creators. We examined whether content creators follow the FTC\\'s disclosure guidelines, how they word the disclosures, and whether these disclosures help users identify affiliate marketing content as advertisements. To do so, we first measured the prevalence of and identified the types of disclosures in over 500,000 YouTube videos and 2.1 million Pinterest pins. We then conducted a user study with 1,791 participants to test the efficacy of these disclosures. Our findings reveal that only about 10% of affiliate marketing content on both platforms contains any disclosures at all. Further, users fail to understand shorter, non-explanatory disclosures. Based on our findings, we make various design and policy suggestions to help improve advertising disclosure practices on social media platforms.',\n",
       " 'Stewards of social science data face a fundamental tension. On one hand, they want to make their data accessible to as many researchers as possible to facilitate new discoveries. At the same time, they want to restrict access to their data as much as possible in order to protect the people represented in the data. In this paper, we provide a case study addressing this common tension in an uncommon setting: the Fragile Families Challenge, a scientific mass collaboration designed to yield insights that could improve the lives of disadvantaged children in the United States. We describe our process of threat modeling, threat mitigation, and third-party guidance. We also describe the ethical principles that formed the basis of our process. We are open about our process and the trade-offs that we made in the hopes that others can improve on what we have done.',\n",
       " 'Let $\\\\mathbb{F}[X]$ be the polynomial ring over the variables $X=\\\\{x_1,x_2, \\\\ldots, x_n\\\\}$. An ideal $I=\\\\langle p_1(x_1), \\\\ldots, p_n(x_n)\\\\rangle$ generated by univariate polynomials $\\\\{p_i(x_i)\\\\}_{i=1}^n$ is a \\\\emph{univariate ideal}. We study the ideal membership problem for the univariate ideals and show the following results.\\n  \\\\item Let $f(X)\\\\in\\\\mathbb{F}[\\\\ell_1, \\\\ldots, \\\\ell_r]$ be a (low rank) polynomial given by an arithmetic circuit where $\\\\ell_i : 1\\\\leq i\\\\leq r$ are linear forms, and $I=\\\\langle p_1(x_1), \\\\ldots, p_n(x_n)\\\\rangle$ be a univariate ideal. Given $\\\\vecÎ±\\\\in {\\\\mathbb{F}}^n$, the (unique) remainder $f(X) \\\\pmod I$ can be evaluated at $\\\\vecÎ±$ in deterministic time $d^{O(r)}\\\\cdot poly(n)$, where $d=\\\\max\\\\{Â°(f),Â°(p_1)\\\\ldots,Â°(p_n)\\\\}$. This yields an $n^{O(r)}$ algorithm for minimum vertex cover in graphs with rank-$r$ adjacency matrices. It also yields an $n^{O(r)}$ algorithm for evaluating the permanent of a $n\\\\times n$ matrix of rank $r$, over any field $\\\\mathbb{F}$. Over $\\\\mathbb{Q}$, an algorithm of similar run time for low rank permanent is due to Barvinok[Bar96] via a different technique.\\n  \\\\item Let $f(X)\\\\in\\\\mathbb{F}[X]$ be given by an arithmetic circuit of degree $k$ ($k$ treated as fixed parameter) and $I=\\\\langle p_1(x_1), \\\\ldots, p_n(x_n)\\\\rangle$. We show in the special case when $I=\\\\langle x_1^{e_1}, \\\\ldots, x_n^{e_n}\\\\rangle$, we obtain a randomized $O^*(4.08^k)$ algorithm that uses $poly(n,k)$ space.\\n  \\\\item Given $f(X)\\\\in\\\\mathbb{F}[X]$ by an arithmetic circuit and $I=\\\\langle p_1(x_1), \\\\ldots, p_k(x_k) \\\\rangle$, membership testing is $W[1]$-hard, parameterized by $k$. The problem is $MINI[1]$-hard in the special case when $I=\\\\langle x_1^{e_1}, \\\\ldots, x_k^{e_k}\\\\rangle$.',\n",
       " \"For linear inverse problems with a large number of unknown parameters, uncertainty quantification remains a challenging task. In this work, we use Krylov subspace methods to approximate the posterior covariance matrix and describe efficient methods for exploring the posterior distribution. Assuming that Krylov methods (e.g., based on the generalized Golub-Kahan bidiagonalization) have been used to compute an estimate of the solution, we get an approximation of the posterior covariance matrix for `free.' We provide theoretical results that quantify the accuracy of the approximation and of the resulting posterior distribution. Then, we describe efficient methods that use the approximation to compute measures of uncertainty, including the Kullback-Liebler divergence. We present two methods that use preconditioned Lanczos methods to efficiently generate samples from the posterior distribution. Numerical examples from tomography demonstrate the effectiveness of the described approaches.\",\n",
       " 'Voltage current characteristics of a PN-junction diode are intrinsically nonlinear in nature. It is shown in this paper that a mathematical form of nonlinearity of a PN-junction diode resembles the nonlinear response of electric polarization of a dielectric medium to the electric field. Nonlinearity of a PN-junction can be expressed in a series of successively increasing orders of the nonlinearity. For a PN-junction diode, higher order nonlinear terms become significant as a voltage across the diode is increased. In this paper, a gradual emergence of a nonlinear regime with the amplitude of a sinusoidal voltage is presented. Higher order harmonics are produced by utilizing the nonlinearity of a single PN-junction diode. An experimental realization of a frequency comb with the highest frequency up to the twentieth harmonics is also presented. In addition, in the same circuit by making the nonlinearity significant up to the second order, an experiment on generation of the sum and difference of frequencies is realized.',\n",
       " 'We present a quantum key distribution (QKD) protocol based on long lived coherent states prepared on superconducting rings with a mesoscopic Josephson junction (dc-SQUIDs). This enables storage of the prepared states for long durations before actually performing the key distribution. Our on-demand QKD protocol is closely related to the coherent state based continuous variable quantum key distribution protocol. A detailed analysis of preparation, evolution and different measurement schemes that are required to be implemented on dc-SQUIDs to carry out the QKD is provided. We present two variants of the protocol, one requiring time stamping of states and offering a higher key rate and the other without time stamping and a lower key rate. This is a step towards having non-photon based QKD protocols which will be eventually desirable as photon states cannot be stored for long and therefore the key distribution has to be implemented immediately after photon exchange has occurred. Our protocol offers an innovative scheme to perform QKD and can be realized using current experimental techniques.',\n",
       " 'We study a multispecies generalization of a left-permeable asymmetric exclusion process (LPASEP) in one dimension with open boundaries. We determine all phases in the phase diagram using an exact projection to the LPASEP solved by us in a previous work. In most phases, we observe the phenomenon of dynamical expulsion of one or more species. We explain the density profiles in each phase using interacting shocks. This explanation is corroborated by simulations.',\n",
       " \"We consider circuit complexity in certain interacting scalar quantum field theories, mainly focusing on the $Ï†^4$ theory. We work out the circuit complexity for evolving from a nearly Gaussian unentangled reference state to the entangled ground state of the theory. Our approach uses Nielsen's geometric method, which translates into working out the geodesic equation arising from a certain cost functional. We present a general method, making use of integral transforms, to do the required lattice sums analytically and give explicit expressions for the $d=2,3$ cases. Our method enables a study of circuit complexity in the epsilon expansion for the Wilson-Fisher fixed point. We find that with increasing dimensionality the circuit depth increases in the presence of the $Ï†^4$ interaction eventually causing the perturbative calculation to breakdown. We discuss how circuit complexity relates with the renormalization group.\",\n",
       " 'We use concurrence as an entanglement measure and experimentally demonstrate the entanglement classification of arbitrary three-qubit pure states on a nuclear magnetic resonance (NMR) quantum information processor. Computing the concurrence experimentally under three different bipartitions, for an arbitrary three-qubit pure state, reveals the entanglement class of the state. The experiment involves measuring the expectation values of Pauli operators. This was achieved by mapping the desired expectation values onto the local $z$ magnetization of a single qubit. We tested the entanglement classification protocol on twenty seven different generic states and successfully detected their entanglement class. Full quantum state tomography was performed to construct experimental tomographs of each state and negativity was calculated from them, to validate the experimental results.',\n",
       " 'We analyse the transverse momentum ($p_{\\\\rm T}$)-spectra as a function of charged-particle multiplicity at midrapidity ($|y| < 0.5$) for various identified particles such as $Ï€^{\\\\pm}$, $K^{\\\\pm}$, $K_S^0$, $p+\\\\overline{p}$, $Ï†$, $K^{*0} + \\\\overline {K^{*0}}$, and $Î›$ + $\\\\barÎ›$ in proton-proton collisions at $\\\\sqrt{s}$ = 7 TeV using Boltzmann-Gibbs Blast Wave (BGBW) model and thermodynamically consistent Tsallis distribution function. We obtain the multiplicity dependent kinetic freeze-out temperature ($T_{\\\\rm kin}$) and radial flow ($Î²$) of various particles after fitting the $p_{\\\\rm T}$-distribution with BGBW model. Here, $T_{\\\\rm kin}$ exhibits mild dependence on multiplicity class while $Î²$ shows almost independent behaviour. The information regarding Tsallis temperature and the non-extensivity parameter ($q$) are drawn by fitting the $p_{\\\\rm T}$-spectra with Tsallis distribution function. The extracted parameters of these particles are studied as a function of charged particle multiplicity density ($dN_{ch}/dÎ·$). In addition to this, we also study these parameters as a function of particle mass to observe any possible mass ordering. All the identified hadrons show a mass ordering in temperature, non-extensive parameter and also a strong dependence on multiplicity classes, except the lighter particles. It is observed that as the particle multiplicity increases, the $q$-parameter approaches to Boltzmann-Gibbs value, hence a conclusion can be drawn that system tends to thermal equilibrium. The observations are consistent with a differential freeze-out scenario of the produced particles.',\n",
       " 'Inferring road attributes such as lane count and road type from satellite imagery is challenging. Often, due to the occlusion in satellite imagery and the spatial correlation of road attributes, a road attribute at one position on a road may only be apparent when considering far-away segments of the road. Thus, to robustly infer road attributes, the model must integrate scattered information and capture the spatial correlation of features along roads. Existing solutions that rely on image classifiers fail to capture this correlation, resulting in poor accuracy. We find this failure is caused by a fundamental limitation -- the limited effective receptive field of image classifiers. To overcome this limitation, we propose RoadTagger, an end-to-end architecture which combines both Convolutional Neural Networks (CNNs) and Graph Neural Networks (GNNs) to infer road attributes. The usage of graph neural networks allows information propagation on the road network graph and eliminates the receptive field limitation of image classifiers. We evaluate RoadTagger on both a large real-world dataset covering 688 km^2 area in 20 U.S. cities and a synthesized micro-dataset. In the evaluation, RoadTagger improves inference accuracy over the CNN image classifier based approaches. RoadTagger also demonstrates strong robustness against different disruptions in the satellite imagery and the ability to learn complicated inductive rules for aggregating scattered information along the road network.',\n",
       " 'Street maps are a crucial data source that help to inform a wide range of decisions, from navigating a city to disaster relief and urban planning. However, in many parts of the world, street maps are incomplete or lag behind new construction. Editing maps today involves a tedious process of manually tracing and annotating roads, buildings, and other map features.\\n  Over the past decade, many automatic map inference systems have been proposed to automatically extract street map data from satellite imagery, aerial imagery, and GPS trajectory datasets. However, automatic map inference has failed to gain traction in practice due to two key limitations: high error rates (low precision), which manifest in noisy inference outputs, and a lack of end-to-end system design to leverage inferred data to update existing street maps.\\n  At MIT and QCRI, we have developed a number of algorithms and approaches to address these challenges, which we combined into a new system we call Mapster. Mapster is a human-in-the-loop street map editing system that incorporates three components to robustly accelerate the mapping process over traditional tools and workflows: high-precision automatic map inference, data refinement, and machine-assisted map editing.\\n  Through an evaluation on a large-scale dataset including satellite imagery, GPS trajectories, and ground-truth map data in forty cities, we show that Mapster makes automation practical for map editing, and enables the curation of map datasets that are more complete and up-to-date at less cost.',\n",
       " 'With the transformative technologies and the rapidly changing global R&D landscape, the multimedia and multimodal community is now faced with many new opportunities and uncertainties. With the open source dissemination platform and pervasive computing resources, new research results are being discovered at an unprecedented pace. In addition, the rapid exchange and influence of ideas across traditional discipline boundaries have made the emphasis on multimedia multimodal research even more important than before. To seize these opportunities and respond to the challenges, we have organized a workshop to specifically address and brainstorm the challenges, opportunities, and research roadmaps for MM research. The two-day workshop, held on March 30 and 31, 2017 in Washington DC, was sponsored by the Information and Intelligent Systems Division of the National Science Foundation of the United States. Twenty-three (23) invited participants were asked to review and identify research areas in the MM field that are most important over the next 10-15 year timeframe. Important topics were selected through discussion and consensus, and then discussed in depth in breakout groups. Breakout groups reported initial discussion results to the whole group, who continued with further extensive deliberation. For each identified topic, a summary was produced after the workshop to describe the main findings, including the state of the art, challenges, and research roadmaps planned for the next 5, 10, and 15 years in the identified area.',\n",
       " 'Mapping road networks today is labor-intensive. As a result, road maps have poor coverage outside urban centers in many countries. Systems to automatically infer road network graphs from aerial imagery and GPS trajectories have been proposed to improve coverage of road maps. However, because of high error rates, these systems have not been adopted by mapping communities. We propose machine-assisted map editing, where automatic map inference is integrated into existing, human-centric map editing workflows. To realize this, we build Machine-Assisted iD (MAiD), where we extend the web-based OpenStreetMap editor, iD, with machine-assistance functionality. We complement MAiD with a novel approach for inferring road topology from aerial imagery that combines the speed of prior segmentation approaches with the accuracy of prior iterative graph construction methods. We design MAiD to tackle the addition of major, arterial roads in regions where existing maps have poor coverage, and the incremental improvement of coverage in regions where major roads are already mapped. We conduct two user studies and find that, when participants are given a fixed time to map roads, they are able to add as much as 3.5x more roads with MAiD.',\n",
       " \"To reduce transmit power, increase throughput, and improve communication range, radio systems---such as IoT sensor networks, Wi-Fi and cellular networks---benefit from the ability to direct their signals, to ensure that more of the transmitted power reaches the receiver. Many modern systems beamform with antenna arrays for this purpose. However, a radio's ability to direct its signal is fundamentally limited by its size. Unfortunately practical challenges limit the size of modern radios, and consequently, their ability to beamform. In many settings, radios on devices must be small and inexpensive; today, these settings are unable to benefit from high-precision beamforming.\\n  To address this problem, we introduce RFocus, which moves beamforming functions from the radio endpoints to the environment. RFocus includes a two-dimensional surface with a rectangular array of simple elements, each of which functions as an RF switch. Each element either lets the signal through or reflects it. The surface does not emit any power of its own. The state of the elements is set by a software controller to maximize the signal strength at a receiver, with a novel optimization algorithm that uses signal strength measurements from the receiver. The RFocus surface can be manufactured as an inexpensive thin wallpaper, requiring no wiring. This solution requires only a method to communicate received signal strengths periodically to the RFocus controller. Our prototype implementation improves the median signal strength by 10.5x, and the median channel capacity by 2.1x.\",\n",
       " 'We propose Accel-Brake Control (ABC), a simple and deployable explicit congestion control protocol for network paths with time-varying wireless links. ABC routers mark each packet with an \"accelerate\" or \"brake\", which causes senders to slightly increase or decrease their congestion windows. Routers use this feedback to quickly guide senders towards a desired target rate. ABC requires no changes to header formats or user devices, but achieves better performance than XCP. ABC is also incrementally deployable; it operates correctly when the bottleneck is a non-ABC router, and can coexist with non-ABC traffic sharing the same bottleneck link. We evaluate ABC using a Wi-Fi implementation and trace-driven emulation of cellular links. ABC achieves 30-40% higher throughput than Cubic+Codel for similar delays, and 2.2X lower delays than BBR on a Wi-Fi path. On cellular network paths, ABC achieves 50% higher throughput than Cubic+Codel.',\n",
       " 'Prior research has proposed technical solutions to use peer-to-peer (P2P) content delivery to serve Internet video, showing that it can reduce costs to content providers. Yet, such methods have not become widespread except for a few niche instances. An important challenge is incentivization: what tangible benefits does P2P content delivery offer users who bring resources to the table? In this paper, we ask whether monetary incentives can help attract peers in P2P content delivery systems. We commissioned a professional survey of people around theUnited States to answer several relevant questions. We found that 51% of the 876 respondents--substantially larger than our expectations--answered \"yes\" to whether they would participate for suitable financial incentives. Encouraged by the results of the survey, we propose Gringotts, a system to structure incentives and securely incorporate P2P delivery into content delivery systems. Gringotts provides a novel Proof of Delivery mechanism that allows content providers to verify correct delivery of their files, and shows how to use cryptocurrency to pay peers while guarding against liars and Sybil attacks.',\n",
       " 'This paper introduces Nimbus, a robust technique to detect whether the cross traffic competing with a flow is \"elastic\", and shows that this elasticity detector improves congestion control. If cross traffic is inelastic, then a sender can control queueing delays while achieving high throughput, but in the presence of elastic traffic, it may lose throughput if it attempts to control packet delay. To estimate elasticity, Nimbus modulates the flow\\'s sending rate with sinusoidal pulses that create small traffic fluctuations at the bottleneck link, and measures the frequency response of the rate of the cross traffic. Our results on emulated and real-world paths show that congestion control using elasticity detection achieves throughput comparable to Cubic, but with delays that are 50-70 ms lower when cross traffic is inelastic. Nimbus detects the nature of the cross traffic more accurately than Copa, and is usable as a building block by other end-to-end algorithms.',\n",
       " 'Mapping road networks is currently both expensive and labor-intensive. High-resolution aerial imagery provides a promising avenue to automatically infer a road network. Prior work uses convolutional neural networks (CNNs) to detect which pixels belong to a road (segmentation), and then uses complex post-processing heuristics to infer graph connectivity. We show that these segmentation methods have high error rates because noisy CNN outputs are difficult to correct. We propose RoadTracer, a new method to automatically construct accurate road network maps from aerial images. RoadTracer uses an iterative search process guided by a CNN-based decision function to derive the road network graph directly from the output of the CNN. We compare our approach with a segmentation method on fifteen cities, and find that at a 5% error rate, RoadTracer correctly captures 45% more junctions across these cities.',\n",
       " 'Switches today provide a small set of scheduling algorithms. While we can tweak scheduling parameters, we cannot modify algorithmic logic, or add a completely new algorithm, after the switch has been designed. This paper presents a design for a programmable packet scheduler, which allows scheduling algorithms---potentially algorithms that are unknown today---to be programmed into a switch without requiring hardware redesign.\\n  Our design builds on the observation that scheduling algorithms make two decisions: in what order to schedule packets and when to schedule them. Further, in many scheduling algorithms these decisions can be made when packets are enqueued. We leverage this observation to build a programmable scheduler using a single abstraction: the push-in first-out queue (PIFO), a priority queue that maintains the scheduling order and time for such algorithms.\\n  We show that a programmable scheduler using PIFOs lets us program a wide variety of scheduling algorithms. We present a detailed hardware design for this scheduler for a 64-port 10 Gbit/s shared-memory switch with <4% chip area overhead on a 16-nm standard-cell library. Our design lets us program many sophisticated algorithms, such as a 5-level hierarchical scheduler with programmable scheduling algorithms at each level.',\n",
       " \"Many algorithms for congestion control, scheduling, network measurement, active queue management, security, and load balancing require custom processing of packets as they traverse the data plane of a network switch. To run at line rate, these data-plane algorithms must be in hardware. With today's switch hardware, algorithms cannot be changed, nor new algorithms installed, after a switch has been built.\\n  This paper shows how to program data-plane algorithms in a high-level language and compile those programs into low-level microcode that can run on emerging programmable line-rate switching chipsets. The key challenge is that these algorithms create and modify algorithmic state. The key idea to achieve line-rate programmability for stateful algorithms is the notion of a packet transaction : a sequential code block that is atomic and isolated from other such code blocks. We have developed this idea in Domino, a C-like imperative language to express data-plane algorithms. We show with many examples that Domino provides a convenient and natural way to express sophisticated data-plane algorithms, and show that these algorithms can be run at line rate with modest estimated die-area overhead.\",\n",
       " 'Fractional Interference Alignment (FIA) is a transmission scheme which achieves any value between [0,1] for the Symbols transmitted per Antenna per Channel use (SpAC). FIA was designed in [1] specifically for Finite Alphabet (FA) signals, under the constraint that the Minimum Distance (MD) detector is used at all the receivers. Similar to classical interference alignment, the FIA precoder also needs perfect channel state information at all the transmitters (CSIT). In this work, a novel Blind Fractional Interference Alignment (B-FIA) scheme is introduced, where the basic assumption is that CSIT is not available. We consider two popular channel models, namely: Broadcast channel, and Interference channel. For these two channel models, the maximum achievable value of SpAC satisfying the constraints of the MD detector is obtained, but with no CSIT, and also a precoder design is provided to obtain any value of SpAC in the achievable range.\\n  Further, the precoder structure provided has one distinct advantage: interference channel state information at the receiver (I-CSIR) is not needed, when all the transmitters and receivers are equipped with one antenna each. When two or more antennas are used at both ends, I-CSIR must be available to obtain the maximum achievable value of SpAC. The receiver designs for both the Minimum Distance and the Maximum Likelihood (ML) decoders are discussed, where the interference statistics is estimated from the received signal samples. Simulation results of the B-FIA show that the ML decoder with estimated statistics achieves a significantly better error rate performance when compared to the MD decoder with known statistics, since the MD decoder assumes the interference plus noise term as colored Gaussian noise.',\n",
       " 'Interference Alignment (IA) is a transmission scheme which achieves 1/2 Degrees-of-Freedom (DoF) per transmit-antenna per user. The constraints imposed on the scheme are based on the linear receiver since conventional IA assumes Gaussian signaling. However, when the transmitters employ Finite Alphabet (FA) signaling, neither the conventional IA precoders nor the linear receiver are optimal structures. Therefore, a novel Fractional Interference Alignment (FIA) scheme is introduced when FA signals are used, where the alignment constraints are now based on the non-linear, minimum distance (MD) detector. Since DoF is defined only as signal-to-noise ratio tends to infinity, we introduce a new metric called SpAC (number of Symbols transmitted-per-transmit Antenna-per-Channel use) for analyzing the FIA scheme. The maximum SpAC is one, and the FIA achieves any value of SpAC in the range [0,1]. The key motivation for this work is that numerical simulations with FA signals and MD detector for fixed SpAC (=1/2, as in IA) over a set of optimization problems, like minimizing bit error rate or maximizing the mutual information, achieves a significantly better error rate performance when compared to the existing algorithms that minimize mean square error or maximize signal-to-interference plus noise ratio.',\n",
       " \"This paper presents an analysis of spinal codes, a class of rateless codes proposed recently. We prove that spinal codes achieve Shannon capacity for the binary symmetric channel (BSC) and the additive white Gaussian noise (AWGN) channel with an efficient polynomial-time encoder and decoder. They are the first rateless codes with proofs of these properties for BSC and AWGN. The key idea in the spinal code is the sequential application of a hash function over the message bits. The sequential structure of the code turns out to be crucial for efficient decoding. Moreover, counter to the wisdom of having an expander structure in good codes, we show that the spinal code, despite its sequential structure, achieves capacity. The pseudo-randomness provided by a hash function suffices for this purpose. Our proof introduces a variant of Gallager's result characterizing the error exponent of random codes for any memoryless channel. We present a novel application of these error-exponent results within the framework of an efficient sequential code. The application of a hash function over the message bits provides a methodical and effective way to de-randomize Shannon's random codebook construction.\",\n",
       " '  This paper describes the implementation and evaluation of an operating system module, the Congestion Manager (CM), which provides integrated network flow management and exports a convenient programming interface that allows applications to be notified of, and adapt to, changing network conditions. We describe the API by which applications interface with the CM, and the architectural considerations that factored into the design. To evaluate the architecture and API, we describe our implementations of TCP; a streaming layered audio/video application; and an interactive audio application using the CM, and show that they achieve adaptive behavior without incurring much end-system overhead. All flows including TCP benefit from the sharing of congestion information, and applications are able to incorporate new functionality such as congestion control and adaptive behavior.',\n",
       " 'Magnetic domain walls are information tokens in both logic and memory devices, and hold particular interest in applications such as neuromorphic accelerators that combine logic in memory. Here, we show that devices based on the electrical manipulation of magnetic domain walls are capable of implementing linear, as well as programmable nonlinear, functions. Unlike other approaches, domain-wall-based devices are ideal for application to both synaptic weight generators and thresholding in deep neural networks. Prototype micrometer-size devices operate with 8 ns current pulses and the energy consumption required for weight modulation is < 16 pJ. Both speed and energy consumption compare favorably to other synaptic nonvolatile devices, with the expected energy dissipation for scaled 20 nm devices close to that of biological neurons.',\n",
       " 'Lead halide-based perovskite thin films have attracted great attention due to the explosive increase in perovskite solar cell efficiencies. The same optoelectronic properties that make perovskites ideal absorber materials in solar cells are also beneficial in other light-harvesting applications and make them prime candidates as triplet sensitizers in upconversion via triplet-triplet annihilation in rubrene. In this contribution, we take advantage of long carrier lifetimes and carrier diffusion lengths in perovskite thin films, their high absorption cross sections throughout the visible spectrum, as well as the strong spin-orbit coupling owing to the abundance of heavy atoms to sensitize the upconverter rubrene. Employing bulk perovskite thin films as the absorber layer and spin-mixer in inorganic/organic heterojunction upconversion devices allows us to forego the additional tunneling barrier owing from the passivating ligands required for colloidal sensitizers. Our bilayer device exhibits an upconversion efficiency in excess of 3% under 785 nm illumination.',\n",
       " 'High-quality micro-lasers are key ingredients in non-linear optics, communication, sensing and low-threshold solar-pumped lasers. However, such micro-lasers exhibit negligible absorption of free-space broadband pump light. Recently, this limitation was lifted by cascade energy transfer, in which the absorption and quality factor are modulated with wavelength, enabling non-resonant pumping of high-quality micro-lasers and solar-pumped laser to operate at record low solar concentration. Here, we present a generic theoretical framework for modeling the absorption, emission and energy transfer of incoherent radiation between cascade sensitizer and laser gain media. Our model is based on linear equations of the modified net radiation method and is therefore robust, fast converging and has low complexity. We apply this formalism to compute the optimal parameters of low-threshold solar-pumped lasers. It is revealed that the interplay between the absorption and self-absorption of such lasers defines the optimal pump absorption below the maximal value, which is in contrast to conventional lasers for which full pump absorption is desired. Numerical results are compared to experimental data on a sensitized Nd:YAG cavity, and quantitative agreement with theoretical models is found. Our work modularizes the gain and sensitizing components and paves the way for the optimal design of broadband-pumped high-quality micro-lasers and efficient solar-pumped lasers.',\n",
       " 'Plexcitons are polaritonic modes that result from the strong coupling between excitons and plasmons. We consider plexcitons emerging from the interaction of excitons in an organic molecular layer with surface plasmons in a metallic film. We predict the emergence of Dirac cones in the two-dimensional bandstructure of plexcitons due to the inherent alignment of the excitonic transitions in the organic layer. These Dirac cones may open up in energy by simultaneously interfacing the metal with a magneto-optical layer and subjecting the whole system to a perpendicular magnetic field. The resulting energy gap becomes populated with topologically protected one-way modes which travel at the interface of this plexcitonic system. Our theoretical proposal suggests that plexcitons are a convenient and simple platform for the exploration of exotic phases of matter as well as of novel ways to direct energy flow at the nanoscale.',\n",
       " 'The formation of 360Â° magnetic domain walls (360DWs) in Co and Ni80Fe20 thin film wires was demonstrated experimentally for different wire widths, by successively injecting two 180Â° domain walls (180DWs) into the wire. For narrow wires (less than 50 nm wide for Co), edge roughness prevented the combination of the 180DWs into a 360DW, and for wide wires (200 nm for Co) the 360DW collapsed, but over an intermediate range of wire widths, reproducible 360DW formation occurred. The annihilation and dissociation of 360DWs was demonstrated by applying a magnetic field parallel to the wire, showing that annihilation fields were several times higher than dissociation fields in agreement with micromagnetic modeling. The annihilation of a 360DW by current pulsing was demonstrated.',\n",
       " 'Organic light emitting devices and solar cells are machines that create, manipulate and destroy excited states in organic semiconductors. It is crucial to characterize these excited states, or excitons, to optimize device performance in applications like displays and solar energy harvesting. This is complicated if the excited state is a triplet because the electronic transition is dark with a vanishing oscillator strength. As a consequence, triplet state spectroscopy must usually be performed at cryogenic temperatures to reduce competition from non-radiative rates. Here, we control non-radiative rates by engineering a solid-state host matrix containing the target molecule, allowing the observation of phosphorescence at room temperature and alleviating constraints of cryogenic experiments. We test these techniques on a wide range of materials with functionalities spanning multi-exciton generation (singlet exciton fission), organic light emitting device host materials, and thermally activated delayed fluorescence type emitters. Control of non-radiative modes in the matrix surrounding a target molecule may also have broader applications in light emitting and photovoltaic devices.',\n",
       " 'We report highly efficient, simultaneous fluorescence and phosphorescence (74% yield) at room temperature from a single molecule ensemble of (BzP)PB dispersed into a polymer host. The slow phosphorescence (208 ms lifetime) is very efficient (50%) at room temperature and only possible because the non-radiative rate for the triplet state is extremely low. The ability of an organic molecule to function as an efficient dual state emitter at room temperature is unusual and opens new fields of applications including the use as broadband down-conversion emitters, optical sensors and attenuators, exciton probes, and spin-independent intermediates for FÃ¶rster resonant energy transfer.',\n",
       " 'Generative models in molecular design tend to be richly parameterized, data-hungry neural models, as they must create complex structured objects as outputs. Estimating such models from data may be challenging due to the lack of sufficient training data. In this paper, we propose a surprisingly effective self-training approach for iteratively creating additional molecular targets. We first pre-train the generative model together with a simple property predictor. The property predictor is then used as a likelihood model for filtering candidate structures from the generative model. Additional targets are iteratively produced and used in the course of stochastic EM iterations to maximize the log-likelihood that the candidate structures are accepted. A simple rejection (re-weighting) sampler suffices to draw posterior samples since the generative model is already reasonable after pre-training. We demonstrate significant gains over strong baselines for both unconditional and conditional molecular design. In particular, our approach outperforms the previous state-of-the-art in conditional molecular design by over 10% in absolute gain.',\n",
       " 'Drug discovery aims to find novel compounds with specified chemical property profiles. In terms of generative modeling, the goal is to learn to sample molecules in the intersection of multiple property constraints. This task becomes increasingly challenging when there are many property constraints. We propose to offset this complexity by composing molecules from a vocabulary of substructures that we call molecular rationales. These rationales are identified from molecules as substructures that are likely responsible for each property of interest. We then learn to expand rationales into a full molecule using graph generative models. Our final generative model composes molecules as mixtures of multiple rationale completions, and this mixture is fine-tuned to preserve the properties of interest. We evaluate our model on various drug design tasks and demonstrate significant improvements over state-of-the-art baselines in terms of accuracy, diversity, and novelty of generated compounds.',\n",
       " 'Graph generation techniques are increasingly being adopted for drug discovery. Previous graph generation approaches have utilized relatively small molecular building blocks such as atoms or simple cycles, limiting their effectiveness to smaller molecules. Indeed, as we demonstrate, their performance degrades significantly for larger molecules. In this paper, we propose a new hierarchical graph encoder-decoder that employs significantly larger and more flexible graph motifs as basic building blocks. Our encoder produces a multi-resolution representation for each molecule in a fine-to-coarse fashion, from atoms to connected motifs. Each level integrates the encoding of constituents below with the graph at that level. Our autoregressive coarse-to-fine decoder adds one motif at a time, interleaving the decision of selecting a new motif with the process of resolving its attachments to the emerging molecule. We evaluate our model on multiple molecule generation tasks, including polymers, and show that our model significantly outperforms previous state-of-the-art baselines.',\n",
       " 'We propose Blank Language Model (BLM), a model that generates sequences by dynamically creating and filling in blanks. Unlike previous masked language models or the Insertion Transformer, BLM uses blanks to control which part of the sequence to expand. This fine-grained control of generation is ideal for a variety of text editing and rewriting tasks. The model can start from a single blank or partially completed text with blanks at specified locations. It iteratively determines which word to place in a blank and whether to insert new blanks, and stops generating when no blanks are left to fill. BLM can be efficiently trained using a lower bound of the marginal data likelihood, and achieves perplexity comparable to traditional left-to-right language models on the Penn Treebank and WikiText datasets. On the task of filling missing text snippets, BLM significantly outperforms all other baselines in terms of both accuracy and fluency. Experiments on style transfer and damaged ancient text restoration demonstrate the potential of this framework for a wide range of applications.',\n",
       " 'Automatic question generation can benefit many applications ranging from dialogue systems to reading comprehension. While questions are often asked with respect to long documents, there are many challenges with modeling such long documents. Many existing techniques generate questions by effectively looking at one sentence at a time, leading to questions that are easy and not reflective of the human process of question generation. Our goal is to incorporate interactions across multiple sentences to generate realistic questions for long documents. In order to link a broad document context to the target answer, we represent the relevant context via a multi-stage attention mechanism, which forms the foundation of a sequence to sequence model. We outperform state-of-the-art methods on question generation on three question-answering datasets -- SQuAD, MS MARCO and NewsQA.',\n",
       " 'We propose a new model for making generalizable and diverse retrosynthetic reaction predictions. Given a target compound, the task is to predict the likely chemical reactants to produce the target. This generative task can be framed as a sequence-to-sequence problem by using the SMILES representations of the molecules. Building on top of the popular Transformer architecture, we propose two novel pre-training methods that construct relevant auxiliary tasks (plausible reactions) for our problem. Furthermore, we incorporate a discrete latent variable model into the architecture to encourage the model to produce a diverse set of alternative predictions. On the 50k subset of reaction examples from the United States patent literature (USPTO-50k) benchmark dataset, our model greatly improves performance over the baseline, while also generating predictions that are more diverse.',\n",
       " 'Online encyclopediae like Wikipedia contain large amounts of text that need frequent corrections and updates. The new information may contradict existing content in encyclopediae. In this paper, we focus on rewriting such dynamically changing articles. This is a challenging constrained generation task, as the output must be consistent with the new information and fit into the rest of the existing document. To this end, we propose a two-step solution: (1) We identify and remove the contradicting components in a target text for a given claim, using a neutralizing stance model; (2) We expand the remaining text to be consistent with the given claim, using a novel two-encoder sequence-to-sequence model with copy attention. Applied to a Wikipedia fact update dataset, our method successfully generates updated sentences for new claims, achieving the highest SARI score. Furthermore, we demonstrate that generating synthetic data through such rewritten sentences can successfully augment the FEVER fact-checking training dataset, leading to a relative error reduction of 13%.',\n",
       " 'This paper explores the task of leveraging typology in the context of cross-lingual dependency parsing. While this linguistic information has shown great promise in pre-neural parsing, results for neural architectures have been mixed. The aim of our investigation is to better understand this state-of-the-art. Our main findings are as follows: 1) The benefit of typological information is derived from coarsely grouping languages into syntactically-homogeneous clusters rather than from learning to leverage variations along individual typological dimensions in a compositional manner; 2) Typology consistent with the actual corpus statistics yields better transfer performance; 3) Typological similarity is only a rough proxy of cross-lingual transferability with respect to parsing.',\n",
       " 'Recent developments in neural language models (LMs) have raised concerns about their potential misuse for automatically spreading misinformation. In light of these concerns, several studies have proposed to detect machine-generated fake news by capturing their stylistic differences from human-written text. These approaches, broadly termed stylometry, have found success in source attribution and misinformation detection in human-written texts. However, in this work, we show that stylometry is limited against machine-generated misinformation. While humans speak differently when trying to deceive, LMs generate stylistically consistent text, regardless of underlying motive. Thus, though stylometry can successfully prevent impersonation by identifying text provenance, it fails to distinguish legitimate LM applications from those that introduce false information. We create two benchmarks demonstrating the stylistic similarity between malicious and legitimate uses of LMs, employed in auto-completion and editing-assistance settings. Our findings highlight the need for non-stylometry approaches in detecting machine-generated misinformation, and open up the discussion on the desired evaluation benchmarks.',\n",
       " 'In this paper, we explore meta-learning for few-shot text classification. Meta-learning has shown strong performance in computer vision, where low-level patterns are transferable across learning tasks. However, directly applying this approach to text is challenging--lexical features highly informative for one task may be insignificant for another. Thus, rather than learning solely from words, our model also leverages their distributional signatures, which encode pertinent word occurrence patterns. Our model is trained within a meta-learning framework to map these signatures into attention scores, which are then used to weight the lexical representations of words. We demonstrate that our model consistently outperforms prototypical networks learned on lexical knowledge (Snell et al., 2017) in both few-shot text classification and relation classification by a significant margin across six benchmark datasets (20.0% on average in 1-shot classification).',\n",
       " 'Fact verification requires validating a claim in the context of evidence. We show, however, that in the popular FEVER dataset this might not necessarily be the case. Claim-only classifiers perform competitively with top evidence-aware models. In this paper, we investigate the cause of this phenomenon, identifying strong cues for predicting labels solely based on the claim, without considering any evidence. We create an evaluation set that avoids those idiosyncrasies. The performance of FEVER-trained models significantly drops when evaluated on this test set. Therefore, we introduce a regularization method which alleviates the effect of bias in the training data, obtaining improvements on the newly created test set. This work is a step towards a more sound evaluation of reasoning capabilities in fact verification models.',\n",
       " 'The problem of accelerating drug discovery relies heavily on automatic tools to optimize precursor molecules to afford them with better biochemical properties. Our work in this paper substantially extends prior state-of-the-art on graph-to-graph translation methods for molecular optimization. In particular, we realize coherent multi-resolution representations by interweaving the encoding of substructure components with the atom-level encoding of the original molecular graph. Moreover, our graph decoder is fully autoregressive, and interleaves each step of adding a new substructure with the process of resolving its attachment to the emerging molecule. We evaluate our model on multiple molecular optimization tasks and show that our model significantly outperforms previous state-of-the-art baselines.',\n",
       " 'In this paper we propose a novel neural approach for automatic decipherment of lost languages. To compensate for the lack of strong supervision signal, our model design is informed by patterns in language change documented in historical linguistics. The model utilizes an expressive sequence-to-sequence model to capture character-level correspondences between cognates. To effectively train the model in an unsupervised manner, we innovate the training procedure by formalizing it as a minimum-cost flow problem. When applied to the decipherment of Ugaritic, we achieve a 5.5% absolute improvement over state-of-the-art results. We also report the first automatic results in deciphering Linear B, a syllabic language related to ancient Greek, where our model correctly translates 67.3% of cognates.',\n",
       " 'Generative autoencoders offer a promising approach for controllable text generation by leveraging their latent sentence representations. However, current models struggle to maintain coherent latent spaces required to perform meaningful text manipulations via latent vector operations. Specifically, we demonstrate by example that neural encoders do not necessarily map similar sentences to nearby latent vectors. A theoretical explanation for this phenomenon establishes that high-capacity autoencoders can learn an arbitrary mapping between sequences and associated latent representations. To remedy this issue, we augment adversarial autoencoders with a denoising objective where original sentences are reconstructed from perturbed versions (referred to as DAAE). We prove that this simple modification guides the latent space geometry of the resulting model by encouraging the encoder to map similar texts to similar latent representations. In empirical comparisons with various types of autoencoders, our model provides the best trade-off between generation quality and reconstruction capacity. Moreover, the improved geometry of the DAAE latent space enables zero-shot text style transfer via simple latent vector arithmetic.',\n",
       " 'Much of the recent work on learning molecular representations has been based on Graph Convolution Networks (GCN). These models rely on local aggregation operations and can therefore miss higher-order graph properties. To remedy this, we propose Path-Augmented Graph Transformer Networks (PAGTN) that are explicitly built on longer-range dependencies in graph-structured data. Specifically, we use path features in molecular graphs to create global attention layers. We compare our PAGTN model against the GCN model and show that our model consistently outperforms GCNs on molecular property prediction datasets including quantum chemistry (QM7, QM8, QM9), physical chemistry (ESOL, Lipophilictiy) and biochemistry (BACE, BBBP).',\n",
       " 'PURPOSE: The medical literature relevant to germline genetics is growing exponentially. Clinicians need tools monitoring and prioritizing the literature to understand the clinical implications of the pathogenic genetic variants. We developed and evaluated two machine learning models to classify abstracts as relevant to the penetrance (risk of cancer for germline mutation carriers) or prevalence of germline genetic mutations. METHODS: We conducted literature searches in PubMed and retrieved paper titles and abstracts to create an annotated dataset for training and evaluating the two machine learning classification models. Our first model is a support vector machine (SVM) which learns a linear decision rule based on the bag-of-ngrams representation of each title and abstract. Our second model is a convolutional neural network (CNN) which learns a complex nonlinear decision rule based on the raw title and abstract. We evaluated the performance of the two models on the classification of papers as relevant to penetrance or prevalence. RESULTS: For penetrance classification, we annotated 3740 paper titles and abstracts and used 60% for training the model, 20% for tuning the model, and 20% for evaluating the model. The SVM model achieves 89.53% accuracy (percentage of papers that were correctly classified) while the CNN model achieves 88.95 % accuracy. For prevalence classification, we annotated 3753 paper titles and abstracts. The SVM model achieves 89.14% accuracy while the CNN model achieves 89.13 % accuracy. CONCLUSION: Our models achieve high accuracy in classifying abstracts as relevant to penetrance or prevalence. By facilitating literature review, this tool could help clinicians and researchers keep abreast of the burgeoning knowledge of gene-cancer associations and keep the knowledge bases for clinical decision support tools up to date.',\n",
       " 'How do we know if a particular medical treatment actually works? Ideally one would consult all available evidence from relevant clinical trials. Unfortunately, such results are primarily disseminated in natural language scientific articles, imposing substantial burden on those trying to make sense of them. In this paper, we present a new task and corpus for making this unstructured evidence actionable. The task entails inferring reported findings from a full-text article describing a randomized controlled trial (RCT) with respect to a given intervention, comparator, and outcome of interest, e.g., inferring if an article provides evidence supporting the use of aspirin to reduce risk of stroke, as compared to placebo.\\n  We present a new corpus for this task comprising 10,000+ prompts coupled with full-text articles describing RCTs. Results using a suite of models --- ranging from heuristic (rule-based) approaches to attentive neural architectures --- demonstrate the difficulty of the task, which we believe largely owes to the lengthy, technical input texts. To facilitate further work on this important, challenging problem we make the corpus, documentation, a website and leaderboard, and code for baselines and evaluation available at http://evidence-inference.ebm-nlp.com/.',\n",
       " 'Advancements in neural machinery have led to a wide range of algorithmic solutions for molecular property prediction. Two classes of models in particular have yielded promising results: neural networks applied to computed molecular fingerprints or expert-crafted descriptors, and graph convolutional neural networks that construct a learned molecular representation by operating on the graph structure of the molecule. However, recent literature has yet to clearly determine which of these two methods is superior when generalizing to new chemical space. Furthermore, prior research has rarely examined these new models in industry research settings in comparison to existing employed models. In this paper, we benchmark models extensively on 19 public and 16 proprietary industrial datasets spanning a wide variety of chemical endpoints. In addition, we introduce a graph convolutional model that consistently matches or outperforms models using fixed molecular descriptors as well as previous graph neural architectures on both public and proprietary datasets. Our empirical findings indicate that while approaches based on these representations have yet to reach the level of experimental reproducibility, our proposed model nevertheless offers significant improvements over models currently used in industrial workflows.',\n",
       " 'We introduce a novel method for multilingual transfer that utilizes deep contextual embeddings, pretrained in an unsupervised fashion. While contextual embeddings have been shown to yield richer representations of meaning compared to their static counterparts, aligning them poses a challenge due to their dynamic nature. To this end, we construct context-independent variants of the original monolingual spaces and utilize their mapping to derive an alignment for the context-dependent spaces. This mapping readily supports processing of a target language, improving transfer by context-aware embeddings. Our experimental results demonstrate the effectiveness of this approach for zero-shot and few-shot learning of dependency parsing. Specifically, our method consistently outperforms the previous state-of-the-art on 6 tested languages, yielding an improvement of 6.8 LAS points on average.',\n",
       " 'We view molecular optimization as a graph-to-graph translation problem. The goal is to learn to map from one molecular graph to another with better properties based on an available corpus of paired molecules. Since molecules can be optimized in different ways, there are multiple viable translations for each input graph. A key challenge is therefore to model diverse translation outputs. Our primary contributions include a junction tree encoder-decoder for learning diverse graph translations along with a novel adversarial training method for aligning distributions of molecules. Diverse output distributions in our model are explicitly realized by low-dimensional latent vectors that modulate the translation process. We evaluate our model on multiple molecular optimization tasks and show that our model outperforms previous state-of-the-art baselines.',\n",
       " 'Most modern Information Extraction (IE) systems are implemented as sequential taggers and only model local dependencies. Non-local and non-sequential context is, however, a valuable source of information to improve predictions. In this paper, we introduce GraphIE, a framework that operates over a graph representing a broad set of dependencies between textual units (i.e. words or sentences). The algorithm propagates information between connected nodes through graph convolutions, generating a richer representation that can be exploited to improve word-level predictions. Evaluation on three different tasks --- namely textual, social media and visual information extraction --- shows that GraphIE consistently outperforms the state-of-the-art sequence tagging model by a significant margin.',\n",
       " 'We propose a mixture-of-experts approach for unsupervised domain adaptation from multiple sources. The key idea is to explicitly capture the relationship between a target example and different source domains. This relationship, expressed by a point-to-set metric, determines how to combine predictors trained on various domains. The metric is learned in an unsupervised fashion using meta-training. Experimental results on sentiment analysis and part-of-speech tagging demonstrate that our approach consistently outperforms multiple baselines and can robustly handle negative transfer.',\n",
       " 'Attention-based models are successful when trained on large amounts of data. In this paper, we demonstrate that even in the low-resource scenario, attention can be learned effectively. To this end, we start with discrete human-annotated rationales and map them into continuous attention. Our central hypothesis is that this mapping is general across domains, and thus can be transferred from resource-rich domains to low-resource ones. Our model jointly learns a domain-invariant representation and induces the desired mapping between rationales and attention. Our empirical results validate this hypothesis and show that our approach delivers significant gains over state-of-the-art baselines, yielding over 15% average error reduction on benchmark datasets.',\n",
       " 'In this position paper, we describe our vision of the future of machine programming through a categorical examination of three pillars of research. Those pillars are: (i) intention, (ii) invention, and(iii) adaptation. Intention emphasizes advancements in the human-to-computer and computer-to-machine-learning interfaces. Invention emphasizes the creation or refinement of algorithms or core hardware and software building blocks through machine learning (ML). Adaptation emphasizes advances in the use of ML-based constructs to autonomously evolve software.',\n",
       " 'We seek to automate the design of molecules based on specific chemical properties. In computational terms, this task involves continuous embedding and generation of molecular graphs. Our primary contribution is the direct realization of molecular graphs, a task previously approached by generating linear SMILES strings instead of graphs. Our junction tree variational autoencoder generates molecular graphs in two phases, by first generating a tree-structured scaffold over chemical substructures, and then combining them into a molecule with a graph message passing network. This approach allows us to incrementally expand molecules while maintaining chemical validity at every step. We evaluate our model on multiple tasks ranging from molecular generation to optimization. Across these tasks, our model outperforms previous state-of-the-art baselines by a significant margin.',\n",
       " 'The prediction of organic reaction outcomes is a fundamental problem in computational chemistry. Since a reaction may involve hundreds of atoms, fully exploring the space of possible transformations is intractable. The current solution utilizes reaction templates to limit the space, but it suffers from coverage and efficiency issues. In this paper, we propose a template-free approach to efficiently explore the space of product molecules by first pinpointing the reaction center -- the set of nodes and edges where graph edits occur. Since only a small number of atoms contribute to reaction center, we can directly enumerate candidate products. The generated candidates are scored by a Weisfeiler-Lehman Difference Network that models high-order interactions between changes occurring at nodes across the molecule. Our framework outperforms the top-performing template-based approach with a 10\\\\% margin, while running orders of magnitude faster. Finally, we demonstrate that the model accuracy rivals the performance of domain experts.',\n",
       " 'In this paper, we explore the utilization of natural language to drive transfer for reinforcement learning (RL). Despite the wide-spread application of deep RL techniques, learning generalized policy representations that work across domains remains a challenging problem. We demonstrate that textual descriptions of environments provide a compact intermediate channel to facilitate effective policy transfer. Specifically, by learning to ground the meaning of text to the dynamics of the environment such as transitions and rewards, an autonomous agent can effectively bootstrap policy learning on a new domain given its description. We employ a model-based RL approach consisting of a differentiable planning module, a model-free component and a factorized state representation to effectively use entity descriptions. Our model outperforms prior work on both transfer and multi-task scenarios in a variety of different environments. For instance, we achieve up to 14% and 11.5% absolute improvement over previously existing models in terms of average and initial rewards, respectively.',\n",
       " 'The interpretation of spatial references is highly contextual, requiring joint inference over both language and the environment. We consider the task of spatial reasoning in a simulated environment, where an agent can act and receive rewards. The proposed model learns a representation of the world steered by instruction text. This design allows for precise alignment of local neighborhoods with corresponding verbalizations, while also handling global references in the instructions. We train our model with reinforcement learning using a variant of generalized value iteration. The model outperforms state-of-the-art approaches on several metrics, yielding a 45% reduction in goal localization error.',\n",
       " 'This paper focuses on style transfer on the basis of non-parallel text. This is an instance of a broad family of problems including machine translation, decipherment, and sentiment modification. The key challenge is to separate the content from other aspects such as style. We assume a shared latent content distribution across different text corpora, and propose a method that leverages refined alignment of latent representations to perform style transfer. The transferred sentences from one style should match example sentences from the other style as a population. We demonstrate the effectiveness of this cross-alignment method on three tasks: sentiment modification, decipherment of word substitution ciphers, and recovery of word order.',\n",
       " 'The design of neural architectures for structured objects is typically guided by experimental insights rather than a formal process. In this work, we appeal to kernels over combinatorial structures, such as sequences and graphs, to derive appropriate neural operations. We introduce a class of deep recurrent neural operations and formally characterize their associated kernel spaces. Our recurrent modules compare the input to virtual reference objects (cf. filters in CNN) via the kernels. Similar to traditional neural operations, these reference objects are parameterized and directly optimized in end-to-end training. We empirically evaluate the proposed class of neural architectures on standard applications such as language modeling and molecular graph regression, achieving state-of-the-art results across these applications.',\n",
       " 'This paper focuses on unsupervised modeling of morphological families, collectively comprising a forest over the language vocabulary. This formulation enables us to capture edgewise properties reflecting single-step morphological derivations, along with global distributional properties of the entire forest. These global properties constrain the size of the affix set and encourage formation of tight morphological families. The resulting objective is solved using Integer Linear Programming (ILP) paired with contrastive estimation. We train the model by alternating between optimizing the local log-linear model and the global ILP objective. We evaluate our system on three tasks: root detection, clustering of morphological families and segmentation. Our experiments demonstrate that our model yields consistent gains in all three tasks compared with the best published results.',\n",
       " 'We introduce a neural method for transfer learning between two (source and target) classification tasks or aspects over the same domain. Rather than training on target labels, we use a few keywords pertaining to source and target aspects indicating sentence relevance instead of document class labels. Documents are encoded by learning to embed and softly select relevant sentences in an aspect-dependent manner. A shared classifier is trained on the source encoded documents and labels, and applied to target encoded documents. We ensure transfer through aspect-adversarial training so that encoded documents are, as sets, aspect-invariant. Experimental results demonstrate that our approach outperforms different baselines and model variants on two datasets, yielding an improvement of 27% on a pathology dataset and 5% on a review dataset.',\n",
       " 'This paper explores the task of translating natural language queries into regular expressions which embody their meaning. In contrast to prior work, the proposed neural model does not utilize domain-specific crafting, learning to translate directly from a parallel corpus. To fully explore the potential of neural models, we propose a methodology for collecting a large corpus of regular expression, natural language pairs. Our resulting model achieves a performance gain of 19.6% over previous state-of-the-art models.',\n",
       " 'We present a novel technique for automatic program correction in MOOCs, capable of fixing both syntactic and semantic errors without manual, problem specific correction strategies. Given an incorrect student program, it generates candidate programs from a distribution of likely corrections, and checks each candidate for correctness against a test suite.\\n  The key observation is that in MOOCs many programs share similar code fragments, and the seq2seq neural network model, used in the natural-language processing task of machine translation, can be modified and trained to recover these fragments.\\n  Experiment shows our scheme can correct 29% of all incorrect submissions and out-performs state of the art approach which requires manual, problem specific correction strategies.',\n",
       " 'Prediction without justification has limited applicability. As a remedy, we learn to extract pieces of input text as justifications -- rationales -- that are tailored to be short and coherent, yet sufficient for making the same prediction. Our approach combines two modular components, generator and encoder, which are trained to operate well together. The generator specifies a distribution over text fragments as candidate rationales and these are passed through the encoder for prediction. Rationales are never given during training. Instead, the model is regularized by desiderata for rationales. We evaluate the approach on multi-aspect sentiment analysis against manually annotated test cases. Our approach outperforms attention-based baseline by a significant margin. We also successfully illustrate the method on the question retrieval task.',\n",
       " 'Most successful information extraction systems operate with access to a large collection of documents. In this work, we explore the task of acquiring and incorporating external evidence to improve extraction accuracy in domains where the amount of training data is scarce. This process entails issuing search queries, extraction from new sources and reconciliation of extracted values, which are repeated until sufficient evidence is collected. We approach the problem using a reinforcement learning framework where our model learns to select optimal actions based on contextual information. We employ a deep Q-network, trained to optimize a reward function that reflects extraction accuracy while penalizing extra effort. Our experiments on two databases -- of shooting incidents, and food adulteration cases -- demonstrate that our system significantly outperforms traditional extractors and a competitive meta-classifier baseline.',\n",
       " 'Question answering forums are rapidly growing in size with no effective automated ability to refer to and reuse answers already available for previous posted questions. In this paper, we develop a methodology for finding semantically related questions. The task is difficult since 1) key pieces of information are often buried in extraneous details in the question body and 2) available annotations on similar questions are scarce and fragmented. We design a recurrent and convolutional model (gated convolution) to effectively map questions to their semantic representations. The models are pre-trained within an encoder-decoder framework (from body to title) on the basis of the entire raw corpus, and fine-tuned discriminatively from limited annotations. Our evaluation demonstrates that our model yields substantial gains over a standard IR baseline and various neural network architectures (including CNNs, LSTMs and GRUs).',\n",
       " 'The success of deep learning often derives from well-chosen operational building blocks. In this work, we revise the temporal convolution operation in CNNs to better adapt it to text processing. Instead of concatenating word representations, we appeal to tensor algebra and use low-rank n-gram tensors to directly exploit interactions between words already at the convolution stage. Moreover, we extend the n-gram convolution to non-consecutive words to recognize patterns with intervening words. Through a combination of low-rank tensors, and pattern weighting, we can efficiently evaluate the resulting convolution operation via dynamic programming. We test the resulting architecture on standard sentiment classification and news categorization tasks. Our model achieves state-of-the-art performance both in terms of accuracy and training speed. For instance, we obtain 51.2% accuracy on the fine-grained sentiment classification task.',\n",
       " 'In this paper, we consider the task of learning control policies for text-based games. In these games, all interactions in the virtual world are through text and the underlying state is not observed. The resulting language barrier makes such environments challenging for automatic game players. We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback. This framework enables us to map text descriptions into vector representations that capture the semantics of the game states. We evaluate our approach on two game worlds, comparing against baselines using bag-of-words and bag-of-bigrams for state representations. Our algorithm outperforms the baselines on both worlds demonstrating the importance of learning expressive representations.',\n",
       " 'Most state-of-the-art systems today produce morphological analysis based only on orthographic patterns. In contrast, we propose a model for unsupervised morphological analysis that integrates orthographic and semantic views of words. We model word formation in terms of morphological chains, from base words to the observed words, breaking the chains into parent-child relations. We use log-linear models with morpheme and word-level features to predict possible parents, including their modifications, for each word. The limited set of candidate parents for each word render contrastive estimation feasible. Our model consistently matches or outperforms five state-of-the-art systems on Arabic, English and Turkish.',\n",
       " 'We present a model for aggregation of product review snippets by joint aspect identification and sentiment analysis.  Our model simultaneously identifies an underlying set of ratable aspects presented in the reviews of a product (e.g., sushi and miso for a Japanese restaurant) and determines the corresponding sentiment of each aspect.  This approach directly enables discovery of highly-rated or inconsistent aspects of a product.  Our generative model admits an efficient variational mean-field inference algorithm.  It is also easily extensible, and we describe several modifications and their effects on model structure and inference.  We test our model on two tasks, joint aspect identification and sentiment analysis on a set of Yelp reviews and aspect identification alone on a set of medical summaries.  We evaluate the performance of the model on aspect identification, sentiment analysis, and per-word labeling accuracy.  We demonstrate that our model outperforms applicable baselines by a considerable margin, yielding up to 32% relative error reduction on aspect identification and up to 20% relative error reduction on sentiment analysis.',\n",
       " 'We demonstrate the effectiveness of multilingual learning for unsupervised part-of-speech tagging. The central assumption of our work is that by combining cues from multiple languages, the structure of each becomes more apparent. We consider two ways of applying this intuition to the problem of unsupervised part-of-speech tagging: a model that directly merges tag structures for a pair of languages into a single sequence and a second model which instead incorporates multilingual context using latent variables. Both approaches are formulated as hierarchical Bayesian models, using Markov Chain Monte Carlo sampling techniques for inference. Our results demonstrate that by incorporating multilingual evidence we can achieve impressive performance gains across a range of scenarios. We also found that performance improves steadily as the number of available languages increases.',\n",
       " 'Domain knowledge is crucial for effective performance in autonomous control systems.  Typically, human effort is required to encode this knowledge into a control algorithm.  In this paper, we present an approach to language grounding which automatically interprets text in the context of a complex control application, such as a game, and uses domain knowledge extracted from the text to improve control performance.  Both text analysis and control strategies are learned jointly using only a feedback signal inherent to the application.  To effectively leverage textual information, our method automatically extracts the text segment most relevant to the current game state, and labels it with a task-centric predicate structure.  This labeled text is then used to bias an action selection policy for the game, guiding it towards promising regions of the action space.  We encode our model for text analysis and game playing in a multi-layer neural network, representing linguistic decisions via latent variables in the hidden layers, and game action quality via the output layer.  Operating within the Monte-Carlo Search framework, we estimate model parameters using feedback from simulated games.  We apply our approach to the complex strategy game Civilization II using the official game manual as the text guide.  Our results show that a linguistically-informed game-playing agent significantly outperforms its language-unaware counterpart, yielding a 34% absolute improvement and winning over 65% of games when playing against the built-in AI of Civilization.',\n",
       " 'We present a novel Bayesian topic model for learning discourse-level document structure. Our model leverages insights from discourse theory to constrain latent topic assignments in a way that reflects the underlying organization of document topics. We propose a global model in which both topic selection and ordering are biased to be similar across a collection of related documents. We show that this space of orderings can be effectively represented using a distribution over permutations called the Generalized Mallows Model. We apply our method to three complementary discourse-level tasks: cross-document alignment, document segmentation, and information ordering. Our experiments show that incorporating our permutation-based model in these applications yields substantial improvements in performance over previously proposed methods.',\n",
       " 'This paper presents a new method for inferring the semantic properties of documents by leveraging free-text keyphrase annotations.  Such annotations are becoming increasingly abundant due to the recent dramatic growth in semi-structured, user-generated online content. One especially relevant domain is product reviews, which are often annotated by their authors with pros/cons keyphrases such as a real bargain or good value. These annotations are representative of the underlying semantic properties; however, unlike expert annotations, they are noisy: lay authors may use different labels to denote the same property, and some labels may be missing.  To learn using such noisy annotations, we find a hidden paraphrase structure which clusters the keyphrases.  The paraphrase structure is linked with a latent topic model of the review texts, enabling the system to predict the properties of unannotated documents and to effectively aggregate the semantic properties of multiple reviews.  Our approach is implemented as a hierarchical Bayesian model with joint inference.  We find that joint inference increases the robustness of the keyphrase clustering and encourages the latent topics to correlate with semantically meaningful properties.  Multiple evaluations demonstrate that our model substantially outperforms alternative approaches for summarizing single and multiple documents into a set of semantically salient keyphrases.',\n",
       " '  We consider the problem of modeling the content structure of texts within a specific domain, in terms of the topics the texts address and the order in which these topics appear. We first present an effective knowledge-lean method for learning content models from un-annotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models. We then apply our method to two complementary tasks: information ordering and extractive summarization. Our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods.',\n",
       " '  We address the text-to-text generation problem of sentence-level paraphrasing -- a phenomenon distinct from and more difficult than word- or phrase-level paraphrasing. Our approach applies multiple-sequence alignment to sentences gathered from unannotated comparable corpora: it learns a set of paraphrasing patterns represented by word lattice pairs and automatically determines how to apply these patterns to rewrite new sentences. The results of our evaluation experiments show that the system derives accurate paraphrases, outperforming baseline systems.',\n",
       " '  An important component of any generation system is the mapping dictionary, a lexicon of elementary semantic expressions and corresponding natural language realizations. Typically, labor-intensive knowledge-based methods are used to construct the dictionary. We instead propose to acquire it automatically via a novel multiple-pass algorithm employing multiple-sequence alignment, a technique commonly used in bioinformatics. Crucially, our method leverages latent information contained in multi-parallel corpora -- datasets that supply several verbalizations of the corresponding semantics rather than just one.\\n  We used our techniques to generate natural language versions of computer-generated mathematical proofs, with good results on both a per-component and overall-output basis. For example, in evaluations involving a dozen human judges, our system produced output whose readability and faithfulness to the semantic input rivaled that of a traditional generation system.',\n",
       " \"Linking superconducting quantum devices to optical fibers via microwave-optical quantum transducers may enable large scale quantum networks. For this application, transducers based on the Pockels electro-optic (EO) effect are promising for their direct conversion mechanism, high bandwidth, and potential for low-noise operation. However, previously demonstrated EO transducers require large optical pump power to overcome weak EO coupling and reach high efficiency. Here, we create an EO transducer in thin-film lithium niobate, leveraging the low optical loss and strong EO coupling in this platform. We demonstrate a transduction efficiency of up to $2.7\\\\times10^{-5}$, and a pump-power normalized efficiency of $1.9\\\\times10^{-6}/\\\\mathrm{Î¼W}$. The transduction efficiency can be improved by further reducing the microwave resonator's piezoelectric coupling to acoustic modes, increasing the optical resonator quality factor to previously demonstrated levels, and changing the electrode geometry for enhanced EO coupling. We expect that with further development, EO transducers in thin-film lithium niobate can achieve near-unity efficiency with low optical pump power.\",\n",
       " 'Superconducting devices are prone to reduced performance caused by impurities and defects along the edges of their wires, which can lead to local current crowding. In this study, we explored the use of helium ion irradiation to modify the lattice structure of the superconducting material to change its intrinsic properties. The process will allow us to directly pattern devices and potentially improve the quality of the nanowires. To achieve this, we used the ion beam from a scanning helium ion microscope (HIM) to localize damage on a superconducting material to create a nanowire. Two experiments were performed in this study. First, a range of helium ion doses was exposed on a niobium nitride (NbN) microwire to determine the estimated dose density to suppress superconductivity. Using the results of this first experiment, nanowires were patterned onto a microwire, and the current-voltage characteristics were measured for each sample. Our results showed that helium ion irradiation is an effective resistless fabrication method for superconducting nanowires.',\n",
       " 'In this work, we demonstrate saturated internal detection efficiency at 1550 nm wavelengths for MoSi single-photon detectors with widths of 1 and 3 Î¼m, and active areas up to 400 by 400 Î¼m^2. Despite hairpin turns and a large number of squares in device structures, the dark count rate was measured in the range of 10^3 cps. This value is about two orders of magnitude lower than results reported recently for short MoSi devices with shunt resistors. We also found a thickness-dependent threshold for detection of single near-infrared photons by MoSi microwire detectors that may be associated with suppression the superconducting order parameter. Furthermore, we observed optical response in microwires with a lower ratio of switching to the depairing current than was predicted using a kinetic-equation approach [1].',\n",
       " 'Electron beams can acquire designed phase modulations by passing through nanostructured material phase plates. These phase modulations enable electron wavefront shaping and benefit electron microscopy, spectroscopy, lithography, and interferometry. However, in the fabrication of electron phase plates, the typically used focused-ion-beam-milling method limits the fabrication throughput and hence the active area of the phase plates. Here, we fabricated large-area electron phase plates with electron-beam lithography and reactive-ion-etching. The phase plates are characterized by electron diffraction in transmission electron microscopes with various electron energies, as well as diffractive imaging in a scanning electron microscope. We found the phase plates could produce a null in the center of the bright-field based on coherent interference of diffractive beams. Our work adds capabilities to the fabrication of electron phase plates. The nullification of the direct beam and the tunable diffraction efficiency demonstrated here also paves the way towards novel dark-field electron-microscopy techniques and tunable electron phase plates.',\n",
       " 'Ultrafast light-matter interactions lead to optical-field-driven photocurrents with an attosecond-level temporal response. These photocurrents can be used to detect the carrier-envelope-phase (CEP) of short optical pulses, and could be utilized to create optical-frequency, petahertz (PHz) electronics for information processing. Despite recent reports on optical-field-driven photocurrents in various nanoscale solid-state materials, little has been done in examining the large-scale integration of these devices. In this work, we demonstrate enhanced, on-chip CEP detection via optical-field-driven photocurrent in a monolithic array of electrically-connected plasmonic bow-tie nanoantennas that are contained within an area of hundreds of square microns. The technique is scalable and could potentially be used for shot-to-shot CEP tagging applications requiring orders of magnitude less pulse energy compared to alternative ionization-based techniques. Our results open new avenues for compact time-domain, on-chip CEP detection, and inform the development of integrated circuits for PHz electronics as well as integrated platforms for attosecond and strong-field science.',\n",
       " 'Just as classical information technology rests on a foundation built of interconnected information-processing systems, quantum information technology (QIT) must do the same. A critical component of such systems is the interconnect, a device or process that allows transfer of information between disparate physical media, for example, semiconductor electronics, individual atoms, light pulses in optical fiber, or microwave fields. While interconnects have been well engineered for decades in the realm of classical information technology, quantum interconnects (QuICs) present special challenges, as they must allow the transfer of fragile quantum states between different physical parts or degrees of freedom of the system. The diversity of QIT platforms (superconducting, atomic, solid-state color center, optical, etc.) that will form a quantum internet poses additional challenges. As quantum systems scale to larger size, the quantum interconnect bottleneck is imminent, and is emerging as a grand challenge for QIT. For these reasons, it is the position of the community represented by participants of the NSF workshop on Quantum Interconnects that accelerating QuIC research is crucial for sustained development of a national quantum science and technology program. Given the diversity of QIT platforms, materials used, applications, and infrastructure required, a convergent research program including partnership between academia, industry and national laboratories is required.\\n  This document is a summary from a U.S. National Science Foundation supported workshop held on 31 October - 1 November 2019 in Alexandria, VA. Attendees were charged to identify the scientific and community needs, opportunities, and significant challenges for quantum interconnects over the next 2-5 years.',\n",
       " 'Time- and number-resolved photon detection is crucial for photonic quantum information processing. Existing photon-number-resolving (PNR) detectors usually have limited timing and dark-count performance or require complex fabrication and operation. Here we demonstrate a PNR detector at telecommunication wavelengths based on a single superconducting nanowire with an integrated impedance-matching taper. The prototyping device was able to resolve up to five absorbed photons and had 16.1 ps timing jitter, <2 c.p.s. device dark count rate, $\\\\sim$86 ns reset time, and 5.6% system detection efficiency (without cavity) at 1550 nm. Its exceptional distinction between single- and two-photon responses is ideal for coincidence counting and allowed us to directly observe bunching of photon pairs from a single output port of a Hong-Ou-Mandel interferometer. This detector architecture may provide a practical solution to applications that require high timing resolution and few-photon discrimination.',\n",
       " 'In this work, we present a novel device that is a combination of a superconducting nanowire single-photon detector and a superconducting multi-level memory. We show that these devices can be used to count the number of detections through single-photon to single-flux conversion. Electrical characterization of the memory properties demonstrates single-flux quantum (SFQ) separated states. Optical measurements using attenuated laser pulses with different mean photon number, pulse energies and repetition rates are shown to differentiate single-photon detection from other possible phenomena, such as multi-photon detection and thermal activation. Finally, different geometries and material stacks to improve device performance, as well as arraying methods are discussed.',\n",
       " 'Training of deep neural networks (DNNs) is a computationally intensive task and requires massive volumes of data transfer. Performing these operations with the conventional von Neumann architectures creates unmanageable time and power costs. Recent studies have shown that mixed-signal designs involving crossbar architectures are capable of achieving acceleration factors as high as 30,000x over the state of the art digital processors. These approaches involve utilization of non-volatile memory (NVM) elements as local processors. However, no technology has been developed to-date that can satisfy the strict device requirements for the unit cell. This paper presents the superconducting nanowire-based processing element as a cross-point device. The unit cell has many programmable non-volatile states that can be used to perform analog multiplication. Importantly, these states are intrinsically discrete due to quantization of flux, which provides symmetric switching characteristics. Operation of these devices in a crossbar is described and verified with electro-thermal circuit simulations. Finally, validation of the concept in an actual DNN training task is shown using an emulator.',\n",
       " \"One of the most challenging obstacles to realizing exascale computing is minimizing the energy consumption of L2 cache, main memory, and interconnects to that memory. For promising cryogenic computing schemes utilizing Josephson junction superconducting logic, this obstacle is exacerbated by the cryogenic system requirements that expose the technology's lack of high-density, high-speed and power-efficient memory. Here we demonstrate an array of cryogenic memory cells consisting of a non-volatile three-terminal magnetic tunnel junction element driven by the spin Hall effect, combined with a superconducting heater-cryotron bit-select element. The write energy of these memory elements is roughly 8 pJ with a bit-select element, designed to achieve a minimum overhead power consumption of about 30%. Individual magnetic memory cells measured at 4 K show reliable switching with write error rates below $10^{-6}$, and a 4x4 array can be fully addressed with bit select error rates of $10^{-6}$. This demonstration is a first step towards a full cryogenic memory architecture targeting energy and performance specifications appropriate for applications in superconducting high performance and quantum computing control systems, which require significant memory resources operating at 4 K.\",\n",
       " \"With the rising societal demand for more information-processing capacity with lower power consumption, alternative architectures inspired by the parallelism and robustness of the human brain have recently emerged as possible solutions. In particular, spiking neural networks (SNNs) offer a bio-realistic approach, relying on pulses analogous to action potentials as units of information. While software encoded networks provide flexibility and precision, they are often computationally expensive. As a result, hardware SNNs based on the spiking dynamics of a device or circuit represent an increasingly appealing direction. Here, we propose to use superconducting nanowires as a platform for the development of an artificial neuron. Building on an architecture first proposed for Josephson junctions, we rely on the intrinsic nonlinearity of two coupled nanowires to generate spiking behavior, and use electrothermal circuit simulations to demonstrate that the nanowire neuron reproduces multiple characteristics of biological neurons. Furthermore, by harnessing the nonlinearity of the superconducting nanowire's inductance, we develop a design for a variable inductive synapse capable of both excitatory and inhibitory control. We demonstrate that this synapse design supports direct fanout, a feature that has been difficult to achieve in other superconducting architectures, and that the nanowire neuron's nominal energy performance is competitive with that of current technologies.\",\n",
       " 'Focused ion beam (FIB) microscopy suffers from source shot noise - random variation in the number of incident ions in any fixed dwell time - along with random variation in the number of detected secondary electrons per incident ion. This multiplicity of sources of randomness increases the variance of the measurements and thus worsens the trade-off between incident ion dose and image accuracy. Time-resolved sensing combined with maximum likelihood estimation from the resulting sets of measurements greatly reduces the effect of source shot noise. Through Fisher information analysis and Monte Carlo simulations, the reduction in mean-squared error or reduction in required dose is shown to be by a factor approximately equal to the secondary electron yield. Experiments with a helium ion microscope (HIM) are consistent with the analyses and suggest accuracy improvement for a fixed source dose, or reduced source dose for a desired imaging accuracy, by a factor of about 3.',\n",
       " 'We propose the use of superconducting nanowires as both target and sensor for direct detection of sub-GeV dark matter. With excellent sensitivity to small energy deposits on electrons, and demonstrated low dark counts, such devices could be used to probe electron recoils from dark matter scattering and absorption processes. We demonstrate the feasibility of this idea using measurements of an existing fabricated tungsten-silicide nanowire prototype with 0.8 eV energy threshold and 4.3 nanograms with 10 thousand seconds of exposure, which showed no dark counts. The results from this device already place meaningful bounds on dark matter-electron interactions, including the strongest terrestrial bounds on sub-eV dark photon absorption to date. Future expected fabrication on larger scales and with lower thresholds should enable probing new territory in the direct detection landscape, establishing the complementarity of this approach to other existing proposals.',\n",
       " 'Interaction-free measurement (IFM) has been proposed as a means of high-resolution, low-damage imaging of radiation-sensitive samples, such as biomolecules and proteins. The basic setup for IFM is a Mach-Zehnder interferometer, and recent progress in nanofabricated electron diffraction gratings has made it possible to incorporate a Mach-Zehnder interferometer in a transmission-electron microscope (TEM). Therefore, the limits of performance of IFM with such an interferometer and a shot-noise limited electron source (such as that in a TEM) are of interest. In this work, we compared the error probability and sample damage for ideal IFM and classical imaging schemes, through theoretical analysis and numerical simulation. We considered a sample that is either completely transparent or completely opaque at each pixel. In our analysis, we also evaluated the impact of an additional detector for scattered electrons. The additional detector resulted in reduction of error by up to an order of magnitude, for both IFM and classical schemes. We also investigated a sample re-illumination scheme based on updating priors after each round of illumination and found that this scheme further reduced error by a factor of two. Implementation of these methods is likely achievable with existing instrumentation and would result in improved resolution in low-dose electron microscopy.',\n",
       " 'Local, bulk response functions, e.g permittivity, and the macroscopic Maxwell equations completely specify the classical electromagnetic problem, which features only wavelength $Î»$ and geometric scales. The above neglect of intrinsic electronic length scales $L_{\\\\text{e}}$ leads to an eventual breakdown in the nanoscopic limit. Here, we present a general theoretical and experimental framework for treating nanoscale electromagnetic phenomena. The framework features surface-response functions---known as the Feibelman $d$-parameters---which reintroduce the missing electronic length scales. As a part of our framework, we establish an experimental procedure to measure these complex, dispersive surface response functions, enabled by quasi-normal-mode perturbation theory and observations of pronounced nonclassical effects---spectral shifts in excess of 30% and the breakdown of Kreibig-like broadening---in a quintessential multiscale architecture: film-coupled nanoresonators, with feature-sizes comparable to both $L_{\\\\text{e}}$ and $Î»$.',\n",
       " 'We present the use of a commercially available fixed-angle multi-wavelength ellipsometer for quickly measuring the thickness of NbN thin films for the fabrication and performance improvement of superconducting nanowire single photon detectors. The process can determine the optical constants of absorbing thin films, removing the need for inaccurate approximations. The tool can be used to observe oxidation growth and allows thickness measurements to be integrated into the characterization of various fabrication processes.',\n",
       " 'The method of negative-tone-PMMA electron-beam lithography is investigated to improve the performance of nanowire-based superconducting detectors. Using this approach, the superconducting nanowire single-photon detectors (SNSPDs) have been fabricated from thick 5-nm NbN film sputtered at the room temperature. To investigate the impact of this process, SNSPDs were prepared by positive-tone and negative-tone-PMMA lithography, and their electrical and photodetection characteristics at 4.2 K were compared. The SNSPDs made by negative-tone-PMMA lithography show higher critical-current density and higher photon count rate at various wavelengths. Our results suggest a higher negative-tone-PMMA technology may be preferable to the standard positive-tone-PMMA lithography for this application.',\n",
       " 'Conventional readout of a superconducting nanowire single-photon detector (SNSPD) sets an upper bound on the output voltage to be the product of the bias current and the load impedance, $I_\\\\mathrm{B}\\\\times Z_\\\\mathrm{load}$, where $Z_\\\\mathrm{load}$ is limited to 50 $Î©$ in standard r.f. electronics. Here, we break this limit by interfacing the 50 $Î©$ load and the SNSPD using an integrated superconducting transmission line taper. The taper is a transformer that effectively loads the SNSPD with high impedance without latching. It increases the amplitude of the detector output while preserving the fast rising edge. Using a taper with a starting width of 500 nm, we experimentally observed a 3.6$\\\\times$ higher pulse amplitude, 3.7$\\\\times$ faster slew rate, and 25.1 ps smaller timing jitter. The results match our numerical simulation, which incorporates both the hotspot dynamics in the SNSPD and the distributed nature in the transmission line taper. The taper studied here may become a useful tool to interface high-impedance superconducting nanowire devices to conventional low-impedance circuits.',\n",
       " 'The basis for superconducting electronics can broadly be divided between two technologies: the Josephson junction and the superconducting nanowire. While the Josephson junction (JJ) remains the dominant technology due to its high speed and low power dissipation, recently proposed nanowire devices offer improvements such as gain, high fanout, and compatibility with CMOS circuits. Despite these benefits, nanowire-based electronics have largely been limited to binary operations, with devices switching between the superconducting state and a high-impedance resistive state dominated by uncontrolled hotspot dynamics. Unlike the JJ, they cannot increment an output through successive switching, and their operation speeds are limited by their slow thermal reset times. Thus, there is a need for an intermediate device with the interfacing capabilities of a nanowire but a faster, moderated response allowing for modulation of the output. Here, we present a nanowire device based on controlled fluxon transport. We show that the device is capable of responding proportionally to the strength of its input, unlike other nanowire technologies. The device can be operated to produce a multilevel output with distinguishable states, which can be tuned by circuit parameters. Agreement between experimental results and electrothermal circuit simulations demonstrates that the device is classical and may be readily engineered for applications including use as a multilevel memory.',\n",
       " 'Semi-transparent mirrors are standard elements in light optics for splitting light beams or creating two versions of the same image. Such mirrors do not exist in electron optics, although they could be beneficial in existing techniques such as electron interferometry and holography and enable novel electron imaging and spectroscopy techniques. We propose a design for an electron beam splitter using the concept of quantum interaction-free measurement (IFM). The design combines an electron resonator with a weak phase grating. Fast switching gates allow electrons to enter and exit the resonator. While in the resonator, the phase grating transfers intensity from the direct beam into one of the weakly diffracted beams at each pass. To make the beam splitter an efficient two-port splitter, the intensity in all other diffracted beams is blocked by an aperture. The IFM principle minimizes the loss of total intensity by this aperture. We use a scattering matrix method to analyze the performance of the beam splitter, including the effects of inelastic scattering in the phase grating. This design can be generalized to beam splitters for not only electrons, but also photons, neutrons, atoms, and other quantum mechanical systems.',\n",
       " 'To analyze the switching dynamics and output performance of a superconducting nanowire single photon detector (SNSPD), the nanowire is usually modelled as an inductor in series with a time-varying resistor induced by absorption of a photon. Our recent experimental results show that, due to the effect of kinetic inductance, for a SNSPD made of a nanowire of sufficient length, its geometry length can be comparable to or even longer than the effective wavelength of frequencies contained in the output pulse. In other words, a superconducting nanowire can behave as a distributed transmission line so that the readout pulse depends on the photon detection location and the transmission line properties of the nanowire. Here, we develop a distributed model for a superconducting nanowire and apply it to simulate the output performance of a long nanowire designed into a coplanar waveguide. We compare this coplanar waveguide geometry to a conventional meander nanowire geometry. The simulation results agree well with our experimental observations. With this distributed model, we discussed the importance of microwave design of a nanowire and how impedance matching can affect the output pulse shape. We also discuss how the distributed model affects the growth and decay of the photon-triggered resistive hotspot.',\n",
       " 'We analyze the origin of the intrinsic timing jitter in superconducting nanowire single photon detectors (SNSPDs) in terms of fluctuations in the latency of the detector response, which is determined by the microscopic physics of the photon detection process. We demonstrate that fluctuations in the physical parameters which determine the latency give rise to the intrinsic timing jitter. We develop a general description of latency by introducing the explicit time dependence of the internal detection efficiency. By considering the dynamic Fano fluctuations together with static spatial inhomogeneities, we study the details of the connection between latency and timing jitter. We develop both a simple phenomenological model and a more general microscopic model of detector latency and timing jitter based on the solution of the generalized time-dependent Ginzburg-Landau equations for the 1D hotbelt geometry. While the analytical model is sufficient for qualitative interpretation of recent data, the general approach establishes the framework for a quantitative analysis of detector latency and the fundamental limits of intrinsic timing jitter. These theoretical advances can be used to interpret the results of recent experiments measuring the dependence of detection latency and timing jitter on photon energy to the few-picosecond level.',\n",
       " 'Report of the first workshop to identify approaches and techniques in the domain of quantum sensing that can be utilized by future High Energy Physics applications to further the scientific goals of High Energy Physics.',\n",
       " 'The International Linear Collider is now proposed with a staged machine design, with the first stage at $\\\\sqrt{s}=$~250 GeV and an integrated luminosity goal of 2~ab$^{-1}$. One of the questions for the machine design is the importance of positron polarization. In this report, we review the impact of positron polarization on the physics goals of the $250$ GeV stage of the ILC and demonstrate that positron polarization has distinct advantages.',\n",
       " 'Coincidence detection of single photons is crucial in numerous quantum technologies and usually requires multiple time-resolved single-photon detectors. However, the electronic readout becomes a major challenge when the measurement basis scales to large numbers of spatial modes. Here, we address this problem by introducing a two-terminal coincidence detector that enables scalable readout of an array of detector segments based on superconducting nanowire microstrip transmission line. Exploiting timing logic, we demonstrate a 16-element detector that resolves all 136 possible single-photon and two-photon coincidence events. We further explore the pulse shapes of the detector output and resolve up to four-photon coincidence events in a 4-element device, giving the detector photon-number-resolving capability. This new detector architecture and operating scheme will be particularly useful for multi-photon coincidence detection in large-scale photonic integrated circuits.',\n",
       " 'A superconducting loop stores persistent current without any ohmic loss, making it an ideal platform for energy efficient memories. Conventional superconducting memories use an architecture based on Josephson junctions (JJs) and have demonstrated access times less than 10 ps and power dissipation as low as $10^{-19}$ J. However, their scalability has been slow to develop due to the challenges in reducing the dimensions of JJs and minimizing the area of the superconducting loops. In addition to the memory itself, complex readout circuits require additional JJs and inductors for coupling signals, increasing the overall area. Here, we have demonstrated a superconducting memory based solely on lithographic nanowires. The small dimensions of the nanowire ensure that the device can be fabricated in a dense area in multiple layers, while the high kinetic inductance makes the loop essentially independent of geometric inductance, allowing it to be scaled down without sacrificing performance. The memory is operated by a group of nanowire cryotrons patterned alongside the storage loop, enabling us to reduce the entire memory cell to 3 Î¼m $\\\\times $ 7 Î¼m in our proof-of-concept device. In this work we present the operation principles of a superconducting nanowire memory (nMem) and characterize its bit error rate, speed, and power dissipation.',\n",
       " 'We present the performance of a superconducting nanowire that can be operated in two detection modes: i) as a kinetic inductance detector (KID) or ii) as a single-photon detector (SPD). Two superconducting nanowires developed for use as single-photon detectors (SNSPDs) are embedded as the inductive (L) component in resonant inductor/capacitor (LC) circuits coupled to a microwave transmission line. The capacitors are low loss commercial chip capacitors and limit the internal quality factor of the resonators to approximately $Q_i = 170$. The resonator quality factor, $Q_r \\\\simeq 23$, is dominated by the coupling to the feedline and limits the detection bandwidth to on the order of 1MHz. When operated in KID mode, the detectors are AC biased with tones at their resonant frequencies of 45.85 and 91.81MHz. In the low-bias, standard KID mode, a single photon produces a hot spot that does not turn an entire section of the line normal but only increases the kinetic inductance. In the high-bias, critical KID mode, a photon event turns a section of the line normal and the resonance is destroyed until the normal region is dissipated. When operated as an SPD in Geiger mode, the resonators are DC biased through cryogenic bias tees and each photon produces a sharp voltage step followed by a ringdown signal at the resonant frequency of the detector which is converted to a standard pulse with an envelop detector. We show that AC biasing in the critical KID mode is inferior to the sensitivity achieved in DC-biased SPD mode due to the small fraction of time spent near the critical current with an AC bias.',\n",
       " 'The International Linear Collider is now proposed with a staged machine design, with the first stage at 250 GeV with a luminosity goal of 2 ab-1. In this paper, we review the physics expectations for this machine. These include precision measurements of Higgs boson couplings, searches for exotic Higgs decays, other searches for particles that decay with zero or small visible energy, and measurements of e+e- annihilation to W+W- and 2-fermion states with improved sensitivity. A summary table gives projections for the achievable levels of precision based on the latest full simulation studies.',\n",
       " 'Recent advances in the fabrication of nanostructures and nanoscale features in metasurfaces offer a new prospect for generating visible, light emission from low energy electrons. In this paper, we present the experimental observation of visible light emission from low-energy free electrons interacting with nanoscale periodic surfaces through the Smith-Purcell (SP) effect. SP radiation is emitted when electrons pass in close proximity over a periodic structure, inducing collective charge motion or dipole excitations near the surface, thereby giving rise to electromagnetic radiation. We demonstrate a controlled emission of SP light from nanoscale gold gratings with periodicity as small as 50 nm, enabling the observation of visible SP radiation by low energy electrons (1.5 to 6 keV), an order of magnitude lower than previously reported. We study the emission wavelength and intensity dependence on the grating pitch and electron energy, showing agreement between experiment and theory. Further reduction of structure periodicity should enable the production of SP-based devices that operate with even slower electrons that allow an even smaller footprint and facilitate the investigation of quantum effects for light generation in nanoscale devices. A tunable light source integrated in an electron microscope would enable the development of novel electron-optical correlated spectroscopic techniques, with additional applications ranging from biological imaging to solid-state lighting.',\n",
       " 'Many superconducting technologies such as rapid single flux quantum computing (RSFQ) and superconducting quantum interference devices (SQUIDs) rely on the modulation of nonlinear dynamics in Josephson junctions for functionality. More recently, however, superconducting devices have been developed based on the switching and thermal heating of nanowires for use in fields such as single photon detection and digital logic. In this paper, we use resistive shunting to control the nonlinear heating of a superconducting nanowire and compare the resulting dynamics to those observed in Josephson junctions. We show that interaction of the hotspot growth with the external shunt produces high frequency relaxation oscillations with similar behavior as observed in Josephson junctions due to their rapid time constants and ability to be modulated by a weak periodic signal. In particular, we use a microwave drive to pull and mix the oscillation frequency, resulting in phase locked features that resemble the AC Josephson effect. New nanowire devices based on these conclusions have promising applications in fields such as parametric amplification and frequency multiplexing.',\n",
       " 'The lack of energy dissipation and abrupt electrical phase transition of superconductors favorite them for nanoscale technologies, including radiation detectors, and quantum technologies. Moreover, understanding the nanoscale behavior of superconductivity is significant for revealing the onset of collective-electron behavior in nature. Nevertheless, the limited number of accessible superconductors restricts availability of the superconducting properties, encumbering the realization of their potential. Superconducting nanowire single photon detectors (SNSPDs) sense single-IR photons faster and more efficient with respect to competing technologies. However, these advantageous properties are material-dependent causing an undesirable speed-efficiency payoff. Usually, SNSPDs based on granular materials are faster, while those based on amorphous materials are more efficient. Here we optimized ultrathin films of granular NbN on SiO2 and of amorphous W5Si3. We showed that hybrid superconducting nanowire single photon detectors (SNSPDs) made of 2-nm-thick W5Si3 films over 2-nm-thick NbN films exhibit advantageous coexistence of timing (< 5-ns reset time and 52-ps timing jitter) and efficiency (> 96% quantum efficiency) performance. We propose that the governing mechanism of this hybridization is the presence of a dual superconducting behavior: native superconductivity of each of the films and superconductivity that is induced from the neighboring film via the proximity effect. In addition to improvement in SNSPDs performance, our results suggest that such hybridization can expand the range of available superconducting properties, impacting nano-superconducting technologies. Lastly, this hybridization may be used to tune the amorphous character of superconducting films and to illuminate the elusive onset of collective-electron behavior near the superconducting-to-insulating transition.',\n",
       " 'Integration with conventional electronics offers a straightforward and economical approach to upgrading existing superconducting technologies, such as scaling up superconducting detectors into large arrays and combining single flux quantum (SFQ) digital circuits with semiconductor logic and memories. However, direct output signals from superconducting devices (e.g., Josephson junctions) are usually not compatible with the input requirements of conventional devices (e.g., transistors). Here, we demonstrate the use of a single three-terminal superconducting-nanowire device, called the nanocryotron (nTron), as a digital comparator to combine SFQ circuits with mature semiconductor circuits such as complementary metal oxide semiconductor (CMOS) circuits. Since SFQ circuits can digitize output signals from general superconducting devices and CMOS circuits can interface existing CMOS-compatible electronics, our results demonstrate the feasibility of a general architecture that uses an nTron as an interface to realize a super-hybrid system consisting of superconducting detectors, superconducting quantum electronics, CMOS logic and memories, and other conventional electronics.',\n",
       " 'We report a self-aligned, monolithic electron interferometer, consisting of two 45 nm thick silicon layers separated by 20 $Î¼$m. This interferometer was fabricated from a single crystal silicon cantilever on a transmission electron microscope grid by gallium focused ion-beam milling. Using this interferometer, we demonstrate beam path-separation, and obtain interference fringes in a Mach-Zehnder geometry, in an unmodified 200 kV transmission electron microscope. The fringes have a period of 0.32 nm, which corresponds to the $\\\\left[\\\\bar{1}\\\\bar{1}1\\\\right]$ lattice planes of silicon, and a maximum contrast of 15 %. This design can potentially be scaled to millimeter-scale, and used in electron holography. It can also be applied to perform fundamental physics experiments, such as interaction-free measurement with electrons.',\n",
       " 'Detection jitter quantifies variance introduced by the detector in the determination of photon arrival time. It is a crucial performance parameter for systems using superconducting nanowire single photon detectors (SNSPDs). In this work, we have demonstrated that the detection timing jitter is limited in part by the spatial variation of photon detection events along the length of the wire. This distribution causes the generated electrical pulses to arrive at the readout at varied times. We define this jitter source as geometric jitter since it is related to the length and area of the SNSPD. To characterize the geometric jitter, we have constructed a novel differential cryogenic readout with less than 7 ps of electronic jitter that can amplify the pulses generated from the two ends of an SNSPD. By differencing the measured arrival times of the two electrical pulses, we were able to partially cancel out the difference of the propagation times and thus reduce the uncertainty of the photon arrival time. Our experimental data indicates that the variation of the differential propagation time was a few ps for a 3 Î¼m x 3 Î¼m device while it increased up to 50 ps for a 20 Î¼m x 20 Î¼m device. In a 20 Î¼m x 20 Î¼m large SNSPD, we achieved a 20% reduction in the overall detection timing jitter for detecting telecom-wavelength photons by using the differential cryogenic readout. The geometric jitter hypothesis was further confirmed by studying jitter in devices that consisted of long wires with 1-Î¼m-long narrowed regions used for sensing photons.',\n",
       " 'We describe a superconducting three-terminal device that uses a simple geometric effect known as current crowding to sense the flow of current and actuate a readout signal. The device consists of a \"Y\"-shaped current combiner, with two currents (sense and bias) entering through the top arms of the \"Y\", intersecting, and then exiting through the bottom leg of the \"Y\"\\'. This geometry--mixing two inputs at a sharp intersection point--takes its inspiration from Y-shaped combiners in fluid flow systems, where variations in the input pressures can produce at turbulence and mixing at the intersection. When current is added to or removed from one of the arms (the sense arm), the superconducting critical current in the other arm (the bias arm) is modulated. The current in the sense arm can thus be determined by measuring the critical current of the bias arm. The dependence of the bias critical current on the sense current is possible because current crowding causes the sense current to interact locally with the bias arm. Measurement of the critical current in the bias arm does not break the superconducting state of the sense arm or of the bottom leg, and thus the signal to be sensed is fully restored after the measurement process. This device thus has potential for broad applicability across superconducting technologies and materials.',\n",
       " \"Detecting spatial and temporal information of individual photons by using single-photon-detector (SPD) arrays is critical to applications in spectroscopy, communication, biological imaging, astronomical observation, and quantum-information processing. Among the current SPDs1,detectors based on superconducting nanowires have outstanding performance2, but are limited in their ability to be integrated into large scale arrays due to the engineering difficulty of high-bandwidth cryogenic electronic readout3-8. Here, we address this problem by demonstrating a scalable single-photon imager using a single continuous photon-sensitive superconducting nanowire microwave-plasmon transmission line. By appropriately designing the nanowire's local electromagnetic environment so that the nanowire guides microwave plasmons, the propagating voltages signals generated by a photon-detection event were slowed down to ~ 2% of the speed of light. As a result, the time difference between arrivals of the signals at the two ends of the nanowire naturally encoded the position and time of absorption of the photon. Thus, with only two readout lines, we demonstrated that a 19.7-mm-long nanowire meandered across an area of 286 Î¼m * 193 Î¼m was capable of resolving ~590 effective pixels while simultaneously recording the arrival times of photons with a temporal resolution of 50 ps. The nanowire imager presents a scalable approach to realizing high-resolution photon imaging in time and space.\",\n",
       " \"We study the microwave impedance of extremely high aspect ratio (length/width ~ 5,000) superconducting niobium nitride nanowires. The nanowires are fabricated in a compact meander geometry that is in series with the center conductor of a 50 ohm coplanar waveguide transmission line. The transmission coefficient of the sample is measured up to 20 GHz. At high frequency, a peak in the transmission coefficient is seen. Numerical simulations show that this is a half-wave resonance along the length of the nanowire, where the nanowire acts as a high impedance, slow wave transmission line. This resonance sets the upper frequency limit for these nanowires as inductive elements. Fitting simulations to the measured resonance enables a precise determination of the nanowire's complex sheet impedance at the resonance frequency. The real part is a measure of dissipation, while the imaginary part is dominated by kinetic inductance. We characterize the dependence of the sheet resistance and sheet inductance on both temperature and current and compare the results to recent theoretical predictions for disordered superconductors. These results can aid in the understanding of high frequency devices based on superconducting nanowires. They may also lead to the development of novel superconducting devices such as ultra-compact resonators and slow-wave structures.\",\n",
       " 'This paper describes the construction of a cryostat and an optical system with a free-space coupling efficiency of 56.5% +/- 3.4% to a superconducting nanowire single-photon detector (SNSPD) for infrared quantum communication and spectrum analysis. A 1K pot decreases the base temperature to T = 1.7 K from the 2.9 K reached by the cold head cooled by a pulse-tube cryocooler. The minimum spot size coupled to the detector chip was 6.6 +/- 0.11 Î¼m starting from a fiber source at wavelength, Î» = 1.55 Î¼m. We demonstrated efficient photon counting on a detector with an 8 x 7.3 Î¼m^2 area. We measured a dark count rate of 95 +/- 3.35 kcps and a system detection efficiency of 1.64% +/- 0.13%. We explain the key steps that are required to further improve the coupling efficiency.',\n",
       " 'One of the astounding consequences of quantum mechanics is that it allows the detection of a target using an incident probe, with only a low probability of interaction of the probe and the target. This \\'quantum weirdness\\' could be applied in the field of electron microscopy to generate images of beam-sensitive specimens with substantially reduced damage to the specimen. A reduction of beam-induced damage to specimens is especially of great importance if it can enable imaging of biological specimens with atomic resolution. Following a recent suggestion that interaction-free measurements are possible with electrons, we now analyze the difficulties of actually building an atomic resolution interaction-free electron microscope, or \"quantum electron microscope\". A quantum electron microscope would require a number of unique components not found in conventional transmission electron microscopes. These components include a coherent electron-beam splitter or two-state-coupler, and a resonator structure to allow each electron to interrogate the specimen multiple times, thus supporting high success probabilities for interaction-free detection of the specimen. Different system designs are presented here, which are based on four different choices of two-state-couplers: a thin crystal, a grating mirror, a standing light wave and an electro-dynamical pseudopotential. Challenges for the detailed electron optical design are identified as future directions for development. While it is concluded that it should be possible to build an atomic resolution quantum electron microscope, we have also identified a number of hurdles to the development of such a microscope and further theoretical investigations that will be required to enable a complete interpretation of the images produced by such a microscope.',\n",
       " 'Methods for patterning biomolecules on a substrate at the single molecule level have been studied as a route to sensors with single-molecular sensitivity or as a way to probe biological phenomena at the single-molecule level. However, the arrangement and orientation of single biomolecules on substrates has been less investigated. Here, we examined the arrangement and orientation of two rod-like coiled-coil proteins, cortexillin and tropomyosin, around patterned gold nanostructures. The high aspect ratio of the coiled coils made it possible to study their orientations and to pursue a strategy of protein orientation via two-point attachment. The proteins were anchored to the surfaces using thiol groups, and the number of cysteine residues in tropomyosin was varied to test how this variation affected the structure and arrangement of the surface-attached proteins. Molecular dynamics studies were used to interpret the observed positional distributions. Based on initial studies of protein attachment to gold post structures, two 31-nm-long tropomyosin molecules were aligned between the two sidewalls of a trench with a width of 68 nm. Because the approach presented in this study uses one of twenty natural amino acids, this method provides a convenient way to pattern biomolecules on substrates using standard chemistry.',\n",
       " 'We present an optical setup that can be used to characterize the thicknesses of thin NbN films to screen samples for fabrication and to better model the performance of the resulting superconducting nanowire single photon detectors. The infrared transmissometer reported here is easy to use, gives results within minutes and is non-destructive. Thus, the thickness measurement can be easily integrated into the workflow of deposition and characterization. Comparison to a similar visible-wavelength transmissometer is provided.',\n",
       " 'Superconducting nanowire avalanche single-photon detectors (SNAPs) with n parallel nanowires are advantageous over single-nanowire detectors because their output signal amplitude scales linearly with n. However, the SNAP architecture has not been viably demonstrated for n > 4. To increase n for larger signal amplification, we designed a multi-stage, successive-avalanche architecture which used nanowires, connected via choke inductors in a binary-tree layout. We demonstrated an avalanche detector with n = 8 parallel nanowires and achieved eight-fold signal amplification, with a timing jitter of 54 ps.',\n",
       " 'Thin superconducting films form a unique platform for geometrically-confined, strongly-interacting electrons. They allow an inherent competition between disorder and superconductivity, which in turn enables the intriguing superconducting-to-insulator transition and believed to facilitate the comprehension of high-Tc superconductivity. Furthermore, understanding thin film superconductivity is technologically essential e.g. for photo-detectors, and quantum-computers. Consequently, the absence of an established universal relationships between critical temperature ($T_c$), film thickness ($d$) and sheet resistance ($R_s$) hinders both our understanding of the onset of the superconductivity and the development of miniaturised superconducting devices. We report that in thin films, superconductivity scales as $d^.$$T_c(R_s)$. We demonstrated this scaling by analysing the data published over the past 46 years for different materials (and facilitated this database for further analysis). Moreover, we experimentally confirmed the discovered scaling for NbN films, quantified it with a power law, explored its possible origin and demonstrated its usefulness for superconducting film-based devices.',\n",
       " 'Photonic integrated circuits (PICs) have emerged as a scalable platform for complex quantum technologies using photonic and atomic systems. A central goal has been to integrate photon-resolving detectors to reduce optical losses, latency, and wiring complexity associated with off-chip detectors. Superconducting nanowire single-photon detectors (SNSPDs) are particularly attractive because of high detection efficiency, sub-50-ps timing jitter, nanosecond-scale reset time, and sensitivity from the visible to the mid-infrared spectrum. However, while single SNSPDs have been incorporated into individual waveguides, the system efficiency of multiple SNSPDs in one photonic circuit has been limited below 0.2% due to low device yield. Here we introduce a micrometer-scale flip-chip process that enables scalable integration of SNSPDs on a range of PICs. Ten low-jitter detectors were integrated on one PIC with 100% device yield. With an average system efficiency beyond 10% for multiple SNSPDs on one PIC, we demonstrate high-fidelity on-chip photon correlation measurements of non-classical light.',\n",
       " 'In existing superconducting electronic systems, Josephson junctions play a central role in processing and transmitting small-amplitude electrical signals. However, Josephson-junction-based devices have a number of limitations including: (1) sensitivity to magnetic fields, (2) limited gain, (3) inability to drive large impedances, and (4) difficulty in controlling the junction critical current (which depends sensitively on sub-Angstrom-scale thickness variation of the tunneling barrier). Here we present a nanowire-based superconducting electronic device, which we call the nanocryotron (nTron), that does not rely on Josephson junctions and can be patterned from a single thin film of superconducting material with conventional electron-beam lithography. The nTron is a 3-terminal, T-shaped planar device with a gain of ~20 that is capable of driving impedances of more than 100 kÎ©, and operates in typical ambient magnetic fields at temperatures of 4.2K. The device uses a localized, Joule-heated hotspot formed in the gate to modulate current flow in a perpendicular superconducting channel. We have characterized the nTron, matched it to a theoretical framework, and applied it both as a digital logic element in a half-adder circuit, and as a digital amplifier for superconducting nanowire single-photon detectors pulses. The nTron has immediate applications in classical and quantum communications, photon sensing and astronomy, and its performance characteristics make it compatible with existing superconducting technologies. Furthermore, because the hotspot effect occurs in all known superconductors, we expect the design to be extensible to other materials, providing a path to digital logic, switching, and amplification in high-temperature superconductors.',\n",
       " 'The optimal orientations are determined for polarized substrate side illumination of three superconducting nanowire single-photon detector (SNSPD) designs: (1) periodic niobium-nitride (NbN) stripes standing in air with dimensions according to conventional SNSPDs, (2) same NbN patterns below ~quarter-wavelength hydrogensilsesquioxane-filled nano-cavity, (3) analogous NbN patterns in HSQ nano-cavity closed by a thin gold reflector. Numerical computation results have shown that the optical response and near-field distribution vary significantly with polar-angle, fi, and these variations are analogous across all azimuthal-angles, gamma, but are fundamentally different in various device designs. Larger absorptance is available due to p-polarized illumination of NbN patterns in P-structure configuration, while s-polarized illumination results in higher absorptance in S-structure arrangement. As a result of p-polarized illumination a global maximum appears on absorptance of bare NbN pattern at polar angle corresponding to NbN-related ATIR; integration with HSQ nano-cavity results in a global absorptance maximum at polar angle corresponding to TIR at sapphire-air interface; while the highest absorptance is observable at perpendicular incidence on P-structures aligned below gold reflector covered HSQ nano-cavity. S-polarized light illumination results in a global absorptance maximum at TIR on bare NbN patterns; the highest absorptance is available below HSQ nano-cavity at polar angle corresponding to ATIR phenomenon; while the benefit of gold reflector is large and polar angle independent absorptance.',\n",
       " 'In this paper we calculate the critical currents in thin superconducting strips with sharp right-angle turns, 180-degree turnarounds, and more complicated geometries, where all the line widths are much smaller than the Pearl length $Î›= 2 Î»^2/d$. We define the critical current as the current that reduces the Gibbs free-energy barrier to zero. We show that current crowding, which occurs whenever the current rounds a sharp turn, tends to reduce the critical current, but we also show that when the radius of curvature is less than the coherence length this effect is partially compensated by a radius-of-curvature effect. We propose several patterns with rounded corners to avoid critical-current reduction due to current crowding. These results are relevant to superconducting nanowire single-photon detectors, where they suggest a means of improving the bias conditions and reducing dark counts. These results also have relevance to normal-metal nanocircuits, as these patterns can reduce the electrical resistance, electromigration, and hot spots caused by nonuniform heating.',\n",
       " 'A novel finite-element method for calculating the illumination-dependence of absorption in three-dimensional nanostructures is presented based on the RF module of the COMSOL software package. This method is capable of numerically determining the optical response and near-field distribution of sub-wavelength periodic structures as a function of illumination orientations specified by polar angle, fi, and azimuthal angle, gamma. The method was applied to determine the illumination-angle-dependent absorptance in cavity-based superconducting-nanowire single-photon detector (SNSPD) designs. Niobium-nitride stripes based on dimensions of conventional SNSPDs and integrated with ~ quarter-wavelength hydrogensilsesquioxane-filled nano-optical cavities and covered by a thin gold film acting as a reflector were illuminated from below by p-polarized light in this study. The numerical results were compared to results from complementary transfer-matrix-method calculations on composite layers made of analogous film-stacks. This comparison helped to uncover the optical phenomena contributing to the appearance of extrema in the optical response. This paper presents an approach to optimizing the absorptance of different sensing and detecting devices via simultaneous numerical optimization of the polar and azimuthal illumination angles.',\n",
       " 'We developed an electro thermal model of NbN superconducting nanowire avalanche photodetectors (SNAPs) on sapphire substrates. SNAPs are single photon detectors consisting of the parallel connection of N superconducting nanowires. We extrapolated the physical constants of the model from experimental data and we simulated the time evolution of the device resistance, temperature and current by solving two coupled electrical and thermal differential equations describing the nanowires. The predictions of the model were in good quantitative agreement with the experimental results.',\n",
       " 'We fabricate superconducting ion traps with niobium and niobium nitride and trap single 88Sr ions at cryogenic temperatures. The superconducting transition is verified and characterized by measuring the resistance and critical current using a 4-wire measurement on the trap structure, and observing change in the rf reflection. The lowest observed heating rate is 2.1(3) quanta/sec at 800 kHz at 6 K and shows no significant change across the superconducting transition, suggesting that anomalous heating is primarily caused by noise sources on the surface. This demonstration of superconducting ion traps opens up possibilities for integrating trapped ions and molecular ions with superconducting devices.',\n",
       " '  Transitions in an artificial atom, driven non-adiabatically through an energy-level avoided crossing, can be controlled by carefully engineering the driving protocol. We have driven a superconducting persistent-current qubit with a large-amplitude, radio-frequency field. By applying a bi-harmonic waveform generated by a digital source, we demonstrate a mapping between the amplitude and phase of the harmonics produced at the source and those received by the device. This allows us to image the actual waveform at the device. This information is used to engineer a desired time dependence, as confirmed by detailed comparison with simulation.',\n",
       " '  We investigate the role of electrothermal feedback in the operation of superconducting nanowire single-photon detectors (SNSPDs). It is found that the desired mode of operation for SNSPDs is only achieved if this feedback is unstable, which happens naturally through the slow electrical response associated with their relatively large kinetic inductance. If this response is sped up in an effort to increase the device count rate, the electrothermal feedback becomes stable and results in an effect known as latching, where the device is locked in a resistive state and can no longer detect photons. We present a set of experiments which elucidate this effect, and a simple model which quantitatively explains the results.',\n",
       " '  We measured the optical absorptance of superconducting nanowire single photon detectors. We found that 200-nm-pitch, 50%-fill-factor devices had an average absorptance of 21% for normally-incident front-illumination of 1.55-um-wavelength light polarized parallel to the nanowires, and only 10% for perpendicularly-polarized light. We also measured devices with lower fill-factors and narrower wires that were five times more sensitive to parallel-polarized photons than perpendicular-polarized photons. We developed a numerical model that predicts the absorptance of our structures. We also used our measurements, coupled with measurements of device detection efficiencies, to determine the probability of photon detection after an absorption event. We found that, remarkably, absorbed parallel-polarized photons were more likely to result in detection events than perpendicular-polarized photons, and we present a hypothesis that qualitatively explains this result. Finally, we also determined the enhancement of device detection efficiency and absorptance due to the inclusion of an integrated optical cavity over a range of wavelengths (700-1700 nm) on a number of devices, and found good agreement with our numerical model.',\n",
       " '  A photon-number-resolving detector based on a four-element superconducting nanowire single photon detector is demonstrated to have sub-30-ps resolution in measuring the arrival time of individual photons. This detector can be used to characterize the photon statistics of non-pulsed light sources and to mitigate dead-time effects in high-speed photon counting applications. Furthermore, a 25% system detection efficiency at 1550 nm was demonstrated, making the detector useful for both low-flux source characterization and high-speed photon-counting and quantum communication applications. The design, fabrication and testing of this detector are described, and a comparison between the measured and theoretical performance is presented.',\n",
       " \"  The energy-level structure of a quantum system plays a fundamental role in determining its behavior and manifests itself in a discrete absorption and emission spectrum. Conventionally, spectra are probed via frequency spectroscopy whereby the frequency Î½of a harmonic driving field is varied to fulfill the conditions Î”E = h Î½, where the driving field is resonant with the level separation Î”E (h is Planck's constant). Although this technique has been successfully employed in a variety of physical systems, including natural and artificial atoms and molecules, its application is not universally straightforward, and becomes extremely challenging for frequencies in the range of 10's and 100's of gigahertz. Here we demonstrate an alternative approach, whereby a harmonic driving field sweeps the atom through its energy-level avoided crossings at a fixed frequency, surmounting many of the limitations of the conventional approach. Spectroscopic information is obtained from the amplitude dependence of the system response. The resulting ``spectroscopy diamonds'' contain interference patterns and population inversion that serve as a fingerprint of the atom's spectrum. By analyzing these features, we determine the energy spectrum of a manifold of states with energies from 0.01 to 120 GHz \\\\times h in a superconducting artificial atom, using a driving frequency near 0.1 GHz. This approach provides a means to manipulate and characterize systems over a broad bandwidth, using only a single driving frequency that may be orders of magnitude smaller than the energy scales being probed.\",\n",
       " '  Novel optical phenomena, including electromagnetically induced transparency, slow light, superluminal light propagation, have recently been demonstrated in diverse physical implementations. These phenomena are challenging to realize in practical systems because they require quantum coherence as well as careful preparation and control of prescribed quantum states. Here we present a unified approach to engineering optical materials that exhibit these phenomena by using mixtures of active and passive optical materials at frequencies near their resonances. Our approach does not depend on quantum coherence and can realize large and small (much less than 1) indices of refraction and negative permittivity ($Îµ<0$), normal and anomalous dispersion, all while maintaining transparency.',\n",
       " '  We investigate the source of large variations in the observed detection effiiencies of superconducting nanowire single-photon detectors between many nominally identical devices. Through both electrical and optical measurements, we infer that these variations arise from \"constrictions:\" highly localized regions of the nanowires where the effective cross-sectional area for superconducting current is reduced. These constrictions limit the DC bias current density to well below its critical value over the remainder of the wire, and thus prevent the detection efficiency from reaching the high values that occur in these devices only when they are biased near the critical current density.',\n",
       " '  A nonlinear resonant circuit comprising a SQUID magnetometer and a parallel capacitor is studied as a readout scheme for a persistent-current (PC) qubit. The flux state of the qubit is detected as a change in the Josephson inductance of the SQUID magnetometer, which in turn mediates a shift in the resonance frequency of the readout circuit. The nonlinearity and resulting hysteresis in the resonant behavior are characterized as a function of the power of both the input drive and the associated resonance peak response. Numerical simulations based on a phenomenological circuit model are presented which display the features of the observed nonlinearity.',\n",
       " '  We demonstrate Mach-Zehnder-type interferometry in a superconducting flux qubit. The qubit is a tunable artificial atom, whose ground and excited states exhibit an avoided crossing. Strongly driving the qubit with harmonic excitation sweeps it through the avoided crossing two times per period. As the induced Landau-Zener transitions act as coherent beamsplitters, the accumulated phase between transitions, which varies with microwave amplitude, results in quantum interference fringes for n=1...20 photon transitions. The generalization of optical Mach-Zehnder interferometry, performed in qubit phase space, provides an alternative means to manipulate and characterize the qubit in the strongly-driven regime.',\n",
       " '  We investigate the recovery of superconducting NbN-nanowire photon counters after detection of an optical pulse at a wavelength of 1550 nm, and present a model that quantitatively accounts for our observations. The reset time is found to be limited by the large kinetic inductance of these nanowires, which forces a tradeoff between counting rate and either detection efficiency or active area. Devices of usable size and high detection efficiency are found to have reset times orders of magnitude longer than their intrinsic photoresponse time.',\n",
       " '  Quantum optical techniques may yield immersion fluids with high indices of refraction without absorption. We describe one such technique in which a probe field experiences a large index of refraction with amplification rather than absorption, and examine its practicality for an immersion lithography application. Enhanced index can be observed in a three-level system with a tunable, near-resonant, coherent probe and incoherent pump field that inverts population of the probe transition. This observation contradicts the common belief that large indices of refraction are impossible without absorption, however it is well in accord with existing electromagnetic theory and practice. Calculations show that a refractive index >> 2 is possible with practical experimental parameters. A scheme with an incoherent mixture of pumped and unpumped atoms is also examined, and is seen to have a lower refractive index (~2) accompanied by neither gain nor loss.',\n",
       " '  We have implemented a resonant circuit that uses a SQUID as a flux-sensitive Josephson inductor for qubit readout. In contrast to the conventional switching current measurement that generates undesired quasi-particles when the SQUID switches to the voltage state, our approach keeps the readout SQUID biased along the supercurrent branch during the measurement. By incorporating the SQUID inductor in a high-Q resonant circuit, we can distinguish the two flux states of a niobium persistent-current (PC) qubit by observing a shift in the resonant frequency of both the magnitude and the phase spectra. The readout circuit was also characterized in the nonlinear regime to investigate its potential use as a nonlinear amplifier.',\n",
       " \"  We measured the intrawell energy relaxation time Ï„_{d} between macroscopic quantum levels in the double well potential of a Nb persistent-current qubit. Interwell population transitions were generated by irradiating the qubit with microwaves. Zero population in the initial well was then observed due to a multi-level decay process in which the initial population relaxed to the lower energy levels during transitions. The qubit's decoherence time, determined from Ï„_{d}, is longer than 20 microseconds, holding the promise of building a quantum computer with Nb-based superconducting qubits.\",\n",
       " \"  A numerical method for solving Schrodinger's equation based upon a Baker-Campbell-Hausdorff (BCH) expansion of the time evolution operator is presented herein. The technique manifestly preserves wavefunction norm, and it can be applied to problems in any number of spatial dimensions. We also identify a particular dimensionless ratio of potential to kinetic energies as a key coupling constant. This coupling establishes characteristic length and time scales for a large class of low energy quantum states, and it guides the choice of step sizes in numerical work. Using the BCH method in conjunction with an imaginary time rotation, we compute low energy eigenstates for several quantum systems coupled to non-trivial background potentials. The approach is subsequently applied to the study of 1D propagating wave packets and 2D bound state time development. Failures of classical expectations uncovered by simulations of these simple systems help develop quantum intuition.\\n  Finally, we investigate the response of a Superconducting Quantum Interference Device (SQUID) to a time dependent potential. We discuss how to engineer the potential's energy and time scales so that the SQUID acts as a quantum NOT gate. The notional simulation we present for this gate provides useful insight into the design of one candidate building block for a quantum computer.\",\n",
       " '  The Semantic Web drives towards the use of the Web for interacting with logically interconnected data. Through knowledge models such as Resource Description Framework (RDF), the Semantic Web provides a unifying representation of richly structured data. Adding logic to the Web implies the use of rules to make inferences, choose courses of action, and answer questions. This logic must be powerful enough to describe complex properties of objects but not so powerful that agents can be tricked by being asked to consider a paradox. The Web has several characteristics that can lead to problems when existing logics are used, in particular, the inconsistencies that inevitably arise due to the openness of the Web, where anyone can assert anything. N3Logic is a logic that allows rules to be expressed in a Web environment. It extends RDF with syntax for nested graphs and quantified variables and with predicates for implication and accessing resources on the Web, and functions including cryptographic, string, math. The main goal of N3Logic is to be a minimal extension to the RDF data model such that the same language can be used for logic and data. In this paper, we describe N3Logic and illustrate through examples why it is an appropriate logic for the Web.',\n",
       " 'We consider infinite horizon dynamic programming problems, where the control at each stage consists of several distinct decisions, each one made by one of several agents. In an earlier work we introduced a policy iteration algorithm, where the policy improvement is done one-agent-at-a-time in a given order, with knowledge of the choices of the preceding agents in the order. As a result, the amount of computation for each policy improvement grows linearly with the number of agents, as opposed to exponentially for the standard all-agents-at-once method. For the case of a finite-state discounted problem, we showed convergence to an agent-by-agent optimal policy. In this paper, this result is extended to value iteration and optimistic versions of policy iteration, as well as to more general DP problems where the Bellman operator is a contraction mapping, such as stochastic shortest path problems with all policies being proper.',\n",
       " \"We consider an extension of the rollout algorithm that applies to constrained deterministic dynamic programming, including challenging combinatorial optimization problems. The algorithm relies on a suboptimal policy, called base heuristic. Under suitable assumptions, we show that if the base heuristic produces a feasible solution, the rollout algorithm has a cost improvement property: it produces a feasible solution, whose cost is no worse than the base heuristic's cost.\\n  We then focus on multiagent problems, where the control at each stage consists of multiple components (one per agent), which are coupled either through the cost function or the constraints or both. We show that the cost improvement property is maintained with an alternative implementation that has greatly reduced computational requirements, and makes possible the use of rollout in problems with many agents. We demonstrate this alternative algorithm by applying it to layered graph problems that involve both a spatial and a temporal structure. We consider in some detail a prominent example of such problems: multidimensional assignment, where we use the auction algorithm for 2-dimensional assignment as a base heuristic. This auction algorithm is particularly well-suited for our context, because through the use of prices, it can advantageously use the solution of an assignment problem as a starting point for solving other related assignment problems, and this can greatly speed up the execution of the rollout algorithm.\",\n",
       " 'In this paper we consider infinite horizon discounted dynamic programming problems with finite state and control spaces, and partial state observations. We discuss an algorithm that uses multistep lookahead, truncated rollout with a known base policy, and a terminal cost function approximation. This algorithm is also used for policy improvement in an approximate policy iteration scheme, where successive policies are approximated by using a neural network classifier. A novel feature of our approach is that it is well suited for distributed computation through an extended belief space formulation and the use of a partitioned architecture, which is trained with multiple neural networks. We apply our methods in simulation to a class of sequential repair problems where a robot inspects and repairs a pipeline with potentially several rupture sites under partial information about the state of the pipeline.',\n",
       " 'We propose a new aggregation framework for approximate dynamic programming, which provides a connection with rollout algorithms, approximate policy iteration, and other single and multistep lookahead methods. The central novel characteristic is the use of a bias function $V$ of the state, which biases the values of the aggregate cost function towards their correct levels. The classical aggregation framework is obtained when $V\\\\equiv0$, but our scheme works best when $V$ is a known reasonably good approximation to the optimal cost function $J^*$.\\n  When $V$ is equal to the cost function $J_Î¼$ of some known policy $Î¼$ and there is only one aggregate state, our scheme is equivalent to the rollout algorithm based on $Î¼$ (i.e., the result of a single policy improvement starting with the policy $Î¼$). When $V=J_Î¼$ and there are multiple aggregate states, our aggregation approach can be used as a more powerful form of improvement of $Î¼$. Thus, when combined with an approximate policy evaluation scheme, our approach can form the basis for a new and enhanced form of approximate policy iteration.\\n  When $V$ is a generic bias function, our scheme is equivalent to approximation in value space with lookahead function equal to $V$ plus a local correction within each aggregate state. The local correction levels are obtained by solving a low-dimensional aggregate DP problem, yielding an arbitrarily close approximation to $J^*$, when the number of aggregate states is sufficiently large. Except for the bias function, the aggregate DP problem is similar to the one of the classical aggregation framework, and its algorithmic solution by simulation or other methods is nearly identical to one for classical aggregation, assuming values of $V$ are available when needed.',\n",
       " \"We consider finite and infinite horizon dynamic programming problems, where the control at each stage consists of several distinct decisions, each one made by one of several agents. We introduce an approach, whereby at every stage, each agent's decision is made by executing a local rollout algorithm that uses a base policy, together with some coordinating information from the other agents. The amount of local computation required at every stage by each agent is independent of the number of agents, while the amount of total computation (over all agents) grows linearly with the number of agents. By contrast, with the standard rollout algorithm, the amount of total computation grows exponentially with the number of agents. Despite the drastic reduction in required computation, we show that our algorithm has the fundamental cost improvement property of rollout: an improved performance relative to the base policy. We also discuss possibilities to improve further the method's computational efficiency through limited agent coordination and parallelization of the agents' computations. Finally, we explore related approximate policy iteration algorithms for infinite horizon problems, and we prove that the cost improvement property steers the algorithm towards convergence to an agent-by-agent optimal policy.\",\n",
       " 'In this paper we discuss policy iteration methods for approximate solution of a finite-state discounted Markov decision problem, with a focus on feature-based aggregation methods and their connection with deep reinforcement learning schemes. We introduce features of the states of the original problem, and we formulate a smaller \"aggregate\" Markov decision problem, whose states relate to the features. We discuss properties and possible implementations of this type of aggregation, including a new approach to approximate policy iteration. In this approach the policy improvement operation combines feature-based aggregation with feature construction using deep neural networks or other calculations. We argue that the cost function of a policy may be approximated much more accurately by the nonlinear function of the features provided by aggregation, than by the linear function of the features provided by neural network-based reinforcement learning, thereby potentially leading to more effective policy improvement.',\n",
       " \"We consider discrete-time infinite horizon deterministic optimal control problems with nonnegative cost per stage, and a destination that is cost-free and absorbing. The classical linear-quadratic regulator problem is a special case. Our assumptions are very general, and allow the possibility that the optimal policy may not be stabilizing the system, e.g., may not reach the destination either asymptotically or in a finite number of steps. We introduce a new unifying notion of stable feedback policy, based on perturbation of the cost per stage, which in addition to implying convergence of the generated states to the destination, quantifies the speed of convergence. We consider the properties of two distinct cost functions: $\\\\jstar$, the overall optimal, and $\\\\hat J$, the restricted optimal over just the stable policies. Different classes of stable policies (with different speeds of convergence) may yield different values of $\\\\hat J$. We show that for any class of stable policies, $\\\\hat J$ is a solution of Bellman's equation, and we characterize the smallest and the largest solutions: they are $\\\\jstar$, and $J^+$, the restricted optimal cost function over the class of (finitely) terminating policies. We also characterize the regions of convergence of various modified versions of value and policy iteration algorithms, as substitutes for the standard algorithms, which may not work in general.\",\n",
       " \"We consider stochastic shortest path problems with infinite state and control spaces, a nonnegative cost per stage, and a termination state. We extend the notion of a proper policy, a policy that terminates within a finite expected number of steps, from the context of finite state space to the context of infinite state space. We consider the optimal cost function $J^*$, and the optimal cost function $\\\\hat J$ over just the proper policies. We show that $J^*$ and $\\\\hat J$ are the smallest and largest solutions of Bellman's equation, respectively, within a suitable class of Lyapounov-like functions. If the cost per stage is bounded, these functions are those that are bounded over the effective domain of $\\\\hat J$. The standard value iteration algorithm may be attracted to either $J^*$ or $\\\\hat J$, depending on the initial condition.\",\n",
       " 'We consider large linear and nonlinear fixed point problems, and solution with proximal algorithms. We show that there is a close connection between two seemingly different types of methods from distinct fields: 1) Proximal iterations for linear systems of equations, which are prominent in numerical analysis and convex optimization, and 2) Temporal difference (TD) type methods, such as TD(lambda), LSTD(lambda), and LSPE(lambda), which are central in simulation-based approximate dynamic programming/reinforcement learning (DP/RL), and its recent prominent successes in large-scale game contexts, among others.\\n  One benefit of this connection is a new and simple way to accelerate the standard proximal algorithm by extrapolation towards the TD iteration, which generically has a faster convergence rate. Another benefit is the potential integration into the proximal algorithmic context of several new ideas that have emerged in the DP/RL context. We discuss some of the possibilities, and in particular, algorithms that project each proximal iterate onto the subspace spanned by a small number of basis functions, using low-dimensional calculations and simulation. A third benefit is that insights and analysis from proximal algorithms can be brought to bear on the enhancement of TD methods.\\n  The linear fixed point methodology can be extended to nonlinear fixed point problems involving a contraction, thus providing guaranteed and potentially substantial acceleration of the proximal and forward backward splitting algorithms at no extra cost. Moreover, the connection of proximal and TD methods can be extended to nonlinear (nondifferentiable) fixed point problems through new proximal-like algorithms that involve successive linearization, similar to policy iteration in DP.',\n",
       " 'We consider challenging dynamic programming models where the associated Bellman equation, and the value and policy iteration algorithms commonly exhibit complex and even pathological behavior. Our analysis is based on the new notion of regular policies. These are policies that are well-behaved with respect to value and policy iteration, and are patterned after proper policies, which are central in the theory of stochastic shortest path problems. We show that the optimal cost function over regular policies may have favorable value and policy iteration properties, which the optimal cost function over all policies need not have. We accordingly develop a unifying methodology to address long standing analytical and algorithmic issues in broad classes of undiscounted models, including stochastic and minimax shortest path problems, as well as positive cost, negative cost, risk-sensitive, and multiplicative cost problems.',\n",
       " 'In this paper we consider shortest path problems in a directed graph where the transitions between nodes are subject to uncertainty. We use a minimax formulation, where the objective is to guarantee that a special destination state is reached with a minimum cost path under the worst possible instance of the uncertainty. Problems of this type arise, among others, in planning and pursuit-evasion contexts, and in model predictive control. Our analysis makes use of the recently developed theory of abstract semicontractive dynamic programming models. We investigate questions of existence and uniqueness of solution of the optimality equation, existence of optimal paths, and the validity of various algorithms patterned after the classical methods of value and policy iteration, as well as a Dijkstra-like algorithm for problems with nonnegative arc lengths.',\n",
       " \"In this paper we consider a broad class of infinite horizon discrete-time optimal control models that involve a nonnegative cost function and an affine mapping in their dynamic programming equation. They include as special cases classical models such as stochastic undiscounted nonnegative cost problems, stochastic multiplicative cost problems, and risk-sensitive problems with exponential cost. We focus on the case where the state space is finite and the control space has some compactness properties. We assume that the affine mapping has a semicontractive character, whereby for some policies it is a contraction, while for others it is not. In one line of analysis, we impose assumptions that guarantee that the latter policies cannot be optimal. Under these assumptions, we prove strong results that resemble those for discounted Markovian decision problems, such as the uniqueness of solution of Bellman's equation, and the validity of forms of value and policy iteration. In the absence of these assumptions, the results are weaker and unusual in character: the optimal cost function need not be a solution of Bellman's equation, and an optimal policy may not be found by value or policy iteration. Instead the optimal cost function over just the contractive policies solves Bellman's equation, and can be computed by a variety of algorithms.\",\n",
       " 'We consider minimization of the sum of a large number of convex functions, and we propose an incremental aggregated version of the proximal algorithm, which bears similarity to the incremental aggregated gradient and subgradient methods that have received a lot of recent attention. Under cost function differentiability and strong convexity assumptions, we show linear convergence for a sufficiently small constant stepsize. This result also applies to distributed asynchronous variants of the method, involving bounded interprocessor communication delays.\\n  We then consider dual versions of incremental proximal algorithms, which are incremental augmented Lagrangian methods for separable equality-constrained optimization problems. Contrary to the standard augmented Lagrangian method, these methods admit decomposition in the minimization of the augmented Lagrangian, and update the multipliers far more frequently. Our incremental aggregated augmented Lagrangian methods bear similarity to several known decomposition algorithms, including the alternating direction method of multipliers (ADMM) and more recent variations. We compare these methods in terms of their properties, and highlight their potential advantages and limitations.\\n  We also address the solution of separable inequality-constrained optimization problems through the use of nonquadratic augmented Lagrangiias such as the exponential, and we dually consider a corresponding incremental aggregated version of the proximal algorithm that uses nonquadratic regularization, such as an entropy function. We finally propose a closely related linearly convergent method for minimization of large differentiable sums subject to an orthant constraint, which may be viewed as an incremental aggregated version of the mirror descent method.',\n",
       " 'We survey incremental methods for minimizing a sum $\\\\sum_{i=1}^mf_i(x)$ consisting of a large number of convex component functions $f_i$. Our methods consist of iterations applied to single components, and have proved very effective in practice. We introduce a unified algorithmic framework for a variety of such methods, some involving gradient and subgradient iterations, which are known, and some involving combinations of subgradient and proximal methods, which are new and offer greater flexibility in exploiting the special structure of $f_i$. We provide an analysis of the convergence and rate of convergence properties of these methods, including the advantages offered by randomization in the selection of components. We also survey applications in inference/machine learning, signal processing, and large-scale and distributed optimization.',\n",
       " 'In this paper we discuss $Å‚$-policy iteration, a method for exact and approximate dynamic programming. It is intermediate between the classical value iteration (VI) and policy iteration (PI) methods, and it is closely related to optimistic (also known as modified) PI, whereby each policy evaluation is done approximately, using a finite number of VI. We review the theory of the method and associated questions of bias and exploration arising in simulation-based cost function approximation. We then discuss various implementations, which offer advantages over well-established PI methods that use LSPE($Å‚$), LSTD($Å‚$), or TD($Å‚$) for policy evaluation with cost function approximation. One of these implementations is based on a new simulation scheme, called geometric sampling, which uses multiple short trajectories rather than a single infinitely long trajectory.',\n",
       " \"In this paper, we consider discrete-time infinite horizon problems of optimal control to a terminal set of states. These are the problems that are often taken as the starting point for adaptive dynamic programming. Under very general assumptions, we establish the uniqueness of solution of Bellman's equation, and we provide convergence results for value and policy iteration.\",\n",
       " 'We consider Newton methods for common types of single commodity and multi-commodity network flow problems. Despite the potentially very large dimension of the problem, they can be implemented using the conjugate gradient method and low-dimensional network operations, as shown nearly thirty years ago. We revisit these methods, compare them to more recent proposals, and describe how they can be implemented in a distributed computing system. We also discuss generalizations, including the treatment of arc gains, linear side constraints, and related special structures.',\n",
       " \"We consider stochastic control models with Borel spaces and universally measurable policies. For such models the standard policy iteration is known to have difficult measurability issues and cannot be carried out in general. We present a mixed value and policy iteration method that circumvents this difficulty. The method allows the use of stationary policies in computing the optimal cost function, in a manner that resembles policy iteration. It can also be used to address similar difficulties of policy iteration in the context of upper and lower semicontinuous models. We analyze the convergence of the method in infinite horizon total cost problems, for the discounted case where the one-stage costs are bounded, and for the undiscounted case where the one-stage costs are nonpositive or nonnegative.\\n  For undiscounted total cost problems with nonnegative one-stage costs, we also give a new convergence theorem for value iteration, which shows that value iteration converges whenever it is initialized with a function that is above the optimal cost function and yet bounded by a multiple of the optimal cost function. This condition resembles Whittle's bridging condition and is partly motivated by it. The theorem is also partly motivated by a result of Maitra and Sudderth, which showed that value iteration, when initialized with the constant function zero, could require a transfinite number of iterations to converge. We use the new convergence theorem for value iteration to establish the convergence of our mixed value and policy iteration method for the nonnegative cost case.\",\n",
       " 'In this paper, we propose a new lower approximation scheme for POMDP with discounted and average cost criterion. The approximating functions are determined by their values at a finite number of belief points, and can be computed efficiently using value iteration algorithms for finite-state MDP. While for discounted problems several lower approximation schemes have been proposed earlier, ours seems the first of its kind for average cost problems. We focus primarily on the average cost case, and we show that the corresponding approximation can be computed efficiently using multi-chain algorithms for finite-state MDP. We give a preliminary analysis showing that regardless of the existence of the optimal average cost J in the POMDP, the approximation obtained is a lower bound of the liminf optimal average cost function, and can also be used to calculate an upper bound on the limsup optimal average cost function, as well as bounds on the cost of executing the stationary policy associated with the approximation. Weshow the convergence of the cost approximation, when the optimal average cost is constant and the optimal differential cost is continuous.',\n",
       " 'Recent neural network architectures such as the basic recurrent neural network (RNN) and Gated Recurrent Unit (GRU) have gained prominence as end-to-end learning architectures for natural language processing tasks. But what is the computational power of such systems? We prove that finite precision RNNs with one hidden layer and ReLU activation and finite precision GRUs are exactly as computationally powerful as deterministic finite automata. Allowing arbitrary precision, we prove that RNNs with one hidden layer and ReLU activation are at least as computationally powerful as pushdown automata. If we also allow infinite precision, infinite edge weights, and nonlinear output activation functions, we prove that GRUs are at least as computationally powerful as pushdown automata. All results are shown constructively.',\n",
       " \"While long short-term memory (LSTM) neural net architectures are designed to capture sequence information, human language is generally composed of hierarchical structures. This raises the question as to whether LSTMs can learn hierarchical structures. We explore this question with a well-formed bracket prediction task using two types of brackets modeled by an LSTM. Demonstrating that such a system is learnable by an LSTM is the first step in demonstrating that the entire class of CFLs is also learnable. We observe that the model requires exponential memory in terms of the number of characters and embedded depth, where a sub-linear memory should suffice. Still, the model does more than memorize the training input. It learns how to distinguish between relevant and irrelevant information. On the other hand, we also observe that the model does not generalize well. We conclude that LSTMs do not learn the relevant underlying context-free rules, suggesting the good overall performance is attained rather by an efficient way of evaluating nuisance variables. LSTMs are a way to quickly reach good results for many natural language tasks, but to understand and generate natural language one has to investigate other concepts that can make more direct use of natural language's structural nature.\",\n",
       " 'We consider two different data sets of syntactic parameters and we discuss how to detect relations between parameters through a heat kernel method developed by Belkin-Niyogi, which produces low dimensional representations of the data, based on Laplace eigenfunctions, that preserve neighborhood information. We analyze the different connectivity and clustering structures that arise in the two datasets, and the regions of maximal variance in the two-parameter space of the Belkin-Niyogi construction, which identify preferable choices of independent variables. We compute clustering coefficients and their variance.',\n",
       " \"Using Phylogenetic Algebraic Geometry, we analyze computationally the phylogenetic tree of subfamilies of the Indo-European language family, using data of syntactic structures. The two main sources of syntactic data are the SSWL database and Longobardi's recent data of syntactic parameters. We compute phylogenetic invariants and likelihood functions for two sets of Germanic languages, a set of Romance languages, a set of Slavic languages and a set of early Indo-European languages, and we compare the results with what is known through historical linguistics.\",\n",
       " \"  In Phys. Rev. Letters (73:2, 5 Dec. 94), Mantegna et al. conclude on the basis of Zipf rank frequency data that noncoding DNA sequence regions are more like natural languages than coding regions. We argue on the contrary that an empirical fit to Zipf's ``law'' cannot be used as a criterion for similarity to natural languages. Although DNA is a presumably an ``organized system of signs'' in Mandelbrot's (1961) sense, an observation of statistical features of the sort presented in the Mantegna et al. paper does not shed light on the similarity between DNA's ``grammar'' and natural language grammars, just as the observation of exact Zipf-like behavior cannot distinguish between the underlying processes of tossing an $M$ sided die or a finite-state branching process.\",\n",
       " 'Global eradication of malaria depends on the development of drugs effective against the silent, yet obligate liver stage of the disease. The gold standard in drug development remains microscopic imaging of liver stage parasites in in vitro cell culture models. Image analysis presents a major bottleneck in this pipeline since the parasite has significant variability in size, shape, and density in these models. As with other highly variable datasets, traditional segmentation models have poor generalizability as they rely on hand-crafted features; thus, manual annotation of liver stage malaria images remains standard. To address this need, we develop a convolutional neural network architecture that utilizes spatial dropout sampling for parasite segmentation and epistemic uncertainty estimation in images of liver stage malaria. Our pipeline produces high-precision segmentations nearly identical to expert annotations, generalizes well on a diverse dataset of liver stage malaria parasites, and promotes independence between learned feature maps to model the uncertainty of generated predictions.',\n",
       " \"Modellers of large scale genome rearrangement events, in which segments of DNA are inverted, moved, swapped, or even inserted or deleted, have found a natural syntax in the language of permutations. Despite this, there has been a wide range of modelling choices, assumptions and interpretations that make navigating the literature a significant challenge. Indeed, even authors of papers that use permutations to model genome rearrangement can struggle to interpret each others' work, because of subtle differences in basic assumptions that are often deeply ingrained (and consequently sometimes not even mentioned). In this paper, we describe the different ways in which permutations have been used to model genomes and genome rearrangement events, presenting some features and limitations of each approach, and show how the various models are related. This paper will help researchers navigate the landscape of genome rearrangement models, and make it easier for authors to present clear and consistent models.\",\n",
       " 'Establishing a distance between genomes is a significant problem in computational genomics, because its solution can be used to establish evolutionary relationships including phylogeny.\\n  The \"double cut and join\" (DCJ) model of chromosomal rearrangement proposed by Yancopoulos et al. has received attention as it can model inversions, translocations, fusion and fission on a multichromosomal genome that may contain both linear and circular chromosomes. In this paper, we realize the DCJ operator as a group action on the space of multichromosomal genomes. We study this group action, deriving some properties of the group and finding group-theoretic analogues for the key results in the DCJ theory.',\n",
       " 'Deep Reinforcement Learning (DRL) is vulnerable to small adversarial perturbations on state observations. These perturbations do not alter the environment directly but can mislead the agent into making suboptimal decisions. We analyze the Markov Decision Process (MDP) under this threat model and utilize tools from the neural net-work verification literature to enable robust train-ing for DRL under observational perturbations. Our techniques are general and can be applied to both Deep Q Networks (DQN) and Deep Deterministic Policy Gradient (DDPG) algorithms for discrete and continuous action control problems. We demonstrate that our proposed training procedure significantly improves the robustness of DQN and DDPG agents under a suite of strong white-box attacks on observations, including a few novel attacks we specifically craft. Additionally, our training procedure can produce provable certificates for the robustness of a Deep RL agent.',\n",
       " 'We propose a recurrent neural network for a \"model-free\" simulation of a dynamical system with unknown parameters without prior knowledge. The deep learning model aims to jointly learn the nonlinear time marching operator and the effects of the unknown parameters from a time series dataset. We assume that the time series data set consists of an ensemble of trajectories for a range of the parameters. The learning task is formulated as a statistical inference problem by considering the unknown parameters as random variables. A variational inference method is employed to train a recurrent neural network jointly with a feedforward neural network for an approximately posterior distribution. The approximate posterior distribution makes an inference on a trajectory to identify the effects of the unknown parameters and a recurrent neural network makes a prediction by using the outcome of the inference. In the numerical experiments, it is shown that the proposed variational inference model makes a more accurate simulation compared to the standard recurrent neural networks. It is found that the proposed deep learning model is capable of correctly identifying the dimensions of the random parameters and learning a representation of complex time series data.',\n",
       " 'Training neural networks with verifiable robustness guarantees is challenging. Several existing approaches utilize linear relaxation based neural network output bounds under perturbation, but they can slow down training by a factor of hundreds depending on the underlying network architectures. Meanwhile, interval bound propagation (IBP) based training is efficient and significantly outperforms linear relaxation based methods on many tasks, yet it may suffer from stability issues since the bounds are much looser especially at the beginning of training. In this paper, we propose a new certified adversarial training method, CROWN-IBP, by combining the fast IBP bounds in a forward bounding pass and a tight linear relaxation based bound, CROWN, in a backward bounding pass. CROWN-IBP is computationally efficient and consistently outperforms IBP baselines on training verifiably robust neural networks. We conduct large scale experiments on MNIST and CIFAR datasets, and outperform all previous linear relaxation and bound propagation based certified defenses in $\\\\ell_\\\\infty$ robustness. Notably, we achieve 7.02% verified test error on MNIST at $Îµ=0.3$, and 66.94% on CIFAR-10 with $Îµ=8/255$. Code is available at https://github.com/deepmind/interval-bound-propagation (TensorFlow) and https://github.com/huanzhang12/CROWN-IBP (PyTorch).',\n",
       " 'We study the robustness verification problem for tree-based models, including decision trees, random forests (RFs) and gradient boosted decision trees (GBDTs). Formal robustness verification of decision tree ensembles involves finding the exact minimal adversarial perturbation or a guaranteed lower bound of it. Existing approaches find the minimal adversarial perturbation by a mixed integer linear programming (MILP) problem, which takes exponential time so is impractical for large ensembles. Although this verification problem is NP-complete in general, we give a more precise complexity characterization. We show that there is a simple linear time algorithm for verifying a single tree, and for tree ensembles, the verification problem can be cast as a max-clique problem on a multi-partite graph with bounded boxicity. For low dimensional problems when boxicity can be viewed as constant, this reformulation leads to a polynomial time algorithm. For general problems, by exploiting the boxicity of the graph, we develop an efficient multi-level verification algorithm that can give tight lower bounds on the robustness of decision tree ensembles, while allowing iterative improvement and any-time termination. OnRF/GBDT models trained on 10 datasets, our algorithm is hundreds of times faster than the previous approach that requires solving MILPs, and is able to give tight robustness verification bounds on large GBDTs with hundreds of deep trees.',\n",
       " 'Although adversarial examples and model robustness have been extensively studied in the context of linear models and neural networks, research on this issue in tree-based models and how to make tree-based models robust against adversarial examples is still limited. In this paper, we show that tree based models are also vulnerable to adversarial examples and develop a novel algorithm to learn robust trees. At its core, our method aims to optimize the performance under the worst-case perturbation of input features, which leads to a max-min saddle point problem. Incorporating this saddle point objective into the decision tree building procedure is non-trivial due to the discrete nature of trees --- a naive approach to finding the best split according to this saddle point objective will take exponential time. To make our approach practical and scalable, we propose efficient tree building algorithms by approximating the inner minimizer in this saddle point problem, and present efficient implementations for classical information gain based trees as well as state-of-the-art tree boosting models such as XGBoost. Experimental results on real world datasets demonstrate that the proposed algorithms can substantially improve the robustness of tree-based models against adversarial examples.',\n",
       " 'The adversarial training procedure proposed by Madry et al. (2018) is one of the most effective methods to defend against adversarial examples in deep neural networks (DNNs). In our paper, we shed some lights on the practicality and the hardness of adversarial training by showing that the effectiveness (robustness on test set) of adversarial training has a strong correlation with the distance between a test point and the manifold of training data embedded by the network. Test examples that are relatively far away from this manifold are more likely to be vulnerable to adversarial attacks. Consequentially, an adversarial training based defense is susceptible to a new class of attacks, the \"blind-spot attack\", where the input images reside in \"blind-spots\" (low density regions) of the empirical distribution of training data but is still on the ground-truth data manifold. For MNIST, we found that these blind-spots can be easily found by simply scaling and shifting image pixel values. Most importantly, for large datasets with high dimensional and complex data manifold (CIFAR, ImageNet, etc), the existence of blind-spots in adversarial training makes defending on any valid test examples difficult due to the curse of dimensionality and the scarcity of training data. Additionally, we find that blind-spots also exist on provable defenses including (Wong & Kolter, 2018) and (Sinha et al., 2018) because these trainable robustness certificates can only be practically optimized on a limited set of training data.',\n",
       " 'Verifying the robustness property of a general Rectified Linear Unit (ReLU) network is an NP-complete problem [Katz, Barrett, Dill, Julian and Kochenderfer CAV17]. Although finding the exact minimum adversarial distortion is hard, giving a certified lower bound of the minimum distortion is possible. Current available methods of computing such a bound are either time-consuming or delivering low quality bounds that are too loose to be useful. In this paper, we exploit the special structure of ReLU networks and provide two computationally efficient algorithms Fast-Lin and Fast-Lip that are able to certify non-trivial lower bounds of minimum distortions, by bounding the ReLU units with appropriate linear functions Fast-Lin, or by bounding the local Lipschitz constant Fast-Lip. Experiments show that (1) our proposed methods deliver bounds close to (the gap is 2-3X) exact minimum distortion found by Reluplex in small MNIST networks while our algorithms are more than 10,000 times faster; (2) our methods deliver similar quality of bounds (the gap is within 35% and usually around 10%; sometimes our bounds are even better) for larger networks compared to the methods based on solving linear programming problems but our algorithms are 33-14,000 times faster; (3) our method is capable of solving large MNIST and CIFAR networks up to 7 layers with more than 10,000 neurons within tens of seconds on a single CPU core.\\n  In addition, we show that, in fact, there is no polynomial time algorithm that can approximately find the minimum $\\\\ell_1$ adversarial distortion of a ReLU network with a $0.99\\\\ln n$ approximation ratio unless $\\\\mathsf{NP}$=$\\\\mathsf{P}$, where $n$ is the number of neurons in the network.',\n",
       " 'In this paper, we propose a novel method to estimate and characterize spatial variations on dies or wafers. This new technique exploits recent developments in matrix completion, enabling estimation of spatial variation across wafers or dies with a small number of randomly picked sampling points while still achieving fairly high accuracy. This new approach can be easily generalized, including for estimation of mixed spatial and structure or device type information.',\n",
       " 'We develop a corrective mechanism for neural network approximation: the total available non-linear units are divided into multiple groups and the first group approximates the function under consideration, the second group approximates the error in approximation produced by the first group and corrects it, the third group approximates the error produced by the first and second groups together and so on. This technique yields several new representation and learning results for neural networks. First, we show that two-layer neural networks in the random features regime (RF) can memorize arbitrary labels for arbitrary points under under Euclidean distance separation condition using $\\\\tilde{O}(n)$ ReLU or Step activation functions which is optimal in $n$ up to logarithmic factors. Next, we give a powerful representation result for two-layer neural networks with ReLU and smoothed ReLU units which can achieve a squared error of at most $Îµ$ with $O(C(a,d)Îµ^{-1/(a+1)})$ for $a \\\\in \\\\mathbb{N}\\\\cup\\\\{0\\\\}$ when the function is smooth enough (roughly when it has $Î˜(ad)$ bounded derivatives). In certain cases $d$ can be replaced with effective dimension $q \\\\ll d$. Previous results of this type implement Taylor series approximation using deep architectures. We also consider three-layer neural networks and show that the corrective mechanism yields faster representation rates for smooth radial functions. Lastly, we obtain the first $O(\\\\mathrm{subpoly}(1/Îµ))$ upper bound on the number of neurons required for a two layer network to learn low degree polynomials up to squared error $Îµ$ via gradient descent. Even though deep networks can express these polynomials with $O(\\\\mathrm{polylog}(1/Îµ))$ neurons, the best learning bounds on this problem require $\\\\mathrm{poly}(1/Îµ)$ neurons.',\n",
       " 'Random graphs with latent geometric structure are popular models of social and biological networks, with applications ranging from network user profiling to circuit design. These graphs are also of purely theoretical interest within computer science, probability and statistics. A fundamental initial question regarding these models is: when are these random graphs affected by their latent geometry and when are they indistinguishable from simpler models without latent structure, such as the ErdÅ‘s-RÃ©nyi graph $\\\\mathcal{G}(n, p)$? We address this question for two of the most well-studied models of random graphs with latent geometry -- the random intersection and random geometric graph.\\n  Our results are as follows: (1) we prove that the random intersection graph converges in total variation to $\\\\mathcal{G}(n, p)$ when $d = \\\\tildeÏ‰(n^3)$, and does not if $d = o(n^3)$, resolving an open problem in Fill et al. (2000), Rybarczyk (2011) and Kim et al. (2018); (2) we provide conditions under which the matrix of intersection sizes of random family of sets converges in total variation to a symmetric matrix with independent Poisson entries, yielding the first total variation convergence result for $Ï„$-random intersection graphs to $\\\\mathcal{G}(n, p)$; and (3) we show that the random geometric graph on $\\\\mathbb{S}^{d - 1}$ with edge density $p$ converges in total variation to $\\\\mathcal{G}(n, p)$ when $d = \\\\tildeÏ‰\\\\left(\\\\min\\\\{ pn^3, p^2 n^{7/2} \\\\} \\\\right)$, yielding the first progress towards a conjecture of Bubeck et al. (2016). The first of these three results was obtained simultaneously and independently by Bubeck, Racz and Richey.',\n",
       " 'This paper develops several average-case reduction techniques to show new hardness results for three central high-dimensional statistics problems, implying a statistical-computational gap induced by robustness, a detection-recovery gap and a universality principle for these gaps. A main feature of our approach is to map to these problems via a common intermediate problem that we introduce, which we call Imbalanced Sparse Gaussian Mixtures. We assume the planted clique conjecture for a version of the planted clique problem where the position of the planted clique is mildly constrained, and from this obtain the following computational lower bounds: (1) a $k$-to-$k^2$ statistical-computational gap for robust sparse mean estimation, providing the first average-case evidence for a conjecture of Li (2017) and Balakrishnan et al. (2017); (2) a tight lower bound for semirandom planted dense subgraph, which shows that a semirandom adversary shifts the detection threshold in planted dense subgraph to the conjectured recovery threshold; and (3) a universality principle for $k$-to-$k^2$ gaps in a broad class of sparse mixture problems that includes many natural formulations such as the spiked covariance model.\\n  Our main approach is to introduce several average-case techniques to produce structured and Gaussianized versions of an input graph problem, and then to rotate these high-dimensional Gaussians by matrices carefully constructed from hyperplanes in $\\\\mathbb{F}_r^t$. For our universality result, we introduce a new method to perform an algorithmic change of measure tailored to sparse mixtures. We also provide evidence that the mild promise in our variant of planted clique does not change the complexity of the problem.',\n",
       " 'The complexity of clique problems on Erdos-Renyi random graphs has become a central topic in average-case complexity. Algorithmic phase transitions in these problems have been shown to have broad connections ranging from mixing of Markov chains to information-computation gaps in high-dimensional statistics. We consider the problem of counting $k$-cliques in $s$-uniform Erdos-Renyi hypergraphs $G(n,c,s)$ with edge density $c$, and show that its fine-grained average-case complexity can be based on its worst-case complexity. We prove the following:\\n  1. Dense Erdos-Renyi graphs and hypergraphs: Counting $k$-cliques on $G(n,c,s)$ with $k$ and $c$ constant matches its worst-case complexity up to a $\\\\mathrm{polylog}(n)$ factor. Assuming ETH, it takes $n^{Î©(k)}$ time to count $k$-cliques in $G(n,c,s)$ if $k$ and $c$ are constant.\\n  2. Sparse Erdos-Renyi graphs and hypergraphs: When $c = Î˜(n^{-Î±})$, our reduction yields different average-case phase diagrams depicting a tradeoff between runtime and $k$ for each fixed $Î±$. Assuming the best known worst-case algorithms are optimal, in the graph case of $s = 2$, we establish that the exponent in $n$ of the optimal running time for $k$-clique counting in $G(n,c,s)$ is $\\\\frac{Ï‰k}{3} - C Î±\\\\binom{k}{2} + O_{k,Î±}(1)$, where $\\\\fracÏ‰{9} \\\\le C \\\\le 1$ and $Ï‰$ is the matrix multiplication constant. In the hypergraph case of $s \\\\ge 3$, we show a lower bound at the exponent of $k - Î±\\\\binom{k}{s} + O_{k,Î±}(1)$, which surprisingly is tight exactly for the set of $c$ above the Erdos-Renyi $k$-clique percolation threshold.\\n  Our reduction yields the first known average-case hardness result on Erdos-Renyi hypergraphs based on worst-case hardness conjectures. We also analyze algorithms for counting $k$-cliques in $G(n,c,s)$ to prove our upper bounds in the sparse case $c = Î˜(n^{-Î±})$.',\n",
       " 'In the past decade, sparse principal component analysis has emerged as an archetypal problem for illustrating statistical-computational tradeoffs. This trend has largely been driven by a line of research aiming to characterize the average-case complexity of sparse PCA through reductions from the planted clique (PC) conjecture - which conjectures that there is no polynomial-time algorithm to detect a planted clique of size $K = o(N^{1/2})$ in $\\\\mathcal{G}(N, \\\\frac{1}{2})$. All previous reductions to sparse PCA either fail to show tight computational lower bounds matching existing algorithms or show lower bounds for formulations of sparse PCA other than its canonical generative model, the spiked covariance model. Also, these lower bounds all quickly degrade with the exponent in the PC conjecture. Specifically, when only given the PC conjecture up to $K = o(N^Î±)$ where $Î±< 1/2$, there is no sparsity level $k$ at which these lower bounds remain tight. If $Î±\\\\le 1/3$ these reductions fail to even show the existence of a statistical-computational tradeoff at any sparsity $k$. We give a reduction from PC that yields the first full characterization of the computational barrier in the spiked covariance model, providing tight lower bounds at all sparsities $k$. We also show the surprising result that weaker forms of the PC conjecture up to clique size $K = o(N^Î±)$ for any given $Î±\\\\in (0, 1/2]$ imply tight computational lower bounds for sparse PCA at sparsities $k = o(n^{Î±/3})$. This shows that even a mild improvement in the signal strength needed by the best known polynomial-time sparse PCA algorithms would imply that the hardness threshold for PC is subpolynomial. This is the first instance of a suboptimal hardness assumption implying optimal lower bounds for another problem in unsupervised learning.',\n",
       " 'In the general submatrix detection problem, the task is to detect the presence of a small $k \\\\times k$ submatrix with entries sampled from a distribution $\\\\mathcal{P}$ in an $n \\\\times n$ matrix of samples from $\\\\mathcal{Q}$. This formulation includes a number of well-studied problems, such as biclustering when $\\\\mathcal{P}$ and $\\\\mathcal{Q}$ are Gaussians and the planted dense subgraph formulation of community detection when the submatrix is a principal minor and $\\\\mathcal{P}$ and $\\\\mathcal{Q}$ are Bernoulli random variables. These problems all seem to exhibit a universal phenomenon: there is a statistical-computational gap depending on $\\\\mathcal{P}$ and $\\\\mathcal{Q}$ between the minimum $k$ at which this task can be solved and the minimum $k$ at which it can be solved in polynomial time. Our main result is to tightly characterize this computational barrier as a tradeoff between $k$ and the KL divergences between $\\\\mathcal{P}$ and $\\\\mathcal{Q}$ through average-case reductions from the planted clique conjecture. These computational lower bounds hold given mild assumptions on $\\\\mathcal{P}$ and $\\\\mathcal{Q}$ arising naturally from classical binary hypothesis testing. Our results recover and generalize the planted clique lower bounds for Gaussian biclustering in Ma-Wu (2015) and Brennan et al. (2018) and for the sparse and general regimes of planted dense subgraph in Hajek et al. (2015) and Brennan et al. (2018). This yields the first universality principle for computational lower bounds obtained through average-case reductions.',\n",
       " 'Sparse Principal Component Analysis (SPCA) and Sparse Linear Regression (SLR) have a wide range of applications and have attracted a tremendous amount of attention in the last two decades as canonical examples of statistical problems in high dimension. A variety of algorithms have been proposed for both SPCA and SLR, but an explicit connection between the two had not been made. We show how to efficiently transform a black-box solver for SLR into an algorithm for SPCA: assuming the SLR solver satisfies prediction error guarantees achieved by existing efficient algorithms such as those based on the Lasso, the SPCA algorithm derived from it achieves near state of the art guarantees for testing and for support recovery for the single spiked covariance model as obtained by the current best polynomialtime algorithms. Our reduction not only highlights the inherent similarity between the two problems, but also, from a practical standpoint, allows one to obtain a collection of algorithms for SPCA directly from known algorithms for SLR. We provide experimental results on simulated data comparing our proposed framework to other algorithms for SPCA.',\n",
       " 'The prototypical high-dimensional statistics problem entails finding a structured signal in noise. Many of these problems exhibit an intriguing phenomenon: the amount of data needed by all known computationally efficient algorithms far exceeds what is needed for inefficient algorithms that search over all possible structures. A line of work initiated by Berthet and Rigollet in 2013 has aimed to explain these statistical-computational gaps by reducing from conjecturally hard average-case problems in computer science. However, the delicate nature of average-case reductions has limited the applicability of this approach. In this work we introduce several new techniques to give a web of average-case reductions showing strong computational lower bounds based on the planted clique conjecture using natural problems as intermediates. These include tight lower bounds for Planted Independent Set, Planted Dense Subgraph, Sparse Spiked Wigner, Sparse PCA, a subgraph variant of the Stochastic Block Model and a biased variant of Sparse PCA. We also give algorithms matching our lower bounds and identify the information-theoretic limits of the models we consider.',\n",
       " 'Graphical models are a rich language for describing high-dimensional distributions in terms of their dependence structure. While there are algorithms with provable guarantees for learning undirected graphical models in a variety of settings, there has been much less progress in the important scenario when there are latent variables. Here we study Restricted Boltzmann Machines (or RBMs), which are a popular model with wide-ranging applications in dimensionality reduction, collaborative filtering, topic modeling, feature extraction and deep learning.\\n  The main message of our paper is a strong dichotomy in the feasibility of learning RBMs, depending on the nature of the interactions between variables: ferromagnetic models can be learned efficiently, while general models cannot. In particular, we give a simple greedy algorithm based on influence maximization to learn ferromagnetic RBMs with bounded degree. In fact, we learn a description of the distribution on the observed variables as a Markov Random Field. Our analysis is based on tools from mathematical physics that were developed to show the concavity of magnetization. Our algorithm extends straighforwardly to general ferromagnetic Ising models with latent variables.\\n  Conversely, we show that even for a contant number of latent variables with constant degree, without ferromagneticity the problem is as hard as sparse parity with noise. This hardness result is based on a sharp and surprising characterization of the representational power of bounded degree RBMs: the distribution on their observed variables can simulate any bounded order MRF. This result is of independent interest since RBMs are the building blocks of deep belief networks.',\n",
       " 'Most information systems store data by modifying the local state of matter, in the hope that atomic (or sub-atomic) local interactions would stabilize the state for a sufficiently long time, thereby allowing later recovery. In this work we initiate the study of information retention in locally-interacting systems. The evolution in time of the interacting particles is modeled via the stochastic Ising model (SIM). The initial spin configuration $X_0$ serves as the user-controlled input. The output configuration $X_t$ is produced by running $t$ steps of the Glauber chain. Our main goal is to evaluate the information capacity $I_n(t)\\\\triangleq\\\\max_{p_{X_0}}I(X_0;X_t)$ when the time $t$ scales with the size of the system $n$. For the zero-temperature SIM on the two-dimensional $\\\\sqrt{n}\\\\times\\\\sqrt{n}$ grid and free boundary conditions, it is easy to show that $I_n(t) = Î˜(n)$ for $t=O(n)$. In addition, we show that on the order of $\\\\sqrt{n}$ bits can be stored for infinite time in striped configurations. The $\\\\sqrt{n}$ achievability is optimal when $t\\\\to\\\\infty$ and $n$ is fixed.\\n  One of the main results of this work is an achievability scheme that stores more than $\\\\sqrt{n}$ bits (in orders of magnitude) for superlinear (in $n$) times. The analysis of the scheme decomposes the system into $Î©(\\\\sqrt{n})$ independent Z-channels whose crossover probability is found via the (recently rigorously established) Lifshitz law of phase boundary movement. We also provide results for the positive but small temperature regime. We show that an initial configuration drawn according to the Gibbs measure cannot retain more than a single bit for $t\\\\geq e^{cn^{\\\\frac{1}{4}+Îµ}}$. On the other hand, when scaling time with $Î²$, the stripe-based coding scheme (that stores for infinite time at zero temperature) is shown to retain its bits for time that is exponential in $Î²$.',\n",
       " \"We study the problem of testing, using only a single sample, between mean field distributions (like Curie-Weiss, ErdÅ‘s-RÃ©nyi) and structured Gibbs distributions (like Ising model on sparse graphs and Exponential Random Graphs). Our goal is to test without knowing the parameter values of the underlying models: only the \\\\emph{structure} of dependencies is known. We develop a new approach that applies to both the Ising and Exponential Random Graph settings based on a general and natural statistical test. The test can distinguish the hypotheses with high probability above a certain threshold in the (inverse) temperature parameter, and is optimal in that below the threshold no test can distinguish the hypotheses.\\n  The thresholds do not correspond to the presence of long-range order in the models. By aggregating information at a global scale, our test works even at very high temperatures.\\n  The proofs are based on distributional approximation and sharp concentration of quadratic forms, when restricted to Hamming spheres. The restriction to Hamming spheres is necessary, since otherwise any scalar statistic is useless without explicit knowledge of the temperature parameter. At the same time, this restriction radically changes the behavior of the functions under consideration, resulting in a much smaller variance than in the independent setting; this makes it hard to directly apply standard methods (i.e., Stein's method) for concentration of weakly dependent variables. Instead, we carry out an additional tensorization argument using a Markov chain that respects the symmetry of the Hamming sphere.\",\n",
       " \"We develop a new technique, based on Stein's method, for comparing two stationary distributions of irreducible Markov Chains whose update rules are `close enough'. We apply this technique to compare Ising models on $d$-regular expander graphs to the Curie-Weiss model (complete graph) in terms of pairwise correlations and more generally $k$th order moments. Concretely, we show that $d$-regular Ramanujan graphs approximate the $k$th order moments of the Curie-Weiss model to within average error $k/\\\\sqrt{d}$ (averaged over the size $k$ subsets). The result applies even in the low-temperature regime; we also derive some simpler approximation results for functionals of Ising models that hold only at high enough temperatures.\",\n",
       " \"We consider an online model for recommendation systems, with each user being recommended an item at each time-step and providing 'like' or 'dislike' feedback. Each user may be recommended a given item at most once. A latent variable model specifies the user preferences: both users and items are clustered into types. All users of a given type have identical preferences for the items, and similarly, items of a given type are either all liked or all disliked by a given user. We assume that the matrix encoding the preferences of each user type for each item type is randomly generated; in this way, the model captures structure in both the item and user spaces, the amount of structure depending on the number of each of the types. The measure of performance of the recommendation system is the expected number of disliked recommendations per user, defined as expected regret. We propose two algorithms inspired by user-user and item-item collaborative filtering (CF), modified to explicitly make exploratory recommendations, and prove performance guarantees in terms of their expected regret. For two regimes of model parameters, with structure only in item space or only in user space, we prove information-theoretic lower bounds on regret that match our upper bounds up to logarithmic factors. Our analysis elucidates system operating regimes in which existing CF algorithms are nearly optimal.\",\n",
       " 'We study the problem of learning a tree Ising model from samples such that subsequent predictions made using the model are accurate. The prediction task considered in this paper is that of predicting the values of a subset of variables given values of some other subset of variables. Virtually all previous work on graphical model learning has focused on recovering the true underlying graph. We define a distance (\"small set TV\" or ssTV) between distributions $P$ and $Q$ by taking the maximum, over all subsets $\\\\mathcal{S}$ of a given size, of the total variation between the marginals of $P$ and $Q$ on $\\\\mathcal{S}$; this distance captures the accuracy of the prediction task of interest. We derive non-asymptotic bounds on the number of samples needed to get a distribution (from the same class) with small ssTV relative to the one generating the samples. One of the main messages of this paper is that far fewer samples are needed than for recovering the underlying tree, which means that accurate predictions are possible using the wrong tree.',\n",
       " 'There is much empirical evidence that item-item collaborative filtering works well in practice. Motivated to understand this, we provide a framework to design and analyze various recommendation algorithms. The setup amounts to online binary matrix completion, where at each time a random user requests a recommendation and the algorithm chooses an entry to reveal in the user\\'s row. The goal is to minimize regret, or equivalently to maximize the number of +1 entries revealed at any time. We analyze an item-item collaborative filtering algorithm that can achieve fundamentally better performance compared to user-user collaborative filtering. The algorithm achieves good \"cold-start\" performance (appropriately defined) by quickly making good recommendations to new users about whom there is little information.',\n",
       " 'In this paper we investigate the computational complexity of learning the graph structure underlying a discrete undirected graphical model from i.i.d. samples. We first observe that the notoriously difficult problem of learning parities with noise can be captured as a special case of learning graphical models. This leads to an unconditional computational lower bound of $Î©(p^{d/2})$ for learning general graphical models on $p$ nodes of maximum degree $d$, for the class of so-called statistical algorithms recently introduced by Feldman et al (2013). The lower bound suggests that the $O(p^d)$ runtime required to exhaustively search over neighborhoods cannot be significantly improved without restricting the class of models.\\n  Aside from structural assumptions on the graph such as it being a tree, hypertree, tree-like, etc., many recent papers on structure learning assume that the model has the correlation decay property. Indeed, focusing on ferromagnetic Ising models, Bento and Montanari (2009) showed that all known low-complexity algorithms fail to learn simple graphs when the interaction strength exceeds a number related to the correlation decay threshold. Our second set of results gives a class of repelling (antiferromagnetic) models that have the opposite behavior: very strong interaction allows efficient learning in time $O(p^2)$. We provide an algorithm whose performance interpolates between $O(p^2)$ and $O(p^{d+2})$ depending on the strength of the repulsion.',\n",
       " 'Despite the prevalence of collaborative filtering in recommendation systems, there has been little theoretical development on why and how well it works, especially in the \"online\" setting, where items are recommended to users over time. We address this theoretical gap by introducing a model for online recommendation systems, cast item recommendation under the model as a learning problem, and analyze the performance of a cosine-similarity collaborative filtering method. In our model, each of $n$ users either likes or dislikes each of $m$ items. We assume there to be $k$ types of users, and all the users of a given type share a common string of probabilities determining the chance of liking each item. At each time step, we recommend an item to each user, where a key distinction from related bandit literature is that once a user consumes an item (e.g., watches a movie), then that item cannot be recommended to the same user again. The goal is to maximize the number of likable items recommended to users over time. Our main result establishes that after nearly $\\\\log(km)$ initial learning time steps, a simple collaborative filtering algorithm achieves essentially optimal performance without knowing $k$. The algorithm has an exploitation step that uses cosine similarity and two types of exploration steps, one to explore the space of items (standard in the literature) and the other to explore similarity between users (novel to this work).',\n",
       " 'We consider the problem of reconstructing the graph underlying an Ising model from i.i.d. samples. Over the last fifteen years this problem has been of significant interest in the statistics, machine learning, and statistical physics communities, and much of the effort has been directed towards finding algorithms with low computational cost for various restricted classes of models. Nevertheless, for learning Ising models on general graphs with $p$ nodes of degree at most $d$, it is not known whether or not it is possible to improve upon the $p^{d}$ computation needed to exhaustively search over all possible neighborhoods for each node.\\n  In this paper we show that a simple greedy procedure allows to learn the structure of an Ising model on an arbitrary bounded-degree graph in time on the order of $p^2$. We make no assumptions on the parameters except what is necessary for identifiability of the model, and in particular the results hold at low-temperatures as well as for highly non-uniform models. The proof rests on a new structural property of Ising models: we show that for any node there exists at least one neighbor with which it has a high mutual information. This structural property may be of independent interest.',\n",
       " 'In this paper we consider the problem of learning undirected graphical models from data generated according to the Glauber dynamics. The Glauber dynamics is a Markov chain that sequentially updates individual nodes (variables) in a graphical model and it is frequently used to sample from the stationary distribution (to which it converges given sufficient time). Additionally, the Glauber dynamics is a natural dynamical model in a variety of settings. This work deviates from the standard formulation of graphical model learning in the literature, where one assumes access to i.i.d. samples from the distribution.\\n  Much of the research on graphical model learning has been directed towards finding algorithms with low computational cost. As the main result of this work, we establish that the problem of reconstructing binary pairwise graphical models is computationally tractable when we observe the Glauber dynamics. Specifically, we show that a binary pairwise graphical model on $p$ nodes with maximum degree $d$ can be learned in time $f(d)p^2\\\\log p$, for a function $f(d)$, using nearly the information-theoretic minimum number of samples.',\n",
       " 'We consider the problem of learning the canonical parameters specifying an undirected graphical model (Markov random field) from the mean parameters. For graphical models representing a minimal exponential family, the canonical parameters are uniquely determined by the mean parameters, so the problem is feasible in principle. The goal of this paper is to investigate the computational feasibility of this statistical task. Our main result shows that parameter estimation is in general intractable: no algorithm can learn the canonical parameters of a generic pair-wise binary graphical model from the mean parameters in time bounded by a polynomial in the number of variables (unless RP = NP). Indeed, such a result has been believed to be true (see the monograph by Wainwright and Jordan (2008)) but no proof was known.\\n  Our proof gives a polynomial time reduction from approximating the partition function of the hard-core model, known to be hard, to learning approximate parameters. Our reduction entails showing that the marginal polytope boundary has an inherent repulsive property, which validates an optimization procedure over the polytope that does not use any knowledge of its structure (as required by the ellipsoid method and others).',\n",
       " 'We study vector space interference alignment for the MIMO interference channel with no time or frequency diversity, and no symbol extensions. We prove both necessary and sufficient conditions for alignment. In particular, we characterize the feasibility of alignment for the symmetric three-user channel where all users transmit along d dimensions, all transmitters have M antennas and all receivers have N antennas, as well as feasibility of alignment for the fully symmetric (M=N) channel with an arbitrary number of users.\\n  An implication of our results is that the total degrees of freedom available in a K-user interference channel, using only spatial diversity from the multiple antennas, is at most 2. This is in sharp contrast to the K/2 degrees of freedom shown to be possible by Cadambe and Jafar with arbitrarily large time or frequency diversity.\\n  Moving beyond the question of feasibility, we additionally discuss computation of the number of solutions using Schubert calculus in cases where there are a finite number of solutions.',\n",
       " \"We present a framework for the design of optimal assembly algorithms for shotgun sequencing under the criterion of complete reconstruction. We derive a lower bound on the read length and the coverage depth required for reconstruction in terms of the repeat statistics of the genome. Building on earlier works, we design a de Brujin graph based assembly algorithm which can achieve very close to the lower bound for repeat statistics of a wide range of sequenced genomes, including the GAGE datasets. The results are based on a set of necessary and sufficient conditions on the DNA sequence and the reads for reconstruction. The conditions can be viewed as the shotgun sequencing analogue of Ukkonen-Pevzner's necessary and sufficient conditions for Sequencing by Hybridization.\",\n",
       " 'DNA sequencing is the basic workhorse of modern day biology and medicine. Shotgun sequencing is the dominant technique used: many randomly located short fragments called reads are extracted from the DNA sequence, and these reads are assembled to reconstruct the original sequence. A basic question is: given a sequencing technology and the statistics of the DNA sequence, what is the minimum number of reads required for reliable reconstruction? This number provides a fundamental limit to the performance of {\\\\em any} assembly algorithm. For a simple statistical model of the DNA sequence and the read process, we show that the answer admits a critical phenomena in the asymptotic limit of long DNA sequences: if the read length is below a threshold, reconstruction is impossible no matter how many reads are observed, and if the read length is above the threshold, having enough reads to cover the DNA sequence is sufficient to reconstruct. The threshold is computed in terms of the Renyi entropy rate of the DNA sequence. We also study the impact of noise in the read process on the performance.',\n",
       " 'This paper studies vector space interference alignment for the three-user MIMO interference channel with no time or frequency diversity. The main result is a characterization of the feasibility of interference alignment in the symmetric case where all transmitters have M antennas and all receivers have N antennas. If N >= M and all users desire d transmit dimensions, then alignment is feasible if and only if (2r+1)d <= max(rN,(r+1)M) for all nonnegative integers r. The analogous result holds with M and N switched if M >= N.\\n  It turns out that, just as for the 3-user parallel interference channel \\\\cite{BT09}, the length of alignment paths captures the essence of the problem. In fact, for each feasible value of M and N the maximum alignment path length dictates both the converse and achievability arguments.\\n  One of the implications of our feasibility criterion is that simply counting equations and comparing to the number of variables does not predict feasibility. Instead, a more careful investigation of the geometry of the alignment problem is required. The necessary condition obtained by counting equations is implied by our new feasibility criterion.',\n",
       " 'Determining the feasibility conditions for vector space interference alignment in the K-user MIMO interference channel with constant channel coefficients has attracted much recent attention yet remains unsolved. The main result of this paper is restricted to the symmetric square case where all transmitters and receivers have N antennas, and each user desires d transmit dimensions. We prove that alignment is possible if and only if the number of antennas satisfies N>= d(K+1)/2. We also show a necessary condition for feasibility of alignment with arbitrary system parameters. An algebraic geometry approach is central to the results.',\n",
       " '  Exponential random graphs are used extensively in the sociology literature. This model seeks to incorporate in random graphs the notion of reciprocity, that is, the larger than expected number of triangles and other small subgraphs. Sampling from these distributions is crucial for parameter estimation hypothesis testing, and more generally for understanding basic features of the network model itself. In practice sampling is typically carried out using Markov chain Monte Carlo, in particular either the Glauber dynamics or the Metropolis-Hasting procedure.\\n  In this paper we characterize the high and low temperature regimes of the exponential random graph model. We establish that in the high temperature regime the mixing time of the Glauber dynamics is $Î˜(n^2 \\\\log n)$, where $n$ is the number of vertices in the graph; in contrast, we show that in the low temperature regime the mixing is exponentially slow for any local Markov chain. Our results, moreover, give a rigorous basis for criticisms made of such models. In the high temperature regime, where sampling with MCMC is possible, we show that any finite collection of edges are asymptotically independent; thus, the model does not possess the desired reciprocity property, and is not appreciably different from the ErdÅ‘s-RÃ©nyi random graph.',\n",
       " '  Recently, Etkin, Tse, and Wang found the capacity region of the two-user Gaussian interference channel to within one bit/s/Hz. A natural goal is to apply this approach to the Gaussian interference channel with an arbitrary number of users. We make progress towards this goal by finding the capacity region of the many-to-one and one-to-many Gaussian interference channels to within a constant number of bits. The result makes use of a deterministic model to provide insight into the Gaussian channel. The deterministic model makes explicit the dimension of signal scale. A central theme emerges: the use of lattice codes for alignment of interfering signals on the signal scale.',\n",
       " '  This paper explores the two-user Gaussian interference channel through the lens of a natural deterministic channel model. The main result is that the deterministic channel uniformly approximates the Gaussian channel, the capacity regions differing by a universal constant. The problem of finding the capacity of the Gaussian channel to within a constant error is therefore reduced to that of finding the capacity of the far simpler deterministic channel. Thus, the paper provides an alternative derivation of the recent constant gap capacity characterization of Etkin, Tse, and Wang. Additionally, the deterministic model gives significant insight towards the Gaussian channel.',\n",
       " 'Markov random fields are used to model high dimensional distributions in a number of applied areas. Much recent interest has been devoted to the reconstruction of the dependency structure from independent samples from the Markov random fields. We analyze a simple algorithm for reconstructing the underlying graph defining a Markov random field on $n$ nodes and maximum degree $d$ given observations. We show that under mild non-degeneracy conditions it reconstructs the generating graph with high probability using $Î˜(d Îµ^{-2}Î´^{-4} \\\\log n)$ samples where $Îµ,Î´$ depend on the local interactions. For most local interaction $\\\\eps,Î´$ are of order $\\\\exp(-O(d))$.\\n Our results are optimal as a function of $n$ up to a multiplicative constant depending on $d$ and the strength of the local interactions.  Our results seem to be the first results for general models that guarantee that {\\\\em the} generating model is reconstructed. Furthermore, we provide explicit $O(n^{d+2} Îµ^{-2}Î´^{-4}  \\\\log n)$ running time bound. In cases where the measure on the graph has correlation decay, the running time is $O(n^2 \\\\log n)$ for all fixed $d$. We also discuss the effect of observing noisy samples and show that as long as the noise level is low, our algorithm is effective. On the other hand, we construct an example where large noise implies non-identifiability even for generic noise and interactions. Finally, we briefly show that in some simple cases, models with hidden nodes can also be recovered.',\n",
       " 'While the cost of sequencing genomes has decreased dramatically in recent years, this expense often remains non-trivial. Under a fixed budget, then, scientists face a natural trade-off between quantity and quality; they can spend resources to sequence a greater number of genomes (quantity) or spend resources to sequence genomes with increased accuracy (quality). Our goal is to find the optimal allocation of resources between quantity and quality. Optimizing resource allocation promises to reveal as many new variations in the genome as possible, and thus as many new scientific insights as possible. In this paper, we consider the common setting where scientists have already conducted a pilot study to reveal variants in a genome and are contemplating a follow-up study. We introduce a Bayesian nonparametric methodology to predict the number of new variants in the follow-up study based on the pilot study. When experimental conditions are kept constant between the pilot and follow-up, we demonstrate on real data from the gnomAD project that our prediction is more accurate than three recent proposals, and competitive with a more classic proposal. Unlike existing methods, though, our method allows practitioners to change experimental conditions between the pilot and the follow-up. We demonstrate how this distinction allows our method to be used for (i) more realistic predictions and (ii) optimal allocation of a fixed budget between quality and quantity.',\n",
       " 'Variational inference has become an increasingly attractive fast alternative to Markov chain Monte Carlo methods for approximate Bayesian inference. However, a major obstacle to the widespread use of variational methods is the lack of post-hoc accuracy measures that are both theoretically justified and computationally efficient. In this paper, we provide rigorous bounds on the error of posterior mean and uncertainty estimates that arise from full-distribution approximations, as in variational inference. Our bounds are widely applicable, as they require only that the approximating and exact posteriors have polynomial moments. Our bounds are also computationally efficient for variational inference because they require only standard values from variational objectives, straightforward analytic calculations, and simple Monte Carlo estimates. We show that our analysis naturally leads to a new and improved workflow for validated variational inference. Finally, we demonstrate the utility of our proposed workflow and error bounds on a robust regression problem and on a real-data example with a widely used multilevel hierarchical model.',\n",
       " 'Cross validation (CV) and the bootstrap are ubiquitous model-agnostic tools for assessing the error or variability of machine learning and statistical estimators. However, these methods require repeatedly re-fitting the model with different weighted versions of the original dataset, which can be prohibitively time-consuming. For sufficiently regular optimization problems the optimum depends smoothly on the data weights, and so the process of repeatedly re-fitting can be approximated with a Taylor series that can be often evaluated relatively quickly. The first-order approximation is known as the \"infinitesimal jackknife\" in the statistics literature and has been the subject of recent interest in machine learning for approximate CV. In this work, we consider high-order approximations, which we call the \"higher-order infinitesimal jackknife\" (HOIJ). Under mild regularity conditions, we provide a simple recursive procedure to compute approximations of all orders with finite-sample accuracy bounds. Additionally, we show that the HOIJ can be efficiently computed even in high dimensions using forward-mode automatic differentiation. We show that a linear approximation with bootstrap weights approximation is equivalent to those provided by asymptotic normal approximations. Consequently, the HOIJ opens up the possibility of enjoying higher-order accuracy properties of the bootstrap using local approximations. Consistency of the HOIJ for leave-one-out CV under different asymptotic regimes follows as corollaries from our finite-sample bounds under additional regularity assumptions. The generality of the computation and bounds motivate the name \"higher-order Swiss Army infinitesimal jackknife.\"',\n",
       " \"Exchangeability---in which the distribution of an infinite sequence is invariant to reorderings of its elements---implies the existence of a simple conditional independence structure that may be leveraged in the design of probabilistic models and efficient inference algorithms. In practice, however, this assumption is too strong an idealization; the distribution typically fails to be exactly invariant to permutations and de Finetti's representation theory does not apply. Thus there is the need for a distributional assumption that is both weak enough to hold in practice, and strong enough to guarantee a useful underlying representation. We introduce a relaxed notion of local exchangeability---where swapping data associated with nearby covariates causes a bounded change in the distribution. Next, we prove that locally exchangeable processes correspond to independent observations from an underlying measure-valued stochastic process, showing that de Finetti's theorem is robust to perturbation and providing further justification for the Bayesian modelling approach. We also provide an investigation of approximate sufficiency and sample continuity properties of locally exchangeable processes on the real line. The paper concludes with examples of popular statistical models that exhibit local exchangeability.\",\n",
       " 'Leave-one-out cross validation (LOOCV) can be particularly accurate among cross validation (CV) variants for estimating out-of-sample error. But it is expensive to re-fit a model $N$ times for a dataset of size $N$. Previous work has shown that approximations to LOOCV can be both fast and accurate -- when the unknown parameter is of small, fixed dimension. However, these approximations incur a running time roughly cubic in dimension -- and we show that, even when computed perfectly, their accuracy dramatically deteriorates in high dimensions. Authors have suggested many potential and seemingly intuitive solutions, but these methods have not yet been systematically evaluated or compared. In our analysis, we find that all but one perform so poorly as to be unusable for approximating LOOCV. Crucially, though, we are able to show, both empirically and theoretically, that one approximation can perform well in high dimensions -- in cases where the high-dimensional parameter exhibits sparsity. Under interpretable assumptions, our theory demonstrates that the problem can be reduced to working within an empirically recovered (small) support. The corresponding algorithm is straightforward to implement, and we prove that its running time and error depend on the (small) support size even when the full parameter dimension is large.',\n",
       " 'Due to the ease of modern data collection, applied statisticians often have access to a large set of covariates that they wish to relate to some observed outcome. Generalized linear models (GLMs) offer a particularly interpretable framework for such an analysis. In these high-dimensional problems, the number of covariates is often large relative to the number of observations, so we face non-trivial inferential uncertainty; a Bayesian approach allows coherent quantification of this uncertainty. Unfortunately, existing methods for Bayesian inference in GLMs require running times roughly cubic in parameter dimension, and so are limited to settings with at most tens of thousand parameters. We propose to reduce time and memory costs with a low-rank approximation of the data in an approach we call LR-GLM. When used with the Laplace approximation or Markov chain Monte Carlo, LR-GLM provides a full Bayesian posterior approximation and admits running times reduced by a full factor of the parameter dimension. We rigorously establish the quality of our approximation and show how the choice of rank allows a tunable computational-statistical trade-off. Experiments support our theory and demonstrate the efficacy of LR-GLM on real large-scale datasets.',\n",
       " 'Discovering interaction effects on a response of interest is a fundamental problem faced in biology, medicine, economics, and many other scientific disciplines. In theory, Bayesian methods for discovering pairwise interactions enjoy many benefits such as coherent uncertainty quantification, the ability to incorporate background knowledge, and desirable shrinkage properties. In practice, however, Bayesian methods are often computationally intractable for even moderate-dimensional problems. Our key insight is that many hierarchical models of practical interest admit a particular Gaussian process (GP) representation; the GP allows us to capture the posterior with a vector of O(p) kernel hyper-parameters rather than O(p^2) interactions and main effects. With the implicit representation, we can run Markov chain Monte Carlo (MCMC) over model hyper-parameters in time and memory linear in p per iteration. We focus on sparsity-inducing models and show on datasets with a variety of covariate behaviors that our method: (1) reduces runtime by orders of magnitude over naive applications of MCMC, (2) provides lower Type I and Type II error relative to state-of-the-art LASSO-based approaches, and (3) offers improved computational scaling in high dimensions relative to existing Bayesian and LASSO-based approaches.',\n",
       " 'Until recently, transcriptomics was limited to bulk RNA sequencing, obscuring the underlying expression patterns of individual cells in favor of a global average. Thanks to technological advances, we can now profile gene expression across thousands or millions of individual cells in parallel. This new type of data has led to the intriguing discovery that individual cell profiles can reflect the imprint of time or dynamic processes. However, synthesizing this information to reconstruct dynamic biological phenomena from data that are noisy, heterogenous, and sparse---and from processes that may unfold asynchronously---poses a complex computational and statistical challenge. Here, we develop a full generative model for probabilistically reconstructing trees of cellular differentiation from single-cell RNA-seq data. Specifically, we extend the framework of the classical Dirichlet diffusion tree to simultaneously infer branch topology and latent cell states along continuous trajectories over the full tree. In tandem, we construct a novel Markov chain Monte Carlo sampler that interleaves Metropolis-Hastings and message passing to leverage model structure for efficient inference. Finally, we demonstrate that these techniques can recover latent trajectories from simulated single-cell transcriptomes. While this work is motivated by cellular differentiation, we derive a tractable model that provides flexible densities for any data (coupled with an appropriate noise model) that arise from continuous evolution along a latent nonparametric tree.',\n",
       " 'A central question in many probabilistic clustering problems is how many distinct clusters are present in a particular dataset. A Bayesian nonparametric (BNP) model addresses this question by placing a generative process on cluster assignment. However, like all Bayesian approaches, BNP requires the specification of a prior. In practice, it is important to quantitatively establish that the prior is not too informative, particularly when the particular form of the prior is chosen for mathematical convenience rather than because of a considered subjective belief.\\n  We derive local sensitivity measures for a truncated variational Bayes (VB) approximation and approximate nonlinear dependence of a VB optimum on prior parameters using a local Taylor series approximation. Using a stick-breaking representation of a Dirichlet process, we consider perturbations both to the scalar concentration parameter and to the functional form of the stick- breaking distribution.\\n  Unlike previous work on local Bayesian sensitivity for BNP, we pay special attention to the ability of our sensitivity measures to extrapolate to different priors, rather than treating the sensitivity as a measure of robustness per se. Extrapolation motivates the use of multiplicative perturbations to the functional form of the prior for VB. Additionally, we linearly approximate only the computationally intensive part of inference -- the optimization of the global parameters -- and retain the nonlinearity of easily computed quantities as functions of the global parameters.\\n  We apply our methods to estimate sensitivity of the expected number of distinct clusters present in the Iris dataset to the BNP prior specification. We evaluate the accuracy of our approximations by comparing to the much more expensive process of re-fitting the model.',\n",
       " 'Kernel methods offer the flexibility to learn complex relationships in modern, large data sets while enjoying strong theoretical guarantees on quality. Unfortunately, these methods typically require cubic running time in the data set size, a prohibitive cost in the large-data setting. Random feature maps (RFMs) and the Nystrom method both consider low-rank approximations to the kernel matrix as a potential solution. But, in order to achieve desirable theoretical guarantees, the former may require a prohibitively large number of features J+, and the latter may be prohibitively expensive for high-dimensional problems. We propose to combine the simplicity and generality of RFMs with a data-dependent feature selection scheme to achieve desirable theoretical approximation properties of Nystrom with just O(log J+) features. Our key insight is to begin with a large set of random features, then reduce them to a small number of weighted features in a data-dependent, computationally efficient way, while preserving the statistical guarantees of using the original large set of features. We demonstrate the efficacy of our method with theory and experiments--including on a data set with over 50 million observations. In particular, we show that our method achieves small kernel matrix approximation error and better test set accuracy with provably fewer random features than state-of-the-art methods.',\n",
       " 'Bayesian inference typically requires the computation of an approximation to the posterior distribution. An important requirement for an approximate Bayesian inference algorithm is to output high-accuracy posterior mean and uncertainty estimates. Classical Monte Carlo methods, particularly Markov Chain Monte Carlo, remain the gold standard for approximate Bayesian inference because they have a robust finite-sample theory and reliable convergence diagnostics. However, alternative methods, which are more scalable or apply to problems where Markov Chain Monte Carlo cannot be used, lack the same finite-data approximation theory and tools for evaluating their accuracy. In this work, we develop a flexible new approach to bounding the error of mean and uncertainty estimates of scalable inference algorithms. Our strategy is to control the estimation errors in terms of Wasserstein distance, then bound the Wasserstein distance via a generalized notion of Fisher distance. Unlike computing the Wasserstein distance, which requires access to the normalized posterior distribution, the Fisher distance is tractable to compute because it requires access only to the gradient of the log posterior density. We demonstrate the usefulness of our Fisher distance approach by deriving bounds on the Wasserstein error of the Laplace approximation and Hilbert coresets. We anticipate that our approach will be applicable to many other approximate inference methods such as the integrated Laplace approximation, variational inference, and approximate Bayesian computation',\n",
       " 'Gaussian processes (GPs) offer a flexible class of priors for nonparametric Bayesian regression, but popular GP posterior inference methods are typically prohibitively slow or lack desirable finite-data guarantees on quality. We develop an approach to scalable approximate GP regression with finite-data guarantees on the accuracy of pointwise posterior mean and variance estimates. Our main contribution is a novel objective for approximate inference in the nonparametric setting: the preconditioned Fisher (pF) divergence. We show that unlike the Kullback--Leibler divergence (used in variational inference), the pF divergence bounds the 2-Wasserstein distance, which in turn provides tight bounds the pointwise difference of the mean and variance functions. We demonstrate that, for sparse GP likelihood approximations, we can minimize the pF divergence efficiently. Our experiments show that optimizing the pF divergence has the same computational requirements as variational sparse GPs while providing comparable empirical performance--in addition to our novel finite-data quality guarantees.',\n",
       " 'The error or variability of machine learning algorithms is often assessed by repeatedly re-fitting a model with different weighted versions of the observed data. The ubiquitous tools of cross-validation (CV) and the bootstrap are examples of this technique. These methods are powerful in large part due to their model agnosticism but can be slow to run on modern, large data sets due to the need to repeatedly re-fit the model. In this work, we use a linear approximation to the dependence of the fitting procedure on the weights, producing results that can be faster than repeated re-fitting by an order of magnitude. This linear approximation is sometimes known as the \"infinitesimal jackknife\" in the statistics literature, where it is mostly used as a theoretical tool to prove asymptotic results. We provide explicit finite-sample error bounds for the infinitesimal jackknife in terms of a small number of simple, verifiable assumptions. Our results apply whether the weights and data are stochastic or deterministic, and so can be used as a tool for proving the accuracy of the infinitesimal jackknife on a wide variety of problems. As a corollary, we state mild regularity conditions under which our approximation consistently estimates true leave-$k$-out cross-validation for any fixed $k$. These theoretical results, together with modern automatic differentiation software, support the application of the infinitesimal jackknife to a wide variety of practical problems in machine learning, providing a \"Swiss Army infinitesimal jackknife\". We demonstrate the accuracy of our methods on a range of simulated and real datasets.',\n",
       " 'Learning a Bayesian network (BN) from data can be useful for decision-making or discovering causal relationships. However, traditional methods often fail in modern applications, which exhibit a larger number of observed variables than data points. The resulting uncertainty about the underlying network as well as the desire to incorporate prior information recommend a Bayesian approach to learning the BN, but the highly combinatorial structure of BNs poses a striking challenge for inference. The current state-of-the-art methods such as order MCMC are faster than previous methods but prevent the use of many natural structural priors and still have running time exponential in the maximum indegree of the true directed acyclic graph (DAG) of the BN. We here propose an alternative posterior approximation based on the observation that, if we incorporate empirical conditional independence tests, we can focus on a high-probability DAG associated with each order of the vertices. We show that our method allows the desired flexibility in prior specification, removes timing dependence on the maximum indegree and yields provably good posterior approximations; in addition, we show that it achieves superior accuracy, scalability, and sampler mixing on several datasets.',\n",
       " 'Coherent uncertainty quantification is a key strength of Bayesian methods. But modern algorithms for approximate Bayesian posterior inference often sacrifice accurate posterior uncertainty estimation in the pursuit of scalability. This work shows that previous Bayesian coreset construction algorithms---which build a small, weighted subset of the data that approximates the full dataset---are no exception. We demonstrate that these algorithms scale the coreset log-likelihood suboptimally, resulting in underestimated posterior uncertainty. To address this shortcoming, we develop greedy iterative geodesic ascent (GIGA), a novel algorithm for Bayesian coreset construction that scales the coreset log-likelihood optimally. GIGA provides geometric decay in posterior approximation error as a function of coreset size, and maintains the fast running time of its predecessors. The paper concludes with validation of GIGA on both synthetic and real datasets, demonstrating that it reduces posterior approximation error by orders of magnitude compared with previous coreset constructions.',\n",
       " 'Clustering procedures typically estimate which data points are clustered together, a quantity of primary importance in many analyses. Often used as a preliminary step for dimensionality reduction or to facilitate interpretation, finding robust and stable clusters is often crucial for appropriate for downstream analysis. In the present work, we consider Bayesian nonparametric (BNP) models, a particularly popular set of Bayesian models for clustering due to their flexibility. Because of its complexity, the Bayesian posterior often cannot be computed exactly, and approximations must be employed. Mean-field variational Bayes forms a posterior approximation by solving an optimization problem and is widely used due to its speed. An exact BNP posterior might vary dramatically when presented with different data. As such, stability and robustness of the clustering should be assessed.\\n  A popular mean to assess stability is to apply the bootstrap by resampling the data, and rerun the clustering for each simulated data set. The time cost is thus often very expensive, especially for the sort of exploratory analysis where clustering is typically used. We propose to use a fast and automatic approximation to the full bootstrap called the \"linear bootstrap\", which can be seen by local data perturbation. In this work, we demonstrate how to apply this idea to a data analysis pipeline, consisting of an MFVB approximation to a BNP clustering posterior of time course gene expression data. We show that using auto-differentiation tools, the necessary calculations can be done automatically, and that the linear bootstrap is a fast but approximate alternative to the bootstrap.',\n",
       " 'The automation of posterior inference in Bayesian data analysis has enabled experts and nonexperts alike to use more sophisticated models, engage in faster exploratory modeling and analysis, and ensure experimental reproducibility. However, standard automated posterior inference algorithms are not tractable at the scale of massive modern datasets, and modifications to make them so are typically model-specific, require expert tuning, and can break theoretical guarantees on inferential quality. Building on the Bayesian coresets framework, this work instead takes advantage of data redundancy to shrink the dataset itself as a preprocessing step, providing fully-automated, scalable Bayesian inference with theoretical guarantees. We begin with an intuitive reformulation of Bayesian coreset construction as sparse vector sum approximation, and demonstrate that its automation and performance-based shortcomings arise from the use of the supremum norm. To address these shortcomings we develop Hilbert coresets, i.e., Bayesian coresets constructed under a norm induced by an inner-product on the log-likelihood function space. We propose two Hilbert coreset construction algorithms---one based on importance sampling, and one based on the Frank-Wolfe algorithm---along with theoretical guarantees on approximation quality as a function of coreset size. Since the exact computation of the proposed inner-products is model-specific, we automate the construction with a random finite-dimensional projection of the log-likelihood functions. The resulting automated coreset construction algorithm is simple to implement, and experiments on a variety of models with real and synthetic datasets show that it provides high-quality posterior approximations and a significant reduction in the computational cost of inference.',\n",
       " 'Generalized linear models (GLMs) -- such as logistic regression, Poisson regression, and robust regression -- provide interpretable models for diverse data types. Probabilistic approaches, particularly Bayesian ones, allow coherent estimates of uncertainty, incorporation of prior information, and sharing of power across experiments via hierarchical models. In practice, however, the approximate Bayesian methods necessary for inference have either failed to scale to large data sets or failed to provide theoretical guarantees on the quality of inference. We propose a new approach based on constructing polynomial approximate sufficient statistics for GLMs (PASS-GLM). We demonstrate that our method admits a simple algorithm as well as trivial streaming and distributed extensions that do not compound error across computations. We provide theoretical guarantees on the quality of point (MAP) estimates, the approximate posterior, and posterior mean and uncertainty estimates. We validate our approach empirically in the case of logistic regression using a quadratic approximation and show competitive performance with stochastic gradient descent, MCMC, and the Laplace approximation in terms of speed and multiple measures of accuracy -- including on an advertising data set with 40 million data points and 20,000 covariates.',\n",
       " 'Mean-field Variational Bayes (MFVB) is an approximate Bayesian posterior inference technique that is increasingly popular due to its fast runtimes on large-scale datasets. However, even when MFVB provides accurate posterior means for certain parameters, it often mis-estimates variances and covariances. Furthermore, prior robustness measures have remained undeveloped for MFVB. By deriving a simple formula for the effect of infinitesimal model perturbations on MFVB posterior means, we provide both improved covariance estimates and local robustness measures for MFVB, thus greatly expanding the practical usefulness of MFVB posterior approximations. The estimates for MFVB posterior covariances rely on a result from the classical Bayesian robustness literature relating derivatives of posterior expectations to posterior covariances and include the Laplace approximation as a special case. Our key condition is that the MFVB approximation provides good estimates of a select subset of posterior means---an assumption that has been shown to hold in many practical settings. In our experiments, we demonstrate that our methods are simple, general, and fast, providing accurate posterior uncertainty estimates and robustness measures with runtimes that can be an order of magnitude faster than MCMC.',\n",
       " 'Many popular network models rely on the assumption of (vertex) exchangeability, in which the distribution of the graph is invariant to relabelings of the vertices. However, the Aldous-Hoover theorem guarantees that these graphs are dense or empty with probability one, whereas many real-world graphs are sparse. We present an alternative notion of exchangeability for random graphs, which we call edge exchangeability, in which the distribution of a graph sequence is invariant to the order of the edges. We demonstrate that edge-exchangeable models, unlike models that are traditionally vertex exchangeable, can exhibit sparsity. To do so, we outline a general framework for graph generative models; by contrast to the pioneering work of Caron and Fox (2015), models within our framework are stationary across steps of the graph sequence. In particular, our model grows the graph by instantiating more latent atoms of a single random measure as the dataset size increases, rather than adding new atoms to the measure.',\n",
       " 'In Bayesian analysis, the posterior follows from the data and a choice of a prior and a likelihood. One hopes that the posterior is robust to reasonable variation in the choice of prior, since this choice is made by the modeler and is often somewhat subjective. A different, equally subjectively plausible choice of prior may result in a substantially different posterior, and so different conclusions drawn from the data. Were this to be the case, our conclusions would not be robust to the choice of prior. To determine whether our model is robust, we must quantify how sensitive our posterior is to perturbations of our prior. Despite the importance of the problem and a considerable body of literature, generic, easy-to-use methods to quantify Bayesian robustness are still lacking.\\n  Abstract In this paper, we demonstrate that powerful measures of robustness can be easily calculated from Variational Bayes (VB) approximate posteriors. We begin with local robustness, which measures the effect of infinitesimal changes to the prior on a posterior mean of interest. In particular, we show that the influence function of Gustafson (2012) has a simple, easy-to-calculate closed form expression for VB approximations. We then demonstrate how local robustness measures can be inadequate for non-local prior changes, such as replacing one prior entirely with another. We propose a simple approximate non-local robustness measure and demonstrate its effectiveness on a simulated data set.',\n",
       " 'Variational inference (VI) provides fast approximations of a Bayesian posterior in part because it formulates posterior approximation as an optimization problem: to find the closest distribution to the exact posterior over some family of distributions. For practical reasons, the family of distributions in VI is usually constrained so that it does not include the exact posterior, even as a limit point. Thus, no matter how long VI is run, the resulting approximation will not approach the exact posterior. We propose to instead consider a more flexible approximating family consisting of all possible finite mixtures of a parametric base distribution (e.g., Gaussian). For efficient inference, we borrow ideas from gradient boosting to develop an algorithm we call boosting variational inference (BVI). BVI iteratively improves the current approximation by mixing it with a new component from the base distribution family and thereby yields progressively more accurate posterior approximations as more computing time is spent. Unlike a number of common VI variants including mean-field VI, BVI is able to capture multimodality, general posterior covariance, and nonstandard posterior shapes.',\n",
       " 'Trait allocations are a class of combinatorial structures in which data may belong to multiple groups and may have different levels of belonging in each group. Often the data are also exchangeable, i.e., their joint distribution is invariant to reordering. In clustering---a special case of trait allocation---exchangeability implies the existence of both a de Finetti representation and an exchangeable partition probability function (EPPF), distributional representations useful for computational and theoretical purposes. In this work, we develop the analogous de Finetti representation and exchangeable trait probability function (ETPF) for trait allocations, along with a characterization of all trait allocations with an ETPF. Unlike previous feature allocation characterizations, our proofs fully capture single-occurrence \"dust\" groups. We further introduce a novel constrained version of the ETPF that we use to establish an intuitive connection between the probability functions for clustering, feature allocations, and trait allocations. As an application of our general theory, we characterize the distribution of all edge-exchangeable graphs, a class of recently-developed models that captures realistic sparse graph sequences.',\n",
       " 'Bayesian hierarchical models are increasing popular in economics. When using hierarchical models, it is useful not only to calculate posterior expectations, but also to measure the robustness of these expectations to reasonable alternative prior choices. We use variational Bayes and linear response methods to provide fast, accurate posterior means and robustness measures with an application to measuring the effectiveness of microcredit in the developing world.',\n",
       " 'The use of Bayesian methods in large-scale data settings is attractive because of the rich hierarchical models, uncertainty quantification, and prior specification they provide. Standard Bayesian inference algorithms are computationally expensive, however, making their direct application to large datasets difficult or infeasible. Recent work on scaling Bayesian inference has focused on modifying the underlying algorithms to, for example, use only a random data subsample at each iteration. We leverage the insight that data is often redundant to instead obtain a weighted subset of the data (called a coreset) that is much smaller than the original dataset. We can then use this small coreset in any number of existing posterior inference algorithms without modification. In this paper, we develop an efficient coreset construction algorithm for Bayesian logistic regression models. We provide theoretical guarantees on the size and approximation quality of the coreset -- both for fixed, known datasets, and in expectation for a wide class of data generative models. Crucially, the proposed approach also permits efficient construction of the coreset in both streaming and parallel settings, with minimal additional effort. We demonstrate the efficacy of our approach on a number of synthetic and real-world datasets, and find that, in practice, the size of the coreset is independent of the original dataset size. Furthermore, constructing the coreset takes a negligible amount of time compared to that required to run MCMC on it.',\n",
       " 'Network data appear in a number of applications, such as online social networks and biological networks, and there is growing interest in both developing models for networks as well as studying the properties of such data. Since individual network datasets continue to grow in size, it is necessary to develop models that accurately represent the real-life scaling properties of networks. One behavior of interest is having a power law in the degree distribution. However, other types of power laws that have been observed empirically and considered for applications such as clustering and feature allocation models have not been studied as frequently in models for graph data. In this paper, we enumerate desirable asymptotic behavior that may be of interest for modeling graph data, including sparsity and several types of power laws. We outline a general framework for graph generative models using completely random measures; by contrast to the pioneering work of Caron and Fox (2015), we consider instantiating more of the existing atoms of the random measure as the dataset size increases rather than adding new atoms to the measure. We see that these two models can be complementary; they respectively yield interpretations as (1) time passing among existing members of a network and (2) new individuals joining a network. We detail a particular instance of this framework and show simulated results that suggest this model exhibits some desirable asymptotic power-law behavior.',\n",
       " 'A known failing of many popular random graph models is that the Aldous-Hoover Theorem guarantees these graphs are dense with probability one; that is, the number of edges grows quadratically with the number of nodes. This behavior is considered unrealistic in observed graphs. We define a notion of edge exchangeability for random graphs in contrast to the established notion of infinite exchangeability for random graphs --- which has traditionally relied on exchangeability of nodes (rather than edges) in a graph. We show that, unlike node exchangeability, edge exchangeability encompasses models that are known to provide a projective sequence of random graphs that circumvent the Aldous-Hoover Theorem and exhibit sparsity, i.e., sub-quadratic growth of the number of edges with the number of nodes. We show how edge-exchangeability of graphs relates naturally to existing notions of exchangeability from clustering (a.k.a. partitions) and other familiar combinatorial structures.',\n",
       " 'Completely random measures (CRMs) and their normalizations are a rich source of Bayesian nonparametric priors. Examples include the beta, gamma, and Dirichlet processes. In this paper we detail two major classes of sequential CRM representations---series representations and superposition representations---within which we organize both novel and existing sequential representations that can be used for simulation and posterior inference. These two classes and their constituent representations subsume existing ones that have previously been developed in an ad hoc manner for specific processes. Since a complete infinite-dimensional CRM cannot be used explicitly for computation, sequential representations are often truncated for tractability. We provide truncation error analyses for each type of sequential representation, as well as their normalized versions, thereby generalizing and improving upon existing truncation error bounds in the literature. We analyze the computational complexity of the sequential representations, which in conjunction with our error bounds allows us to directly compare representations and discuss their relative efficiency. We include numerous applications of our theoretical results to commonly-used (normalized) CRMs, demonstrating that our results enable a straightforward representation and analysis of CRMs that has not previously been available in a Bayesian nonparametric context.',\n",
       " 'In Bayesian analysis, the posterior follows from the data and a choice of a prior and a likelihood. One hopes that the posterior is robust to reasonable variation in the choice of prior and likelihood, since this choice is made by the modeler and is necessarily somewhat subjective. Despite the fundamental importance of the problem and a considerable body of literature, the tools of robust Bayes are not commonly used in practice. This is in large part due to the difficulty of calculating robustness measures from MCMC draws. Although methods for computing robustness measures from MCMC draws exist, they lack generality and often require additional coding or computation.\\n  In contrast to MCMC, variational Bayes (VB) techniques are readily amenable to robustness analysis. The derivative of a posterior expectation with respect to a prior or data perturbation is a measure of local robustness to the prior or likelihood. Because VB casts posterior inference as an optimization problem, its methodology is built on the ability to calculate derivatives of posterior quantities with respect to model parameters, even in very complex models. In the present work, we develop local prior robustness measures for mean-field variational Bayes(MFVB), a VB technique which imposes a particular factorization assumption on the variational posterior approximation. We start by outlining existing local prior measures of robustness. Next, we use these results to derive closed-form measures of the sensitivity of mean-field variational posterior approximation to prior specification. We demonstrate our method on a meta-analysis of randomized controlled interventions in access to microcredit in developing countries.',\n",
       " 'This article is a translation of Bruno de Finetti\\'s paper \"Funzione Caratteristica di un fenomeno aleatorio\" which appeared in Atti del Congresso Internazionale dei Matematici, Bologna 3-10 Settembre 1928, Tomo VI, pp. 179-190, originally published by Nicola Zanichelli Editore S.p.A. The translation was made as close as possible to the original in form and style, except for apparent mistakes found in the original document, which were corrected and are mentioned as footnotes. Most of these were resolved by comparing against a longer version of this work by de Finetti, published shortly after this one under the same titlea. The interested reader is highly encouraged to consult this other version for a more detailed treatment of the topics covered here. Footnotes regarding the translation are labeled with letters to distinguish them from de Finetti\\'s original footnotes.',\n",
       " 'Mean field variational Bayes (MFVB) is a popular posterior approximation method due to its fast runtime on large-scale data sets. However, it is well known that a major failing of MFVB is that it underestimates the uncertainty of model variables (sometimes severely) and provides no information about model variable covariance.\\n  We generalize linear response methods from statistical physics to deliver accurate uncertainty estimates for model variables---both for individual variables and coherently across variables. We call our method linear response variational Bayes (LRVB). When the MFVB posterior approximation is in the exponential family, LRVB has a simple, analytic form, even for non-conjugate models. Indeed, we make no assumptions about the form of the true posterior. We demonstrate the accuracy and scalability of our method on a range of models for both simulated and real data.',\n",
       " 'Mean field variational Bayes (MFVB) is a popular posterior approximation method due to its fast runtime on large-scale data sets. However, it is well known that a major failing of MFVB is that it underestimates the uncertainty of model variables (sometimes severely) and provides no information about model variable covariance. We develop a fast, general methodology for exponential families that augments MFVB to deliver accurate uncertainty estimates for model variables -- both for individual variables and coherently across variables. MFVB for exponential families defines a fixed-point equation in the means of the approximating posterior, and our approach yields a covariance estimate by perturbing this fixed point. Inspired by linear response theory, we call our method linear response variational Bayes (LRVB). We also show how LRVB can be used to quickly calculate a measure of the influence of individual data points on parameter point estimates. We demonstrate the accuracy and scalability of our method by learning Gaussian mixture models for both simulated and real data.',\n",
       " 'Mean Field Variational Bayes (MFVB) is a popular posterior approximation method due to its fast runtime on large-scale data sets. However, it is well known that a major failing of MFVB is its (sometimes severe) underestimates of the uncertainty of model variables and lack of information about model variable covariance. We develop a fast, general methodology for exponential families that augments MFVB to deliver accurate uncertainty estimates for model variables -- both for individual variables and coherently across variables. MFVB for exponential families defines a fixed-point equation in the means of the approximating posterior, and our approach yields a covariance estimate by perturbing this fixed point. Inspired by linear response theory, we call our method linear response variational Bayes (LRVB). We demonstrate the accuracy of our method on simulated data sets.',\n",
       " 'We demonstrate how to calculate posteriors for general CRM-based priors and likelihoods for Bayesian nonparametric models. We further show how to represent Bayesian nonparametric priors as a sequence of finite draws using a size-biasing approach---and how to represent full Bayesian nonparametric models via finite marginals. Motivated by conjugate priors based on exponential family representations of likelihoods, we introduce a notion of exponential families for CRMs, which we call exponential CRMs. This construction allows us to specify automatic Bayesian nonparametric conjugate priors for exponential CRM likelihoods. We demonstrate that our exponential CRMs allow particularly straightforward recipes for size-biased and marginal representations of Bayesian nonparametric models. Along the way, we prove that the gamma process is a conjugate prior for the Poisson likelihood process and the beta prime process is a conjugate prior for a process we call the odds Bernoulli process. We deliver a size-biased representation of the gamma process and a marginal representation of the gamma process coupled with a Poisson likelihood process.',\n",
       " 'Bayesian entity resolution merges together multiple, noisy databases and returns the minimal collection of unique individuals represented, together with their true, latent record values. Bayesian methods allow flexible generative models that share power across databases as well as principled quantification of uncertainty for queries of the final, resolved database. However, existing Bayesian methods for entity resolution use Markov monte Carlo method (MCMC) approximations and are too slow to run on modern databases containing millions or billions of records. Instead, we propose applying variational approximations to allow scalable Bayesian inference in these models. We derive a coordinate-ascent approximation for mean-field variational Bayes, qualitatively compare our algorithm to existing methods, note unique challenges for inference that arise from the expected distribution of cluster sizes in entity resolution, and discuss directions for future work in this domain.',\n",
       " 'Research on distributed machine learning algorithms has focused primarily on one of two extremes - algorithms that obey strict concurrency constraints or algorithms that obey few or no such constraints. We consider an intermediate alternative in which algorithms optimistically assume that conflicts are unlikely and if conflicts do arise a conflict-resolution protocol is invoked. We view this \"optimistic concurrency control\" paradigm as particularly appropriate for large-scale machine learning algorithms, particularly in the unsupervised setting. We demonstrate our approach in three problem areas: clustering, feature learning and online facility location. We evaluate our methods via large-scale experiments in a cluster computing environment.',\n",
       " 'We present SDA-Bayes, a framework for (S)treaming, (D)istributed, (A)synchronous computation of a Bayesian posterior. The framework makes streaming updates to the estimated posterior according to a user-specified approximation batch primitive. We demonstrate the usefulness of our framework, with variational Bayes (VB) as the primitive, by fitting the latent Dirichlet allocation model to two large-scale document collections. We demonstrate the advantages of our algorithm over stochastic variational inference (SVI) by comparing the two after a single pass through a known amount of data---a case where SVI may be applied---and in the streaming setting, where SVI does not apply.',\n",
       " 'The problem of inferring a clustering of a data set has been the subject of much research in Bayesian analysis, and there currently exists a solid mathematical foundation for Bayesian approaches to clustering. In particular, the class of probability distributions over partitions of a data set has been characterized in a number of ways, including via exchangeable partition probability functions (EPPFs) and the Kingman paintbox. Here, we develop a generalization of the clustering problem, called feature allocation, where we allow each data point to belong to an arbitrary, non-negative integer number of groups, now called features or topics. We define and study an \"exchangeable feature probability function\" (EFPF)---analogous to the EPPF in the clustering setting---for certain types of feature models. Moreover, we introduce a \"feature paintbox\" characterization---analogous to the Kingman paintbox for clustering---of the class of exchangeable feature models. We provide a further characterization of the subclass of feature allocations that have EFPF representations.',\n",
       " 'The classical mixture of Gaussians model is related to K-means via small-variance asymptotics: as the covariances of the Gaussians tend to zero, the negative log-likelihood of the mixture of Gaussians model approaches the K-means objective, and the EM algorithm approaches the K-means algorithm. Kulis & Jordan (2012) used this observation to obtain a novel K-means-like algorithm from a Gibbs sampler for the Dirichlet process (DP) mixture. We instead consider applying small-variance asymptotics directly to the posterior in Bayesian nonparametric models. This framework is independent of any specific Bayesian inference algorithm, and it has the major advantage that it generalizes immediately to a range of models beyond the DP mixture. To illustrate, we apply our framework to the feature learning setting, where the beta process and Indian buffet process provide an appropriate Bayesian nonparametric prior. We obtain a novel objective function that goes beyond clustering to learn (and penalize new) groupings for which we relax the mutual exclusivity and exhaustivity assumptions of clustering. We demonstrate several other algorithms, all of which are scalable and simple to implement. Empirical results demonstrate the benefits of the new framework.',\n",
       " 'We develop algorithms for performing semiparametric regression analysis in real time, with data processed as it is collected and made immediately available via modern telecommunications technologies. Our definition of semiparametric regression is quite broad and includes, as special cases, generalized linear mixed models, generalized additive models, geostatistical models, wavelet nonparametric regression models and their various combinations. Fast updating of regression fits is achieved by couching semiparametric regression into a Bayesian hierarchical model or, equivalently, graphical model framework and employing online mean field variational ideas. An internet site attached to this article, realtime-semiparametric-regression.net, illustrates the methodology for continually arriving stock market, real estate and airline data. Flexible real-time analyses, based on increasingly ubiquitous streaming data sources stand to benefit.',\n",
       " 'One of the focal points of the modern literature on Bayesian nonparametrics has been the problem of clustering, or partitioning, where each data point is modeled as being associated with one and only one of some collection of groups called clusters or partition blocks. Underlying these Bayesian nonparametric models are a set of interrelated stochastic processes, most notably the Dirichlet process and the Chinese restaurant process. In this paper we provide a formal development of an analogous problem, called feature modeling, for associating data points with arbitrary nonnegative integer numbers of groups, now called features or topics. We review the existing combinatorial stochastic process representations for the clustering problem and develop analogous representations for the feature modeling problem. These representations include the beta process and the Indian buffet process as well as new representations that provide insight into the connections between these processes. We thereby bring the same level of completeness to the treatment of Bayesian nonparametric feature modeling that has previously been achieved for Bayesian nonparametric clustering.',\n",
       " 'We introduce a new graphical model for tracking radio-tagged animals and learning their movement patterns. The model provides a principled way to combine radio telemetry data with an arbitrary set of userdefined, spatial features. We describe an efficient stochastic gradient algorithm for fitting model parameters to data and demonstrate its effectiveness via asymptotic analysis and synthetic experiments. We also apply our model to real datasets, and show that it outperforms the most popular radio telemetry software package used in ecology. We conclude that integration of different data sources under a single statistical framework, coupled with appropriate parameter and state estimation procedures, produces both accurate location estimates and an interpretable statistical model of animal movement.',\n",
       " 'As the number of observed Gamma-Ray Bursts (GRBs) continues to grow, follow-up resources need to be used more efficiently in order to maximize science output from limited telescope time. As such, it is becoming increasingly important to rapidly identify bursts of interest as soon as possible after the event, before the afterglows fade beyond detectability. Studying the most distant (highest redshift) events, for instance, remains a primary goal for many in the field. Here we present our Random forest Automated Triage Estimator for GRB redshifts (RATE GRB-z) for rapid identification of high-redshift candidates using early-time metrics from the three telescopes onboard Swift. While the basic RATE methodology is generalizable to a number of resource allocation problems, here we demonstrate its utility for telescope-constrained follow-up efforts with the primary goal to identify and study high-z GRBs. For each new GRB, RATE GRB-z provides a recommendation - based on the available telescope time - of whether the event warrants additional follow-up resources. We train RATE GRB-z using a set consisting of 135 Swift bursts with known redshifts, only 18 of which are z > 4. Cross-validated performance metrics on this training data suggest that ~56% of high-z bursts can be captured from following up the top 20% of the ranked candidates, and ~84% of high-z bursts are identified after following up the top ~40% of candidates. We further use the method to rank 200+ Swift bursts with unknown redshifts according to their likelihood of being high-z.',\n",
       " 'We develop a Bayesian nonparametric approach to a general family of latent class problems in which individuals can belong simultaneously to multiple classes and where each class can be exhibited multiple times by an individual. We introduce a combinatorial stochastic process known as the negative binomial process (NBP) as an infinite-dimensional prior appropriate for such problems. We show that the NBP is conjugate to the beta process, and we characterize the posterior distribution under the beta-negative binomial process (BNBP) and hierarchical models based on the BNBP (the HBNBP). We study the asymptotic properties of the BNBP and develop a three-parameter extension of the BNBP that exhibits power-law behavior. We derive MCMC algorithms for posterior inference under the HBNBP, and we present experiments using these algorithms in the domains of image segmentation, object recognition, and document analysis.',\n",
       " 'The beta-Bernoulli process provides a Bayesian nonparametric prior for models involving collections of binary-valued features. A draw from the beta process yields an infinite collection of probabilities in the unit interval, and a draw from the Bernoulli process turns these into binary-valued features. Recent work has provided stick-breaking representations for the beta process analogous to the well-known stick-breaking representation for the Dirichlet process. We derive one such stick-breaking representation directly from the characterization of the beta process as a completely random measure. This approach motivates a three-parameter generalization of the beta process, and we study the power laws that can be obtained from this generalized beta process. We present a posterior inference algorithm for the beta-Bernoulli process that exploits the stick-breaking representation, and we present experimental results for a discrete factor-analysis model.',\n",
       " '  Selection methods that require only a single-switch input, such as a button click or blink, are potentially useful for individuals with motor impairments, mobile technology users, and individuals wishing to transmit information securely. We present a single-switch selection method, \"Nomon,\" that is general and efficient. Existing single-switch selection methods require selectable options to be arranged in ways that limit potential applications. By contrast, traditional operating systems, web browsers, and free-form applications (such as drawing) place options at arbitrary points on the screen. Nomon, however, has the flexibility to select any point on a screen. Nomon adapts automatically to an individual\\'s clicking ability; it allows a person who clicks precisely to make a selection quickly and allows a person who clicks imprecisely more time to make a selection without error. Nomon reaps gains in information rate by allowing the specification of beliefs (priors) about option selection probabilities and by avoiding tree-based selection schemes in favor of direct (posterior) inference. We have developed both a Nomon-based writing application and a drawing application. To evaluate Nomon\\'s performance, we compared the writing application with a popular existing method for single-switch writing (row-column scanning). Novice users wrote 35% faster with the Nomon interface than with the scanning interface. An experienced user (author TB, with > 10 hours practice) wrote at speeds of 9.3 words per minute with Nomon, using 1.2 clicks per character and making no errors in the final text.',\n",
       " 'Recognizing the successes of treed Gaussian process (TGP) models as an interpretable and thrifty model for nonparametric regression, we seek to extend the model to classification. Both treed models and Gaussian processes (GPs) have, separately, enjoyed great success in application to classification problems. An example of the former is Bayesian CART. In the latter, real-valued GP output may be utilized for classification via latent variables, which provide classification rules by means of a softmax function. We formulate a Bayesian model averaging scheme to combine these two models and describe a Monte Carlo method for sampling from the full posterior distribution with joint proposals for the tree topology and the GP parameters corresponding to latent variables at the leaves. We concentrate on efficient sampling of the latent variables, which is important to obtain good mixing in the expanded parameter space. The tree structure is particularly helpful for this task and also for developing an efficient scheme for handling categorical predictors, which commonly arise in classification problems. Our proposed classification TGP (CTGP) methodology is illustrated on a collection of synthetic and real data sets. We assess performance relative to existing methods and thereby show how CTGP is highly flexible, offers tractable inference, produces rules that are easy to interpret, and performs well out of sample.',\n",
       " '  Recent work has shown that probabilistic models based on pairwise interactions-in the simplest case, the Ising model-provide surprisingly accurate descriptions of experiments on real biological networks ranging from neurons to genes. Finding these models requires us to solve an inverse problem: given experimentally measured expectation values, what are the parameters of the underlying Hamiltonian? This problem sits at the intersection of statistical physics and machine learning, and we suggest that more efficient solutions are possible by merging ideas from the two fields. We use a combination of recent coordinate descent algorithms with an adaptation of the histogram Monte Carlo method, and implement these techniques to take advantage of the sparseness found in data on real neurons. The resulting algorithm learns the parameters of an Ising model describing a network of forty neurons within a few minutes. This opens the possibility of analyzing much larger data sets now emerging, and thus testing hypotheses about the collective behaviors of these networks.',\n",
       " '  We present the results of attempts to detect the ellipticity of dark matter halos using galaxy-galaxy weak lensing with SDSS data. We use 2,020,256 galaxies brighter than r=19 with photometric redshifts (divided into colour and luminosity subsamples) as lenses and 31,697,869 source galaxies. We search for and identify several signal contaminants, which if not removed lead to a spurious detection. These include systematic shear that leads to a slight spurious alignment of lens and source ellipticities, intrinsic alignments (due to contamination of the source sample by physically-associated lens source pairs), and anisotropic magnification bias. We develop methods that allow us to remove these contaminants to the signal. We split the analysis into blue (spiral) and red (elliptical) galaxies. Assuming Gaussian errors as in previous work and a power-law profile, we find f_h=e_h/e_g=0.1+/-0.06 for red galaxies and -0.8+/-0.4 for blue galaxies using 20-300 kpc/h, averaged over luminosity. Inclusion of the more realistic non-Gaussian error distributions and of the NFW density profile (which predicts much smaller ellipticity of the shear for scales above the scale radius) yields 0.60+/-0.38 for ellipticals and -1.4+1.7-2.0 for spirals. While there is no concrete detection of alignment in either case, there is a suggestion in the data of a positive alignment in the brightest lens sample of ellipticals. Our results appear to be mildly inconsistent with a previously reported detection by Hoekstra et al. (2004), but more data and further tests are needed to clarify whether the discrepancy is real or a consequence of differences in the lens galaxy samples used and analysis methods.',\n",
       " '  We investigate the required redshift accuracy of type Ia supernova and cluster number-count surveys in order for the redshift uncertainties not to contribute appreciably to the dark energy parameter error budget. For the SNAP supernova experiment, we find that, without the assistance of ground-based measurements, individual supernova redshifts would need to be determined to about 0.002 or better, which is a challenging but feasible requirement for a low-resolution spectrograph. However, we find that accurate redshifts for z<0.1 supernovae, obtained with ground-based experiments, are sufficient to immunize the results against even relatively large redshift errors at high z. For the future cluster number-count surveys such as the South Pole Telescope, Planck or DUET, we find that the purely statistical error in photometric redshift is less important, and that the irreducible, systematic bias in redshift drives the requirements. The redshift bias will have to be kept below 0.001-0.005 per redshift bin (which is determined by the filter set), depending on the sky coverage and details of the definition of the minimal mass of the survey. Furthermore, we find that X-ray surveys have a more stringent required redshift accuracy than Sunyaev-Zeldovich (SZ) effect surveys since they use a shorter lever arm in redshift; conversely, SZ surveys benefit from their high redshift reach only so long as some redshift information is available for distant (z>1) clusters.',\n",
       " 'Transneptunian objects (TNOs) are a source of invaluable information to access the history and evolution of the outer solar system. However, observing these faint objects is a difficult task. As a consequence, important properties such as size and albedo are known for only a small fraction of them. Now, with the results from deep sky surveys and the Gaia space mission, a new exciting era is within reach as accurate predictions of stellar occultations by numerous distant small solar system bodies become available. From them, diameters with kilometer accuracies can be determined. Albedos, in turn, can be obtained from diameters and absolute magnitudes. We use observations from the Dark Energy Survey (DES) from November 2012 until February 2016, amounting to 4292847 CCD frames. We searched them for all known small solar system bodies and recovered a total of 202 TNOs and Centaurs, 63 of which have been discovered by the DES collaboration until the date of this writing. Their positions were determined using the Gaia Data Release 2 as reference and their orbits were refined. Stellar occultations were then predicted using these refined orbits plus stellar positions from Gaia. These predictions are maintained, and updated, in a dedicated web service. The techniques developed here are also part of an ambitious preparation to use the data from the Large Synoptic Survey Telescope (LSST), that expects to obtain accurate positions and multifilter photometry for tens of thousands of TNOs.',\n",
       " 'Recently there has been an explosion of books and articles complaining about the weirdness of Quantum Mechanics and crying out for a solution. Three problems in particular have been singled out: the double-slit experiment, the measurement problem, and entanglement. One of these (entanglement) was the subject of an episode of the BBC TV show NOVA. In this article it is shown that Quantum Field Theory, as formulated by Julian Schwinger, provides simple solutions for all three problems, and others as well.',\n",
       " 'Energy carrier transport and recombination in emerging semiconductors can be directly monitored with optical microscopy, leading to the measurement of the diffusion coefficient (D), a critical property for the design of efficient optoelectronic devices. The diffusion coefficient is often determine by fitting a time-resolved expanding carrier profile after optical excitation using a Mean Squared Displacement Model. Although this approach has gained widespread adoption, there is no clear consensus as to which material systems, and under what experimental conditions (i.e. laser fluence), it can be applied. Here, we simulate diffusion processes in both excitonic and free carrier semiconductors and present a tailorable MSD Model that can accurately predict D for various materials. For perovskites, it is most accurate when sample D values are > 1 cm$^2$ s$^-$$^1$ and excitation densities are < 1x10$^1$$^6$ cm$^-$$^3$. At higher carrier densities, non-linear recombination terms can dominate the expansion of the carrier profile and lead to fitting error. We find that differences in grain size and boundary behavior, present in most thin films, can lead to distinct profiles that are not captured by MSD Models. Finally, we present clear strategies to investigate and control energy transport in disordered materials for more effective design and optimization of electronic and optoelectronic devices.',\n",
       " \"Recent discoveries of broad classes of quantum materials have spurred fundamental study of what quantum phases can be reached and stabilized, and have suggested intriguing practical applications based on control over transitions between quantum phases with different electrical, magnetic, and$/$or optical properties. Tabletop generation of strong terahertz (THz) light fields has set the stage for dramatic advances in our ability to drive quantum materials into novel states that do not exist as equilibrium phases. However, THz-driven irreversible phase transitions are still unexplored. Large and doping-tunable energy barriers between multiple phases in two-dimensional transition metal dichalcogenides (2D TMDs) provide a testbed for THz polymorph engineering. Here we report experimental demonstration of an irreversible phase transition in 2D MoTe$_{2}$ from a semiconducting hexagonal phase (2H) to a predicted topological insulator distorted octahedral ($1T^{'}$) phase induced by field-enhanced terahertz pulses. This is achieved by THz field-induced carrier liberation and multiplication processes that result in a transient high carrier density that favors the $1T^{'}$ phase. Single-shot time-resolved second harmonic generation (SHG) measurements following THz excitation reveal that the transition out of the 2H phase occurs within 10 ns. This observation opens up new possibilities of THz-based phase patterning and has implications for ultrafast THz control over quantum phases in two-dimensional materials.\",\n",
       " 'Lead halide-based perovskite thin films have attracted great attention due to the explosive increase in perovskite solar cell efficiencies. The same optoelectronic properties that make perovskites ideal absorber materials in solar cells are also beneficial in other light-harvesting applications and make them prime candidates as triplet sensitizers in upconversion via triplet-triplet annihilation in rubrene. In this contribution, we take advantage of long carrier lifetimes and carrier diffusion lengths in perovskite thin films, their high absorption cross sections throughout the visible spectrum, as well as the strong spin-orbit coupling owing to the abundance of heavy atoms to sensitize the upconverter rubrene. Employing bulk perovskite thin films as the absorber layer and spin-mixer in inorganic/organic heterojunction upconversion devices allows us to forego the additional tunneling barrier owing from the passivating ligands required for colloidal sensitizers. Our bilayer device exhibits an upconversion efficiency in excess of 3% under 785 nm illumination.',\n",
       " 'Photon recycling is required for a solar cell to achieve an open-circuit voltage ($V_{OC}$) and power conversion efficiency (PCE) approaching the Shockley-Queisser theoretical limit. In metal halide perovskite solar cells, the achievable performance gains from photon recycling remain uncertain due to high variability in perovskite material quality and the non-radiative recombination rate ($k_{1}$). In this work, we study state-of-the-art $\\\\textrm{Cs}_{0.05}(\\\\textrm{MA}_{0.17}\\\\textrm{FA}_{0.83})_{0.95}\\\\textrm{Pb}(\\\\textrm{I}_{0.83}\\\\textrm{Br}_{0.17})_{3}$ films and analyze the impact of varying non-radiative recombination rates on photon recycling and device performance. Importantly, we predict the impact of photon recycling at the maximum power point (MPP), demonstrating an absolute PCE increase of up to 2.0% in the radiative limit, primarily due to a 77 mV increase in $V_{MPP}$. Even with finite non-radiative recombination, benefits from photon recycling can be achieved when non-radiative lifetimes and external LED electroluminescence efficiencies measured at open-circuit, $Q_{e}^{LED}(\\\\textrm{V}_{OC})$, exceed 2 $Î¼$s and 10%, respectively. This analysis clarifies the opportunity to fully exploit photon recycling to push the real-world performance of perovskite solar cells toward theoretical limits.',\n",
       " 'Halide perovskites are promising semiconductors for inexpensive, high-performance optoelectronics. Despite a remarkable defect tolerance compared to conventional semiconductors, perovskite thin films still show substantial microscale heterogeneity in key properties such as luminescence efficiency and device performance. This behavior has been attributed to spatial fluctuations in the population of sub-bandgap electronic states that act as trap-mediated non-radiative recombination sites. However, the origin of the variations, trap states and extent of the defect tolerance remains a topic of debate, and a precise understanding is critical to the rational design of defect management strategies. By combining scanning X-ray diffraction beamlines at two different synchrotrons with high-resolution transmission electron microscopy, we reveal levels of heterogeneity on the ten-micrometer scale (super-grains) and even ten-nanometer scale (sub-grain domains). We find that local strain is associated with enhanced defect concentrations, and correlations between the local structure and time-resolved photoluminescence reveal that these strain-related defects are the cause of non-radiative recombination. We reveal a direct connection between defect concentrations and non-radiative losses, as well as complex heterogeneity across multiple length scales, shedding new light on the presence and influence of structural defects in halide perovskites.',\n",
       " 'Unique optical properties of colloidal semiconductor quantum dots (QDs), arising from quantum mechanical confinement of charge within these structures, present a versatile testbed for the study of how high electric fields affect the electronic structure of nanostructured solids. Earlier studies of quasi-DC electric field modulation of QD properties have been limited by the electrostatic breakdown processes under the high externally applied electric fields, which have restricted the range of modulation of QD properties. In contrast, in the present work we drive CdSe:CdS core:shell QD films with high-field THz-frequency electromagnetic pulses whose duration is only a few picoseconds. Surprisingly, in response to the THz excitation we observe QD luminescence even in the absence of an external charge source. Our experiments show that QD luminescence is associated with a remarkably high and rapid modulation of the QD band-gap, which is changing by more than 0.5 eV (corresponding to 25% of the unperturbed bandgap energy) within the picosecond timeframe of THz field profile. We show that these colossal energy shifts can be consistently explained by the quantum confined Stark effect. Our work demonstrates a route to extreme modulation of material properties without configurational changes in material sets or geometries. Additionally, we expect that this platform can be adapted to a novel compact THz detection scheme where conversion of THz fields (with meV-scale photon energies) to the visible/near-IR band (with eV-scale photon energies) can be achieved at room temperature with high bandwidth and sensitivity.',\n",
       " 'Plexcitons are polaritonic modes that result from the strong coupling between excitons and plasmons. We consider plexcitons emerging from the interaction of excitons in an organic molecular layer with surface plasmons in a metallic film. We predict the emergence of Dirac cones in the two-dimensional bandstructure of plexcitons due to the inherent alignment of the excitonic transitions in the organic layer. These Dirac cones may open up in energy by simultaneously interfacing the metal with a magneto-optical layer and subjecting the whole system to a perpendicular magnetic field. The resulting energy gap becomes populated with topologically protected one-way modes which travel at the interface of this plexcitonic system. Our theoretical proposal suggests that plexcitons are a convenient and simple platform for the exploration of exotic phases of matter as well as of novel ways to direct energy flow at the nanoscale.',\n",
       " 'Many neural network pruning algorithms proceed in three steps: train the network to completion, remove unwanted structure to compress the network, and retrain the remaining structure to recover lost accuracy. The standard retraining technique, fine-tuning, trains the unpruned weights from their final trained values using a small fixed learning rate. In this paper, we compare fine-tuning to alternative retraining techniques. Weight rewinding (as proposed by Frankle et al., (2019)), rewinds unpruned weights to their values from earlier in training and retrains them from there using the original training schedule. Learning rate rewinding (which we propose) trains the unpruned weights from their final values using the same learning rate schedule as weight rewinding. Both rewinding techniques outperform fine-tuning, forming the basis of a network-agnostic pruning algorithm that matches the accuracy and compression ratios of several more network-specific state-of-the-art techniques.',\n",
       " 'We introduce \"instability analysis,\" which assesses whether a neural network optimizes to the same, linearly connected minimum under different samples of SGD noise. We find that standard vision models become \"stable\" in this way early in training. From then on, the outcome of optimization is determined to within a linearly connected region. We use instability to study \"iterative magnitude pruning\" (IMP), the procedure used by work on the lottery ticket hypothesis to identify subnetworks that could have trained to full accuracy from initialization. We find that these subnetworks only reach full accuracy when they are stable, which either occurs at initialization for small-scale settings (MNIST) or early in training for large-scale settings (Resnet-50 and Inception-v3 on ImageNet).\\n  This submission subsumes 1903.01611 (\"Stabilizing the Lottery Ticket Hypothesis\" and \"The Lottery Ticket Hypothesis at Scale\")',\n",
       " \"Synchronous modeling is at the heart of programming languages like Lustre, Esterel, or Scade used routinely for implementing safety critical control software, e.g., fly-by-wire and engine control in planes. However, to date these languages have had limited modern support for modeling uncertainty -- probabilistic aspects of software's environment or behavior -- even though modeling uncertainty is a primary activity when designing a control system.\\n  In this paper we present ProbZelus the first synchronous probabilistic programming language. ProbZelus conservatively provides the facilities of a synchronous language to write control software, with probabilistic constructs to model uncertainties and perform inference-in-the-loop.\\n  We present the design and implementation of the language. We propose a measure-theoretic semantics of probabilistic stream functions and a simple type discipline to separate deterministic and probabilistic expressions. We demonstrate a semantics-preserving compilation into a first-order functional language that lends itself to a simple presentation of inference algorithms for streaming models. We also redesign the delayed sampling inference algorithm to provide efficient streaming inference. Together with an evaluation on several reactive applications, our results demonstrate that ProbZelus enables the design of reactive probabilistic applications and efficient, bounded memory inference.\",\n",
       " 'Pruning is a well-established technique for removing unnecessary structure from neural networks after training to improve the performance of inference. Several recent results have explored the possibility of pruning at initialization time to provide similar benefits during training. In particular, the \"lottery ticket hypothesis\" conjectures that typical neural networks contain small subnetworks that can train to similar accuracy in a commensurate number of steps. The evidence for this claim is that a procedure based on iterative magnitude pruning (IMP) reliably finds such subnetworks retroactively on small vision tasks. However, IMP fails on deeper networks, and proposed methods to prune before training or train pruned networks encounter similar scaling limitations.\\n  In this paper, we argue that these efforts have struggled on deeper networks because they have focused on pruning precisely at initialization. We modify IMP to search for subnetworks that could have been obtained by pruning early in training (0.1% to 7% through) rather than at iteration 0. With this change, it finds small subnetworks of deeper networks (e.g., 80% sparsity on Resnet-50) that can complete the training process to match the accuracy of the original network on more challenging tasks (e.g., ImageNet). In situations where IMP fails at iteration 0, the accuracy benefits of delaying pruning accrue rapidly over the earliest iterations of training. To explain these behaviors, we study subnetwork \"stability,\" finding that - as accuracy improves in this fashion - IMP subnetworks train to parameters closer to those of the full network and do so with improved consistency in the face of gradient noise. These results offer new insights into the opportunity to prune large-scale networks early in training and the behaviors underlying the lottery ticket hypothesis.',\n",
       " 'When a computational task tolerates a relaxation of its specification or when an algorithm tolerates the effects of noise in its execution, hardware, programming languages, and system software can trade deviations from correct behavior for lower resource usage. We present, for the first time, a synthesis of research results on computing systems that only make as many errors as their users can tolerate, from across the disciplines of computer aided design of circuits, digital system design, computer architecture, programming languages, operating systems, and information theory.\\n  Rather than over-provisioning resources at each layer to avoid errors, it can be more efficient to exploit the masking of errors occurring at one layer which can prevent them from propagating to a higher layer. We survey tradeoffs for individual layers of computing systems from the circuit level to the operating system level and illustrate the potential benefits of end-to-end approaches using two illustrative examples. To tie together the survey, we present a consistent formalization of terminology, across the layers, which does not significantly deviate from the terminology traditionally used by research communities in their layer of focus.',\n",
       " \"Predicting the number of clock cycles a processor takes to execute a block of assembly instructions in steady state (the throughput) is important for both compiler designers and performance engineers. Building an analytical model to do so is especially complicated in modern x86-64 Complex Instruction Set Computer (CISC) machines with sophisticated processor microarchitectures in that it is tedious, error prone, and must be performed from scratch for each processor generation. In this paper we present Ithemal, the first tool which learns to predict the throughput of a set of instructions. Ithemal uses a hierarchical LSTM--based approach to predict throughput based on the opcodes and operands of instructions in a basic block. We show that Ithemal is more accurate than state-of-the-art hand-written tools currently used in compiler backends and static machine code analyzers. In particular, our model has less than half the error of state-of-the-art analytical models (LLVM's llvm-mca and Intel's IACA). Ithemal is also able to predict these throughput values just as fast as the aforementioned tools, and is easily ported across a variety of processor microarchitectures with minimal developer effort.\",\n",
       " \"Researchers have recently designed a number of application-specific fault tolerance mechanisms that enable applications to either be naturally resilient to errors or include additional detection and correction steps that can bring the overall execution of an application back into an envelope for which an acceptable execution is eventually guaranteed. A major challenge to building an application that leverages these mechanisms, however, is to verify that the implementation satisfies the basic invariants that these mechanisms require--given a model of how faults may manifest during the application's execution.\\n  To this end we present Leto, an SMT based automatic verification system that enables developers to verify their applications with respect to a first-class execution model specification. Namely, Leto enables software and platform developers to programmatically specify the execution semantics of the underlying hardware system as well as verify assertions about the behavior of the application's resulting execution. In this paper, we present the Leto programming language and its corresponding verification system. We also demonstrate Leto on several applications that leverage application-specific fault tolerance mechanisms.\",\n",
       " \"Researchers have recently proposed several systems that ease the process of performing Bayesian probabilistic inference. These include systems for automatic inference algorithm synthesis as well as stronger abstractions for manual algorithm development. However, existing systems whose performance relies on the developer manually constructing a part of the inference algorithm have limited support for reasoning about the correctness of the resulting algorithm.\\n  In this paper, we present Shuffle, a programming language for manually developing inference procedures that 1) enforces the basic rules of probability theory, 2) enforces the statistical dependencies of the algorithm's corresponding probabilistic model, and 3) generates an optimized implementation. We have used Shuffle to develop inference algorithms for several standard probabilistic models. Our results demonstrate that Shuffle enables a developer to deliver correct and performant implementations of these algorithms.\",\n",
       " 'Though many safety-critical software systems use floating point to represent real-world input and output, programmers usually have idealized versions in mind that compute with real numbers. Significant deviations from the ideal can cause errors and jeopardize safety. Some programming systems implement exact real arithmetic, which resolves this matter but complicates others, such as decision making. In these systems, it is impossible to compute (total and deterministic) discrete decisions based on connected spaces such as $\\\\mathbb{R}$. We present programming-language semantics based on constructive topology with variants allowing nondeterminism and/or partiality. Either nondeterminism or partiality suffices to allow computable decision making on connected spaces such as $\\\\mathbb{R}$. We then introduce pattern matching on spaces, a language construct for creating programs on spaces, generalizing pattern matching in functional programming, where patterns need not represent decidable predicates and also may overlap or be inexhaustive, giving rise to nondeterminism or partiality, respectively. Nondeterminism and/or partiality also yield formal logics for constructing approximate decision procedures. We implemented these constructs in the Marshall language for exact real arithmetic.',\n",
       " 'In this position paper, we describe our vision of the future of machine programming through a categorical examination of three pillars of research. Those pillars are: (i) intention, (ii) invention, and(iii) adaptation. Intention emphasizes advancements in the human-to-computer and computer-to-machine-learning interfaces. Invention emphasizes the creation or refinement of algorithms or core hardware and software building blocks through machine learning (ML). Adaptation emphasizes advances in the use of ML-based constructs to autonomously evolve software.',\n",
       " 'Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance.\\n  We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the \"lottery ticket hypothesis:\" dense, randomly-initialized, feed-forward networks contain subnetworks (\"winning tickets\") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective.\\n  We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.',\n",
       " 'We propose a novel approach to improving software security called Cryptographic Path Hardening, which is aimed at hiding security vulnerabilities in software from attackers through the use of provably secure and obfuscated cryptographic devices to harden paths in programs.\\n  By \"harden\" we mean that certain error-checking if-conditionals in a given program P are replaced by equivalent\" we mean that adversaries cannot use semi-automatic program analysis techniques to reason about the hardened program paths and thus cannot discover as-yet-unknown errors along those paths, except perhaps through black-box dictionary attacks or random testing (which we can never prevent).\\n  Other than these unpreventable attack methods, we can make program analysis aimed at error-finding \"provably hard\" for a resource-bounded attacker, in the same sense that cryptographic schemes are hard to break. Unlike security-through-obscurity, in Cryptographic Path Hardening we use provably-secure crypto devices to hide errors and our mathematical arguments of security are the same as the standard ones used in cryptography.\\n  One application of Cryptographic Path Hardening is that software patches or filters often reveal enough information to an attacker that they can be used to construct error-revealing inputs to exploit an unpatched version of the program. By \"hardening\" the patch we make it difficult for the attacker to analyze the patched program to construct error-revealing inputs, and thus prevent him from potentially constructing exploits.',\n",
       " 'Intelligent reflecting surfaces (IRSs) are envisioned to provide reconfigurable wireless environments for future communication networks. In this paper, both downlink and uplink IRS-aided non-orthogonal multiple access (NOMA) and orthogonal multiple access (OMA) networks are studied, in which an IRS is deployed to enhance the coverage by assisting a cell-edge user device (UD) to communicate with the base station (BS). To characterize system performance, new channel statistics for the BS-IRS-UD link with Nakagami-$m$ fading are investigated. For each scenario, closed-form expressions for the outage probability and the ergodic rate are derived. To gain further insight, the diversity order and the high signal-to-noise ratio (SNR) slope for each scenario are obtained according to asymptotic approximations in the high-SNR regime. It is demonstrated that the diversity order is affected by the number of IRS elements and Nakagami-$m$ fading parameters, but the high-SNR slope is not related to these parameters. Simulation results validate our analysis and reveal the superiority of the IRS over the full-duplex decode-and-forward relay.',\n",
       " \"Given AI's growing role in modeling and improving decision-making, how and when to present users with feedback is an urgent topic to address. We empirically examined the effect of feedback from false AI on moral decision-making about donor kidney allocation. We found some evidence that judgments about whether a patient should receive a kidney can be influenced by feedback about participants' own decision-making perceived to be given by AI, even if the feedback is entirely random. We also discovered different effects between assessments presented as being from human experts and assessments presented as being from AI.\",\n",
       " \"In April 2019, the Event Horizon Telescope (EHT) collaboration revealed the first image of the candidate super-massive black hole (SMBH) at the centre of the giant elliptical galaxy Messier 87 (M87). This event-horizon-scale image shows a ring of glowing plasma with a dark patch at the centre, which is interpreted as the shadow of the black hole. This breakthrough result, which represents a powerful confirmation of Einstein's theory of gravity, or general relativity, was made possible by assembling a global network of radio telescopes operating at millimetre wavelengths that for the first time included the Atacama Large Millimeter/ submillimeter Array (ALMA). The addition of ALMA as an anchor station has enabled a giant leap forward by increasing the sensitivity limits of the EHT by an order of magnitude, effectively turning it into an imaging array. The published image demonstrates that it is now possible to directly study the event horizon shadows of SMBHs via electromagnetic radiation, thereby transforming this elusive frontier from a mathematical concept into an astrophysical reality. The expansion of the array over the next few years will include new stations on different continents - and eventually satellites in space. This will provide progressively sharper and higher-fidelity images of SMBH candidates, and potentially even movies of the hot plasma orbiting around SMBHs. These improvements will shed light on the processes of black hole accretion and jet formation on event-horizon scales, thereby enabling more precise tests of general relativity in the truly strong field regime.\",\n",
       " 'The prosodic aspects of speech signals produced by current text-to-speech systems are typically averaged over training material, and as such lack the variety and liveliness found in natural speech. To avoid monotony and averaged prosody contours, it is desirable to have a way of modeling the variation in the prosodic aspects of speech, so audio signals can be synthesized in multiple ways for a given text. We present a new, hierarchically structured conditional variational autoencoder to generate prosodic features (fundamental frequency, energy and duration) suitable for use with a vocoder or a generative model like WaveNet. At inference time, an embedding representing the prosody of a sentence may be sampled from the variational layer to allow for prosodic variation. To efficiently capture the hierarchical nature of the linguistic input (words, syllables and phones), both the encoder and decoder parts of the auto-encoder are hierarchical, in line with the linguistic structure, with layers being clocked dynamically at the respective rates. We show in our experiments that our dynamic hierarchical network outperforms a non-hierarchical state-of-the-art baseline, and, additionally, that prosody transfer across sentences is possible by employing the prosody embedding of one sentence to generate the speech signal of another.',\n",
       " 'We present the design and implementation of an automated data calibration and reduction pipeline for very-long-baseline interferometric (VLBI) observations taken at millimeter wavelengths. These short radio-wavelengths provide the best imaging resolution available from ground-based VLBI networks such as the Event Horizon Telescope (EHT) and the Global Millimeter VLBI Array (GMVA), but require specialized processing due to the strong effects from atmospheric opacity and turbulence as well as the heterogeneous nature of existing global arrays. The pipeline builds upon a calibration suite (HOPS) originally designed for precision geodetic VLBI. To support the reduction of data for astronomical observations, we have developed an additional framework for global phase and amplitude calibration which provides output in a standard data format for astronomical imaging and analysis. We test the pipeline on observations taken at 3.5 mm (86 GHz) by the GMVA joined by the phased Atacama Large Millimeter/submillimeter Array in April 2017, and demonstrate the benefits from the specialized processing of high frequency VLBI data with respect to classical analysis techniques.',\n",
       " 'Multiple systems estimation strategies have recently been applied to quantify hard-to-reach populations, particularly when estimating the number of victims of human trafficking and modern slavery. In such contexts, it is not uncommon to see sparse or even no overlap between some of the lists on which the estimates are based. These create difficulties in model fitting and selection, and we develop inference procedures to address these challenges. The approach is based on Poisson log-linear regression modeling. Issues investigated in detail include taking proper account of data sparsity in the estimation procedure, as well as the existence and identifiability of maximum likelihood estimates. A stepwise method for choosing the most suitable parameters is developed, together with a bootstrap approach to finding confidence intervals for the total population size. We apply the strategy to two empirical data sets of trafficking in US regions, and find that the approach results in stable, reasonable estimates. An accompanying R software implementation has been made publicly available.',\n",
       " 'Shot noise is an important ingredient to any measurement or theoretical modeling of discrete tracers of the large scale structure. Recent work has shown that the shot noise in the halo power spectrum becomes increasingly sub-Poissonian at high mass. Interestingly, while the halo model predicts a shot noise power spectrum in qualitative agreement with the data, it leads to an unphysical white noise in the cross halo-matter and matter power spectrum. In this work, we show that absorbing all the halo model sources of shot noise into the halo fluctuation field leads to meaningful predictions for the shot noise contributions to halo clustering statistics and remove the unphysical white noise from the cross halo-matter statistics. Our prescription straightforwardly maps onto the general bias expansion, so that the renormalized shot noise terms can be expressed as combinations of the halo model shot noises. Furthermore, we demonstrate that non-Poissonian contributions are related to volume integrals over correlation functions and their response to long-wavelength density perturbations. This leads to a new class of consistency relations for discrete tracers, which appear to be satisfied by our reformulation of the halo model. We test our theoretical predictions against measurements of halo shot noise bispectra extracted from a large suite of numerical simulations. Our model reproduces qualitatively the observed sub-Poissonian noise, although it underestimates the magnitude of this effect.',\n",
       " 'Regular medical records are useful for medical practitioners to analyze and monitor patient health status especially for those with chronic disease, but such records are usually incomplete due to unpunctuality and absence of patients. In order to resolve the missing data problem over time, tensor-based model is suggested for missing data imputation in recent papers because this approach makes use of low rank tensor assumption for highly correlated data. However, when the time intervals between records are long, the data correlation is not high along temporal direction and such assumption is not valid. To address this problem, we propose to decompose a matrix with missing data into its latent factors. Then, the locally linear constraint is imposed on these factors for matrix completion in this paper. By using a publicly available dataset and two medical datasets collected from hospital, experimental results show that the proposed algorithm achieves the best performance by comparing with the existing methods.',\n",
       " 'The Galactic Center black hole Sagittarius A* (Sgr A*) is a prime observing target for the Event Horizon Telescope (EHT), which can resolve the 1.3 mm emission from this source on angular scales comparable to that of the general relativistic shadow. Previous EHT observations have used visibility amplitudes to infer the morphology of the millimeter-wavelength emission. Potentially much richer source information is contained in the phases. We report on 1.3 mm phase information on Sgr A* obtained with the EHT on a total of 13 observing nights over 4 years. Closure phases, the sum of visibility phases along a closed triangle of interferometer baselines, are used because they are robust against phase corruptions introduced by instrumentation and the rapidly variable atmosphere. The median closure phase on a triangle including telescopes in California, Hawaii, and Arizona is nonzero. This result conclusively demonstrates that the millimeter emission is asymmetric on scales of a few Schwarzschild radii and can be used to break 180-degree rotational ambiguities inherent from amplitude data alone. The stability of the sign of the closure phase over most observing nights indicates persistent asymmetry in the image of Sgr A* that is not obscured by refraction due to interstellar electrons along the line of sight.',\n",
       " 'We study the halo-matter cross bispectrum in the presence of primordial non-Gaussianity of the local type. We restrict ourselves to the squeezed limit, for which the calculation are straightforward, and perform the measurements in the initial conditions of N-body simulations, to mitigate the contamination induced by nonlinear gravitational evolution. Interestingly, the halo-matter cross bispectrum is not trivial even in this simple limit as it is strongly sensitive to the scale-dependence of the quadratic and third-order halo bias. Therefore, it can be used to test biasing prescriptions. We consider three different prescription for halo clustering: excursion set peaks (ESP), local bias and a model in which the halo bias parameters are explicitly derived from a peak-background split. In all cases, the model parameters are fully constrained with statistics other than the cross bispectrum. We measure the cross bispectrum involving one halo fluctuation field and two mass overdensity fields for various halo masses and collapse redshifts. We find that the ESP is in reasonably good agreement with the numerical data, while the other alternatives we consider fail in various cases. This suggests that the scale-dependence of halo bias also is a crucial ingredient to the squeezed limit of the halo bispectrum.',\n",
       " 'We study the clustering of voids using $N$-body simulations and simple theoretical models. The excursion-set formalism describes fairly well the abundance of voids identified with the watershed algorithm, although the void formation threshold required is quite different from the spherical collapse value. The void cross bias $b_{\\\\rm c} $ is measured and its large-scale value is found to be consistent with the peak background split results. A simple fitting formula for $b_{\\\\rm c} $ is found. We model the void auto-power spectrum taking into account the void biasing and exclusion effect. A good fit to the simulation data is obtained for voids with radii $\\\\gtrsim$ 30 Mpc/$h$, especially when the void biasing model is extended to 1-loop order. However, the best-fit bias parameters do not agree well with the peak-background split results. Being able to fit the void auto-power spectrum is particularly important not only because it is the direct observable in galaxy surveys, but also our method enables us to treat the bias parameters as nuisance parameters, which are sensitive to the techniques used to identify voids.',\n",
       " 'We investigate nonlocal Lagrangian bias contributions involving gradients of the linear density field, for which we have predictions from the excursion set peak formalism. We begin by writing down a bias expansion which includes all the bias terms, including the nonlocal ones. Having checked that the model furnishes a reasonable fit to the halo mass function, we develop a 1-point cross-correlation technique to measure bias factors associated with 2-distributed quantities. We validate the method with numerical realizations of peaks of Gaussian random fields before we apply it to N-body simulations. We focus on the lowest (quadratic) order nonlocal contributions. We can reproduce our measurement of Ï‡_{10} if we allow for an offset between the Lagrangian halo center-of-mass and the peak position. The sign and magnitude of Ï‡_{10} is consistent with Lagrangian haloes sitting near linear density maxima. The resulting contribution to the halo bias can safely be ignored for M = 10^13 Msun/h, but could become relevant at larger halo masses. For the second nonlocal bias Ï‡_{01} however, we measure a much larger magnitude than predicted by our model. We speculate that some of this discrepancy might originate from nonlocal Lagrangian contributions induced by nonspherical collapse.',\n",
       " 'Let $E \\\\subseteq R^n$ be a closed set of Hausdorff dimension $Î±$. For $m \\\\geq n$, let $\\\\{B_1,\\\\ldots,B_k\\\\}$ be $n \\\\times (m-n)$ matrices. We prove that if the system of matrices $B_j$ is non-degenerate in a suitable sense, $Î±$ is sufficiently close to $n$, and if $E$ supports a probability measure obeying appropriate dimensionality and Fourier decay conditions, then for a range of $m$ depending on $n$ and $k$, the set $E$ contains a translate of a non-trivial $k$-point configuration $\\\\{B_1y,\\\\ldots,B_ky\\\\}$. As a consequence, we are able to establish existence of certain geometric configurations in Salem sets (such as parallelograms in $ R^n$ and isosceles right triangles in $R^2$). This can be viewed as a multidimensional analogue of an earlier result of Laba and Pramanik on 3-term arithmetic progressions in subsets of $R$.',\n",
       " 'We explore the scale dependence of halo bias using real space cross-correlation measurements in N-body simulations and in Pinocchio, an algorithm based on Lagrangian Perturbation Theory. Recent work has shown how to interpret such real space measurements in terms of k-dependent bias in Fourier space, and how to remove the k-dependence to reconstruct the k-independent peak-background split halo bias parameters. We compare our reconstruction of the linear bias, which requires no free parameters, with previous estimates from N-body simulations which were obtained directly in Fourier space at large scales, and find very good agreement. Our reconstruction of the quadratic bias is similarly parameter-free, although in this case there are no previous Fourier space measurements to compare with. Our analysis of N-body simulations explicitly tests the predictions of the excursion set peaks (ESP) formalism of Paranjape et al. (2013) for the scale dependence of bias; we find that the ESP predictions accurately describe our measurements. In addition, our measurements in Pinocchio serve as a useful, successful consistency check between Pinocchio and N-body simulations that is not accessible to traditional measurements.',\n",
       " 'The Orange \"Data for Development\" (D4D) challenge is an open data challenge on anonymous call patterns of Orange\\'s mobile phone users in Ivory Coast. The goal of the challenge is to help address society development questions in novel ways by contributing to the socio-economic development and well-being of the Ivory Coast population. Participants to the challenge are given access to four mobile phone datasets and the purpose of this paper is to describe the four datasets. The website http://www.d4d.orange.com contains more information about the participation rules. The datasets are based on anonymized Call Detail Records (CDR) of phone calls and SMS exchanges between five million of Orange\\'s customers in Ivory Coast between December 1, 2011 and April 28, 2012. The datasets are: (a) antenna-to-antenna traffic on an hourly basis, (b) individual trajectories for 50,000 customers for two week time windows with antenna location information, (3) individual trajectories for 500,000 customers over the entire observation period with sub-prefecture location information, and (4) a sample of communication graphs for 5,000 customers',\n",
       " 'We study the spindown of isolated neutron stars from initially rapid rotation rates, driven by two factors: (i) gravitational wave emission due to r-modes and (ii) magnetic braking. In the context of isolated neutron stars, we present the first study including self-consistently the magnetic damping of r-modes in the spin evolution. We track the spin evolution employing the RNS code, which accounts for the rotating structure of neutron stars for various equations of state. We find that, despite the strong damping due to the magnetic field, r-modes alter the braking rate from pure magnetic braking for B<10^{13}G. For realistic values of the saturation amplitude, the r-mode can also decrease the time to reach the threshold central density for quark deconfinement. Within a phenomenological model, we assess the gravitational waveform that would result from r-mode driven spindown of a magnetized neutron star. To contrast with the persistent signal during the spindown phase, we also present a preliminary estimate of the transient gravitational wave signal from an explosive quark-hadron phase transition, which can be a signal for the deconfinement of quarks inside neutron stars.',\n",
       " '  Due to hardware and computational constraints, wireless sensor networks (WSNs) normally do not take measurements of time-of-arrival or time-difference-of-arrival for rangebased localization. Instead, WSNs in some applications use rangefree localization for simple but less accurate determination of sensor positions. A well-known algorithm for this purpose is the centroid algorithm. This paper presents a range-free localization technique based on the radical line of intersecting circles. This technique provides greater accuracy than the centroid algorithm, at the expense of a slight increase in computational load. Simulation results show that for the scenarios studied, the radical line method can give an approximately 2 to 30% increase in accuracy over the centroid algorithm, depending on whether or not the anchors have identical ranges, and on the value of DOI.',\n",
       " \"This paper presents a dual-factor authentication protocol and its low-power implementation for security of implantable medical devices (IMDs). The protocol incorporates traditional cryptographic first-factor authentication using Datagram Transport Layer Security - Pre-Shared Key (DTLS-PSK) followed by the user's touch-based voluntary second-factor authentication for enhanced security. With a low-power compact always-on wake-up timer and touch-based wake-up circuitry, our test chip consumes only 735 pW idle state power at 20.15 Hz and 2.5 V. The hardware accelerated dual-factor authentication unit consumes 8 $Î¼$W at 660 kHz and 0.87 V. Our test chip was coupled with commercial Bluetooth Low Energy (BLE) transceiver, DC-DC converter, touch sensor and coin cell battery to demonstrate standalone implantable operation and also tested using in-vitro measurement setup.\",\n",
       " \"Public key cryptography protocols, such as RSA and elliptic curve cryptography, will be rendered insecure by Shor's algorithm when large-scale quantum computers are built. Cryptographers are working on quantum-resistant algorithms, and lattice-based cryptography has emerged as a prime candidate. However, high computational complexity of these algorithms makes it challenging to implement lattice-based protocols on low-power embedded devices. To address this challenge, we present Sapphire - a lattice cryptography processor with configurable parameters. Efficient sampling, with a SHA-3-based PRNG, provides two orders of magnitude energy savings; a single-port RAM-based number theoretic transform memory architecture is proposed, which provides 124k-gate area savings; while a low-power modular arithmetic unit accelerates polynomial computations. Our test chip was fabricated in TSMC 40nm low-power CMOS process, with the Sapphire cryptographic core occupying 0.28 mm2 area consisting of 106k logic gates and 40.25 KB SRAM. Sapphire can be programmed with custom instructions for polynomial arithmetic and sampling, and it is coupled with a low-power RISC-V micro-processor to demonstrate NIST Round 2 lattice-based CCA-secure key encapsulation and signature protocols Frodo, NewHope, qTESLA, CRYSTALS-Kyber and CRYSTALS-Dilithium, achieving up to an order of magnitude improvement in performance and energy-efficiency compared to state-of-the-art hardware implementations. All key building blocks of Sapphire are constant-time and secure against timing and simple power analysis side-channel attacks. We also discuss how masking-based DPA countermeasures can be implemented on the Sapphire core without any changes to the hardware.\",\n",
       " 'This paper presents the first hardware implementation of the Datagram Transport Layer Security (DTLS) protocol to enable end-to-end security for the Internet of Things (IoT). A key component of this design is a reconfigurable prime field elliptic curve cryptography (ECC) accelerator, which is 238x and 9x more energy-efficient compared to software and state-of-the-art hardware respectively. Our full hardware implementation of the DTLS 1.3 protocol provides 438x improvement in energy-efficiency over software, along with code size and data memory usage as low as 8 KB and 3 KB respectively. The cryptographic accelerators are coupled with an on-chip low-power RISC-V processor to benchmark applications beyond DTLS with up to two orders of magnitude energy savings. The test chip, fabricated in 65 nm CMOS, demonstrates hardware-accelerated DTLS sessions while consuming 44.08 uJ per handshake, and 0.89 nJ per byte of encrypted data at 16 MHz and 0.8 V.',\n",
       " 'This paper presents a configurable lattice cryptography processor which enables quantum-resistant security protocols for IoT. Efficient sampling architectures, coupled with a low-power SHA-3 core, provide two orders of magnitude energy savings over software. A single-port RAM-based NTT architecture is proposed, which provides ~124k-gate area savings. This is the first ASIC implementation which demonstrates multiple lattice-based protocols proposed for NIST post-quantum standardization.',\n",
       " 'This paper presents a reconfigurable cryptographic engine that implements the DTLS protocol to enable end-to-end security for IoT. This implementation of the DTLS engine demonstrates 10x reduction in code size and 438x improvement in energy-efficiency over software. Our ECC primitive is 237x and 9x more energy-efficient compared to software and state-of-the-art hardware respectively. Pairing the DTLS engine with an on-chip RISC-V allows us to demonstrate applications beyond DTLS with up to 2 orders of magnitude energy savings.',\n",
       " \"The growing popularity of cloud-based machine learning raises a natural question about the privacy guarantees that can be provided in such a setting. Our work tackles this problem in the context where a client wishes to classify private images using a convolutional neural network (CNN) trained by a server. Our goal is to build efficient protocols whereby the client can acquire the classification result without revealing their input to the server, while guaranteeing the privacy of the server's neural network.\\n  To this end, we design Gazelle, a scalable and low-latency system for secure neural network inference, using an intricate combination of homomorphic encryption and traditional two-party computation techniques (such as garbled circuits). Gazelle makes three contributions. First, we design the Gazelle homomorphic encryption library which provides fast algorithms for basic homomorphic operations such as SIMD (single instruction multiple data) addition, SIMD multiplication and ciphertext permutation. Second, we implement the Gazelle homomorphic linear algebra kernels which map neural network layers to optimized homomorphic matrix-vector multiplication and convolution routines. Third, we design optimized encryption switching protocols which seamlessly convert between homomorphic and garbled circuit encodings to enable implementation of complete neural network inference.\\n  We evaluate our protocols on benchmark neural networks trained on the MNIST and CIFAR-10 datasets and show that Gazelle outperforms the best existing systems such as MiniONN (ACM CCS 2017) by 20 times and Chameleon (Crypto Eprint 2017/1164) by 30 times in online runtime. Similarly when compared with fully homomorphic approaches like CryptoNets (ICML 2016) we demonstrate three orders of magnitude faster online run-time.\",\n",
       " '  Ultra-wideband (UWB) communication is an emerging wireless technology that promises high data rates over short distances and precise locationing. The large available bandwidth and the constraint of a maximum power spectral density drives a unique set of system challenges. This paper addresses these challenges using two UWB transceivers and a discrete prototype platform.',\n",
       " '  Wireless microsensor networks, which have been the topic of intensive research in recent years, are now emerging in industrial applications. An important milestone in this transition has been the release of the IEEE 802.15.4 standard that specifies interoperable wireless physical and medium access control layers targeted to sensor node radios. In this paper, we evaluate the potential of an 802.15.4 radio for use in an ultra low power sensor node operating in a dense network. Starting from measurements carried out on the off-the-shelf radio, effective radio activation and link adaptation policies are derived. It is shown that, in a typical sensor network scenario, the average power per node can be reduced down to 211m mm mW. Next, the energy consumption breakdown between the different phases of a packet transmission is presented, indicating which part of the transceiver architecture can most effectively be optimized in order to further reduce the radio power, enabling self-powered wireless microsensor networks.',\n",
       " 'Though many safety-critical software systems use floating point to represent real-world input and output, programmers usually have idealized versions in mind that compute with real numbers. Significant deviations from the ideal can cause errors and jeopardize safety. Some programming systems implement exact real arithmetic, which resolves this matter but complicates others, such as decision making. In these systems, it is impossible to compute (total and deterministic) discrete decisions based on connected spaces such as $\\\\mathbb{R}$. We present programming-language semantics based on constructive topology with variants allowing nondeterminism and/or partiality. Either nondeterminism or partiality suffices to allow computable decision making on connected spaces such as $\\\\mathbb{R}$. We then introduce pattern matching on spaces, a language construct for creating programs on spaces, generalizing pattern matching in functional programming, where patterns need not represent decidable predicates and also may overlap or be inexhaustive, giving rise to nondeterminism or partiality, respectively. Nondeterminism and/or partiality also yield formal logics for constructing approximate decision procedures. We implemented these constructs in the Marshall language for exact real arithmetic.',\n",
       " 'It is a neat result from functional programming that libraries of parser combinators can support rapid construction of decoders for quite a range of formats. With a little more work, the same combinator program can denote both a decoder and an encoder. Unfortunately, the real world is full of gnarly formats, as with the packet formats that make up the standard Internet protocol stack. Most past parser-combinator approaches cannot handle these formats, and the few exceptions require redundancy -- one part of the natural grammar needs to be hand-translated into hints in multiple parts of a parser program. We show how to recover very natural and nonredundant format specifications, covering all popular network packet formats and generating both decoders and encoders automatically. The catch is that we use the Coq proof assistant to derive both kinds of artifacts using tactics, automatically, in a way that guarantees that they form inverses of each other. We used our approach to reimplement packet processing for a full Internet protocol stack, inserting our replacement into the OCaml-based MirageOS unikernel, resulting in minimal performance degradation.',\n",
       " \"We describe our experience implementing a broad category-theory library in Coq. Category theory and computational performance are not usually mentioned in the same breath, but we have needed substantial engineering effort to teach Coq to cope with large categorical constructions without slowing proof script processing unacceptably. In this paper, we share the lessons we have learned about how to represent very abstract mathematical objects and arguments in Coq and how future proof assistants might be designed to better support such reasoning. One particular encoding trick to which we draw attention allows category-theoretic arguments involving duality to be internalized in Coq's logic with definitional equality. Ours may be the largest Coq development to date that uses the relatively new Coq version developed by homotopy type theorists, and we reflect on which new features were especially helpful.\",\n",
       " \"We describe a method for building composable and extensible verification procedures within the Coq proof assistant. Unlike traditional methods that rely on run-time generation and checking of proofs, we use verified-correct procedures with Coq soundness proofs. Though they are internalized in Coq's logic, our provers support sound extension by users with hints over new domains, enabling automated reasoning about user-defined abstract predicates. We maintain soundness by developing an architecture for modular packaging, construction, and composition of hint databases, which had previously only been implemented in Coq at the level of its dynamically typed, proof-generating tactic language. Our provers also include rich handling of unification variables, enabling integration with other tactic-based deduction steps within Coq. We have implemented our techniques in MirrorShard, an open-source framework for reflective verification. We demonstrate its applicability by instantiating it to separation logic in order to reason about imperative program verification.\",\n",
       " 'We report on the implementation of a certified compiler for a high-level hardware description language (HDL) called Fe-Si (FEatherweight SynthesIs). Fe-Si is a simplified version of Bluespec, an HDL based on a notion of guarded atomic actions. Fe-Si is defined as a dependently typed deep embedding in Coq. The target language of the compiler corresponds to a synthesisable subset of Verilog or VHDL. A key aspect of our approach is that input programs to the compiler can be defined and proved correct inside Coq. Then, we use extraction and a Verilog back-end (written in OCaml) to get a certified version of a hardware design.',\n",
       " 'Learning exists in the context of data, yet notions of \\\\emph{confidence} typically focus on model predictions, not label quality. Confident learning (CL) has emerged as an approach for characterizing, identifying, and learning with noisy labels in datasets, based on the principles of pruning noisy data, counting to estimate noise, and ranking examples to train with confidence. Here, we generalize CL, building on the assumption of a classification noise process, to directly estimate the joint distribution between noisy (given) labels and uncorrupted (unknown) labels. This generalized CL, open-sourced as \\\\texttt{cleanlab}, is provably consistent across reasonable conditions, and experimentally performant on ImageNet and CIFAR, outperforming seven recent approaches when label noise is non-uniform. \\\\texttt{cleanlab} also quantifies ontological class overlap, and can increase model accuracy (e.g. ResNet) by providing clean data for training.',\n",
       " 'The efficient simulation of quantum systems is a primary motivating factor for developing controllable quantum machines. For addressing systems with underlying bosonic structure, it is advantageous to utilize a naturally bosonic platform. Optical photons passing through linear networks may be configured to perform quantum simulation tasks, but the efficient preparation and detection of multiphoton quantum states of light in linear optical systems are challenging. Here, we experimentally implement a boson sampling protocol for simulating molecular vibronic spectra [Nature Photonics $\\\\textbf{9}$, 615 (2015)] in a two-mode superconducting device. In addition to enacting the requisite set of Gaussian operations across both modes, we fulfill the scalability requirement by demonstrating, for the first time in any platform, a high-fidelity single-shot photon number resolving detection scheme capable of resolving up to 15 photons per mode. Furthermore, we exercise the capability of synthesizing non-Gaussian input states to simulate spectra of molecular ensembles in vibrational excited states. We show the re-programmability of our implementation by extracting the spectra of photoelectron processes in H$_2$O, O$_3$, NO$_2$, and SO$_2$. The capabilities highlighted in this work establish the superconducting architecture as a promising platform for bosonic simulations, and by combining them with tools such as Kerr interactions and engineered dissipation, enable the simulation of a wider class of bosonic systems.',\n",
       " 'The Information Bottleneck (IB) method (\\\\cite{tishby2000information}) provides an insightful and principled approach for balancing compression and prediction for representation learning. The IB objective $I(X;Z)-Î²I(Y;Z)$ employs a Lagrange multiplier $Î²$ to tune this trade-off. However, in practice, not only is $Î²$ chosen empirically without theoretical guidance, there is also a lack of theoretical understanding between $Î²$, learnability, the intrinsic nature of the dataset and model capacity. In this paper, we show that if $Î²$ is improperly chosen, learning cannot happen -- the trivial representation $P(Z|X)=P(Z)$ becomes the global minimum of the IB objective. We show how this can be avoided, by identifying a sharp phase transition between the unlearnable and the learnable which arises as $Î²$ is varied. This phase transition defines the concept of IB-Learnability. We prove several sufficient conditions for IB-Learnability, which provides theoretical guidance for choosing a good $Î²$. We further show that IB-learnability is determined by the largest confident, typical, and imbalanced subset of the examples (the conspicuous subset), and discuss its relation with model capacity. We give practical algorithms to estimate the minimum $Î²$ for a given dataset. We also empirically demonstrate our theoretical conditions with analyses of synthetic datasets, MNIST, and CIFAR10.',\n",
       " 'The quantum approximate optimization algorithm~(QAOA) first proposed by Farhi et al. promises near-term applications based on its simplicity, universality, and provable optimality. A depth-p QAOA consists of p interleaved unitary transformations induced by two mutually non-commuting Hamiltonians. A long-standing question concerning the performance of QAOA is the dependence of its success probability as a function of circuit depth p. We make initial progress by analyzing the success probability of QAOA for realizing state transfer in a one-dimensional qubit chain using two-qubit XY Hamiltonians and single-qubit Hamiltonians. We provide analytic state transfer success probability dependencies on p in both low and large p limits by leveraging the unique spectral property of the XY Hamiltonian. We support our proof under a given QAOA ansatz with numerical optimizations of QAOA for up to \\\\(N\\\\)=20 qubits. We show that the optimized QAOA can achieve the well-known quadratic speedup, Grover speedup, over the classical alternatives. Treating QAOA optimization as a quantum control problem, we also provide numerical evidence of how the circuit depth determines the controllability of the QAOA ansatz.',\n",
       " 'Experimentally realizable quantum computers are rapidly approaching the threshold of quantum supremacy. Quantum Hamiltonian simulation promises to be one of the first practical applications for which such a device could demonstrate an advantage over all classical systems. However, these early devices will inevitably remain both noisy and small, precluding the use of quantum error correction. We use high-performance classical tools to construct, optimize, and simulate quantum circuits subject to realistic error models in order to empirically determine the \"simulation capacity\" of near-term simulation experiments implemented via quantum signal processing (QSP), describing the relationship between simulation time, system size, and resolution of QSP circuits which are optimally configured to balance algorithmic precision and external noise. From simulation capacity models, we estimate maximum tolerable error rate for meaningful simulation experiments on a near-term quantum computer.\\n  By exploiting symmetry inherent to the QSP circuit, we further demonstrate that its capacity for quantum simulation can be increased by at least two orders of magnitude if errors are systematic and unitary. We find that a device with $Îµ^2=10^{-5}$ systematic amplitude errors could meaningfully simulate systems up to $n\\\\approx16$ with an expected failure rate below $10\\\\%$, whereas the largest system a device with a stochastic error rate of $p_Îµ=10^{-5}$ could meaningfully simulate with the same rate of failure is between $n=3$ and $n=5$ (depending on the stochastic channel). Extrapolating from empirical results, we estimate that one would typically need a stochastic error rate below $p_Îµ=10^{-8}$ to perform a meaningful $n=50$ simulation experiment with a failure rate below $10\\\\%$, while the same experiment could tolerate systematic unitary errors with strength $Îµ^2\\\\approx10^{-6}$.',\n",
       " \"To understand the fundamental trade-offs between training stability, temporal dynamics and architectural complexity of recurrent neural networks~(RNNs), we directly analyze RNN architectures using numerical methods of ordinary differential equations~(ODEs). We define a general family of RNNs--the ODERNNs--by relating the composition rules of RNNs to integration methods of ODEs at discrete time steps. We show that the degree of RNN's functional nonlinearity $n$ and the range of its temporal memory $t$ can be mapped to the corresponding stage of Runge-Kutta recursion and the order of time-derivative of the ODEs. We prove that popular RNN architectures, such as LSTM and URNN, fit into different orders of $n$-$t$-ODERNNs. This exact correspondence between RNN and ODE helps us to establish the sufficient conditions for RNN training stability and facilitates more flexible top-down designs of new RNN architectures using large varieties of toolboxes from numerical integration of ODEs. We provide such an example: Quantum-inspired Universal computing Neural Network~(QUNN), which reduces the required number of training parameters from polynomial in both data length and temporal memory length to only linear in temporal memory length.\",\n",
       " 'While quantum devices rely on interactions between constituent subsystems and with their environment to operate, native interactions alone often fail to deliver targeted performance. Coherent pulsed control provides the ability to tailor effective interactions, known as Hamiltonian engineering. We propose a Hamiltonian engineering method that maximizes desired interactions while mitigating deleterious ones by conducting a pulse sequence search using constrained optimization. The optimization formulation incorporates pulse sequence length and cardinality penalties consistent with linear or integer programming. We apply the general technique to magnetometry with solid state spin ensembles in which inhomogeneous interactions between sensing spins limit coherence. Defining figures of merit for broadband Ramsey magnetometry, we present novel pulse sequences which outperform known techniques for homonuclear spin decoupling in both spin-1/2 and spin-1 systems. When applied to nitrogen vacancy (NV) centers in diamond, this scheme partially preserves the Zeeman interaction while zeroing dipolar coupling between negatively charged NV$^{\\\\text -}$ centers. Such a scheme is of interest for NV$^\\\\text{-}$ magnetometers which have reached the NV$^\\\\text{-}$-NV$^\\\\text{-}$ coupling limit. We discuss experimental implementation in NV ensembles, as well as applicability of the current approach to more general spin bath decoupling and superconducting qubit control.',\n",
       " 'Compared to humans, machine learning models generally require significantly more training examples and fail to extrapolate from experience to solve previously unseen challenges. To help close this performance gap, we augment single-task neural networks with a meta-recognition model which learns a succinct model code via its autoencoder structure, using just a few informative examples. The model code is then employed by a meta-generative model to construct parameters for the task-specific model. We demonstrate that for previously unseen tasks, without additional training, this Meta-Learning Autoencoder (MeLA) framework can build models that closely match the true underlying models, with loss significantly lower than given by fine-tuned baseline networks, and performance that compares favorably with state-of-the-art meta-learning algorithms. MeLA also adds the ability to identify influential training examples and predict which additional data will be most valuable to acquire to improve model prediction.',\n",
       " 'Each time a learner in a self-paced online course seeks to answer an assessment question, it takes some time for the student to read the question and arrive at an answer to submit. If multiple attempts are allowed, and the first answer is incorrect, it takes some time to provide a second answer. Here we study the distribution of such \"response times.\" We find that the log-normal statistical model for such times, previously suggested in the literature, holds for online courses. Users who, according to this model, tend to take longer on submits are more likely to complete the course, have a higher level of engagement, and achieve a higher grade. This finding can be the basis for designing interventions in online courses, such as MOOCs, which would encourage \"fast\" users to slow down.',\n",
       " 'We present a novel set of reversible modular multipliers applicable to quantum computing, derived from three classical techniques: 1) traditional integer division, 2) Montgomery residue arithmetic, and 3) Barrett reduction. Each multiplier computes an exact result for all binary input values, while maintaining the asymptotic resource complexity of a single (non-modular) integer multiplier. We additionally conduct an empirical resource analysis of our designs in order to determine the total gate count and circuit depth of each fully constructed circuit, with inputs as large as 2048 bits. Our comparative analysis considers both circuit implementations which allow for arbitrary (controlled) rotation gates, as well as those restricted to a typical fault-tolerant gate set.',\n",
       " 'We establish a symmetry-operator framework for designing quantum error correcting~(QEC) codes based on fundamental properties of the underlying system dynamics. Based on this framework, we propose three hardware-efficient bosonic QEC codes that are suitable for $Ï‡^{(2)}$-interaction based quantum computation: the $Ï‡^{(2)}$ parity-check code, the $Ï‡^{(2)}$ embedded error-correcting code, and the $Ï‡^{(2)}$ binomial code, all of which detect photon-loss or photon-gain errors by means of photon-number parity measurements and then correct them via $Ï‡^{(2)}$ Hamiltonian evolutions and linear-optics transformations. Our symmetry-operator framework provides a systematic procedure for finding QEC codes that are not stabilizer codes. The $Ï‡^{(2)}$ binomial code is of special interest because, with $m\\\\le N$ identified from channel monitoring, it can correct $m$-photon loss errors, $m$-photon gain errors, and $(m-1)$th-order dephasing errors using logical qudits that are encoded in $O(N)$ photons. In comparison, other bosonic QEC codes require $O(N^2)$ photons to correct the same degree of bosonic errors. Such improved photon-efficiency underscores the additional error-correction power that can be provided by channel monitoring. We develop quantum Hamming bounds for photon-loss errors in the code subspaces associated with the $Ï‡^{(2)}$ parity-check code and the $Ï‡^{(2)}$ embedded error-correcting code, and we prove that these codes saturate their respective bounds. Our $Ï‡^{(2)}$ QEC codes exhibit hardware efficiency in that they address the principal error mechanisms and exploit the available physical interactions of the underlying hardware, thus reducing the physical resources required for implementing their encoding, decoding, and error-correction operations, and their universal encoded-basis gate sets.',\n",
       " 'The exponential speedups promised by Hamiltonian simulation on a quantum computer depends crucially on structure in both the Hamiltonian $\\\\hat{H}$, and the quantum circuit $\\\\hat{U}$ that encodes its description. In the quest to better approximate time-evolution $e^{-i\\\\hat{H}t}$ with error $Îµ$, we motivate a systematic approach to understanding and exploiting structure, in a setting where Hamiltonians are encoded as measurement operators of unitary circuits $\\\\hat{U}$ for generalized measurement. This allows us to define a \\\\emph{uniform spectral amplification} problem on this framework for expanding the spectrum of encoded Hamiltonian with exponentially small distortion. We present general solutions to uniform spectral amplification in a hierarchy where factoring $\\\\hat{U}$ into $n=1,2,3$ unitary oracles represents increasing structural knowledge of the encoding. Combined with structural knowledge of the Hamiltonian, specializing these results allow us simulate time-evolution by $d$-sparse Hamiltonians using $\\\\mathcal{O}\\\\left(t(d \\\\|\\\\hat H\\\\|_{\\\\text{max}}\\\\|\\\\hat H\\\\|_{1})^{1/2}\\\\log{(t\\\\|\\\\hat{H}\\\\|/Îµ)}\\\\right)$ queries, where $\\\\|\\\\hat H\\\\|\\\\le \\\\|\\\\hat H\\\\|_1\\\\le d\\\\|\\\\hat H\\\\|_{\\\\text{max}}$. Up to logarithmic factors, this is a polynomial improvement upon prior art using $\\\\mathcal{O}\\\\left(td\\\\|\\\\hat H\\\\|_{\\\\text{max}}+\\\\frac{\\\\log{(1/Îµ)}}{\\\\log\\\\log{(1/Îµ)}}\\\\right)$ or $\\\\mathcal{O}(t^{3/2}(d \\\\|\\\\hat H\\\\|_{\\\\text{max}}\\\\|\\\\hat H\\\\|_{1}\\\\|\\\\hat H\\\\|/Îµ)^{1/2})$ queries. In the process, we also prove a matching lower bound of $Î©(t(d\\\\|\\\\hat H\\\\|_{\\\\text{max}}\\\\|\\\\hat H\\\\|_{1})^{1/2})$ queries, present a distortion-free generalization of spectral gap amplification, and an amplitude amplification algorithm that performs multiplication on unknown state amplitudes.',\n",
       " 'A non-Clifford gate is required for universal quantum computation, and, typically, this is the most error-prone and resource intensive logical operation on an error-correcting code. Small, single-qubit rotations are popular choices for this non-Clifford gate, but certain three-qubit gates, such as Toffoli or controlled-controlled-Z (CCZ), are equivalent options that are also more suited for implementing some quantum algorithms, for instance those with coherent classical subroutines. Here, we calculate error rates and resource overheads for implementing logical CCZ with pieceable fault-tolerance, a non-transversal method for implementing logical gates. We provide a comparison with a non-local magic-state scheme on a concatenated code and a local magic-state scheme on the surface code. We find the pieceable fault-tolerance scheme particularly advantaged over magic states on concatenated codes and in certain regimes over magic states on the surface code. Our results suggest that pieceable fault-tolerance is a promising candidate for fault-tolerance in a near-future quantum computer.',\n",
       " 'Noisy PN learning is the problem of binary classification when training examples may be mislabeled (flipped) uniformly with noise rate rho1 for positive examples and rho0 for negative examples. We propose Rank Pruning (RP) to solve noisy PN learning and the open problem of estimating the noise rates, i.e. the fraction of wrong positive and negative labels. Unlike prior solutions, RP is time-efficient and general, requiring O(T) for any unrestricted choice of probabilistic classifier with T fitting time. We prove RP has consistent noise estimation and equivalent expected risk as learning with uncorrupted labels in ideal conditions, and derive closed-form solutions when conditions are non-ideal. RP achieves state-of-the-art noise estimation and F1, error, and AUC-PR for both MNIST and CIFAR datasets, regardless of the amount of noise and performs similarly impressively when a large portion of training examples are noise drawn from a third distribution. To highlight, RP with a CNN classifier can predict if an MNIST digit is a \"one\"or \"not\" with only 0.25% error, and 0.46 error across all digits, even when 50% of positive examples are mislabeled and 50% of observed positive labels are mislabeled negative examples.',\n",
       " 'We prove that universal quantum computation can be realized---using only linear optics and $Ï‡^{(2)}$ (three-wave mixing) interactions---in any $(n+1)$-dimensional qudit basis of the $n$-pump-photon subspace. First, we exhibit a strictly universal gate set for the qubit basis in the one-pump-photon subspace. Next, we demonstrate qutrit-basis universality by proving that $Ï‡^{(2)}$ Hamiltonians and photon-number operators generate the full $\\\\mathfrak{u}(3)$ Lie algebra in the two-pump-photon subspace, and showing how the qutrit controlled-$Z$ gate can be implemented with only linear optics and $Ï‡^{(2)}$ interactions. We then use proof by induction to obtain our general qudit result. Our induction proof relies on coherent photon injection/subtraction, a technique enabled by $Ï‡^{(2)}$ interaction between the encoding modes and ancillary modes. Finally, we show that coherent photon injection is more than a conceptual tool in that it offers a route to preparing high-photon-number Fock states from single-photon Fock states.',\n",
       " \"We present the problem of approximating the time-evolution operator $e^{-i\\\\hat{H}t}$ to error $Îµ$, where the Hamiltonian $\\\\hat{H}=(\\\\langle G|\\\\otimes\\\\hat{\\\\mathcal{I}})\\\\hat{U}(|G\\\\rangle\\\\otimes\\\\hat{\\\\mathcal{I}})$ is the projection of a unitary oracle $\\\\hat{U}$ onto the state $|G\\\\rangle$ created by another unitary oracle. Our algorithm solves this with a query complexity $\\\\mathcal{O}\\\\big(t+\\\\log({1/Îµ})\\\\big)$ to both oracles that is optimal with respect to all parameters in both the asymptotic and non-asymptotic regime, and also with low overhead, using at most two additional ancilla qubits. This approach to Hamiltonian simulation subsumes important prior art considering Hamiltonians which are $d$-sparse or a linear combination of unitaries, leading to significant improvements in space and gate complexity, such as a quadratic speed-up for precision simulations. It also motivates useful new instances, such as where $\\\\hat{H}$ is a density matrix. A key technical result is `qubitization', which uses the controlled version of these oracles to embed any $\\\\hat{H}$ in an invariant $\\\\text{SU}(2)$ subspace. A large class of operator functions of $\\\\hat{H}$ can then be computed with optimal query complexity, of which $e^{-i\\\\hat{H}t}$ is a special case.\",\n",
       " \"Fixed-point quantum search algorithms succeed at finding one of $M$ target items among $N$ total items even when the run time of the algorithm is longer than necessary. While the famous Grover's algorithm can search quadratically faster than a classical computer, it lacks the fixed-point property --- the fraction of target items must be known precisely to know when to terminate the algorithm. Recently, Yoder, Low, and Chuang gave an optimal gate-model search algorithm with the fixed-point property. Meanwhile, it is known that an adiabatic quantum algorithm, operating by continuously varying a Hamiltonian, can reproduce the quadratic speedup of gate-model Grover search. We ask, can an adiabatic algorithm also reproduce the fixed-point property? We show that the answer depends on what interpolation schedule is used, so as in the gate model, there are both fixed-point and non-fixed-point versions of adiabatic search, only some of which attain the quadratic quantum speedup. Guided by geometric intuition on the Bloch sphere, we rigorously justify our claims with an explicit upper bound on the error in the adiabatic approximation. We also show that the fixed-point adiabatic search algorithm can be simulated in the gate model with neither loss of the quadratic Grover speedup nor of the fixed-point property. Finally, we discuss natural uses of fixed-point algorithms such as preparation of a relatively prime state and oblivious amplitude amplification.\",\n",
       " 'The physics of quantum mechanics is the inspiration for, and underlies, quantum computation. As such, one expects physical intuition to be highly influential in the understanding and design of many quantum algorithms, particularly simulation of physical systems. Surprisingly, this has been challenging, with current Hamiltonian simulation algorithms remaining abstract and often the result of sophisticated but unintuitive constructions. We contend that physical intuition can lead to optimal simulation methods by showing that a focus on simple single-qubit rotations elegantly furnishes an optimal algorithm for Hamiltonian simulation, a universal problem that encapsulates all the power of quantum computation. Specifically, we show that the query complexity of implementing time evolution by a $d$-sparse Hamiltonian $\\\\hat{H}$ for time-interval $t$ with error $Îµ$ is $\\\\mathcal{O}(td\\\\|\\\\hat{H}\\\\|_{\\\\text{max}}+\\\\frac{\\\\log{(1/Îµ)}}{\\\\log{\\\\log{(1/Îµ)}}})$, which matches lower bounds in all parameters. This connection is made through general three-step \"quantum signal processing\" methodology, comprised of (1) transducing eigenvalues of $\\\\hat{H}$ into a single ancilla qubit, (2) transforming these eigenvalues through an optimal-length sequence of single-qubit rotations, and (3) projecting this ancilla with near unity success probability.',\n",
       " 'Classical imaging works by scattering photons from an object to be imaged, and achieves resolution scaling as $1/\\\\sqrt{t}$, with $t$ the imaging time. By contrast, the laws of quantum mechanics allow one to utilize quantum coherence to obtain imaging resolution that can scale as quickly as $1/t$ -- the so-called \"Heisenberg limit.\" However, ambiguities in the obtained signal often preclude taking full advantage of this quantum enhancement, while imaging techniques designed to be unambiguous often lose this optimal Heisenberg scaling. Here, we demonstrate an imaging technique which combines unambiguous detection of the target with Heisenberg scaling of the resolution. We also demonstrate a binary search algorithm which can efficiently locate a coherent target using the technique, resolving a target trapped ion to within 0.3% of the $1/e^2$ diameter of the excitation beam.',\n",
       " 'We report on a method for measuring the branching ratios of dipole transitions of trapped atomic ions by performing nested sequences of population inversions. This scheme is broadly applicable and does not use ultrafast pulsed or narrow linewidth lasers. It is simple to perform and insensitive to experimental variables such as laser and magnetic field noise as well as ion heating. To demonstrate its effectiveness, we make the most accurate measurements thus far of the branching ratios of both 5P1/2 and 5P3/2 states in 88Sr+ with sub-1% uncertainties. We measure 17.175(27) for the branching ratio of 5P1/2-5S1/2, 15.845(71) for 5P3/2-5S1/2, and 0.05609(21) for 5P3/2-4D5/2, ten- fold and thirty-fold improvements in precision for 5P1/2 and 5P3/2 branching ratios respectively over the best previous experimental values.',\n",
       " 'The creation of composite quantum gates that implement quantum response functions $\\\\hat{U}(Î¸)$ dependent on some parameter of interest $Î¸$ is often more of an art than a science. Through inspired design, a sequence of $L$ primitive gates also depending on $Î¸$ can engineer a highly nontrivial $\\\\hat{U}(Î¸)$ that enables myriad precision metrology, spectroscopy, and control techniques. However, discovering new, useful examples of $\\\\hat{U}(Î¸)$ requires great intuition to perceive the possibilities, and often brute-force to find optimal implementations. We present a systematic and efficient methodology for composite gate design of arbitrary length, where phase-controlled primitive gates all rotating by $Î¸$ act on a single spin. We fully characterize the realizable family of $\\\\hat{U}(Î¸)$, provide an efficient algorithm that decomposes a choice of $\\\\hat{U}(Î¸)$ into its shortest sequence of gates, and show how to efficiently choose an achievable $\\\\hat{U}(Î¸)$ that for fixed $L$, is an optimal approximation to objective functions on its quadratures. A strong connection is forged with \\\\emph{classical} discrete-time signal processing, allowing us to swiftly construct, as examples, compensated gates with optimal bandwidth that implement arbitrary single spin rotations with sub-wavelength spatial selectivity.',\n",
       " 'It is an oft-cited fact that no quantum code can support a set of fault-tolerant logical gates that is both universal and transversal. This no-go theorem is generally responsible for the interest in alternative universality constructions including magic state distillation. Widely overlooked, however, is the possibility of non-transversal, yet still fault-tolerant, gates that work directly on small quantum codes. Here we demonstrate precisely the existence of such gates. In particular, we show how the limits of non-transversality can be overcome by performing rounds of intermediate error-correction to create logical gates on stabilizer codes that use no ancillas other than those required for syndrome measurement. Moreover, the logical gates we construct, the most prominent examples being Toffoli and controlled-controlled-Z, often complete universal gate sets on their codes. We detail such universal constructions for the smallest quantum codes, the 5-qubit and 7-qubit codes, and then proceed to generalize the approach. One remarkable result of this generalization is that any nondegenerate stabilizer code with a complete set of fault-tolerant single-qubit Clifford gates has a universal set of fault-tolerant gates. Another is the interaction of logical qubits across different stabilizer codes, which, for instance, implies a broadly applicable method of code switching.',\n",
       " 'We describe a cheating strategy enabled by the features of massive open online courses (MOOCs) and detectable by virtue of the sophisticated data systems that MOOCs provide. The strategy, Copying Answers using Multiple Existences Online (CAMEO), involves a user who gathers solutions to assessment questions using a \"harvester\" account and then submits correct answers using a separate \"master\" account. We use \"clickstream\" learner data to detect CAMEO use among 1.9 million course participants in 115 MOOCs from two universities. Using conservative thresholds, we estimate CAMEO prevalence at 1,237 certificates, accounting for 1.3% of the certificates in the 69 MOOCs with CAMEO users. Among earners of 20 or more certificates, 25% have used the CAMEO strategy. CAMEO users are more likely to be young, male, and international than other MOOC certificate earners. We identify preventive strategies that can decrease CAMEO rates and show evidence of their effectiveness in science courses.',\n",
       " 'Quantum computers are able to outperform classical algorithms. This was long recognized by the visionary Richard Feynman who pointed out in the 1980s that quantum mechanical problems were better solved with quantum machines. It was only in 1994 that Peter Shor came up with an algorithm that is able to calculate the prime factors of a large number vastly more efficiently than known possible with a classical computer. This paradigmatic algorithm stimulated the flourishing research in quantum information processing and the quest for an actual implementation of a quantum computer. Over the last fifteen years, using skillful optimizations, several instances of a Shor algorithm have been implemented on various platforms and clearly proved the feasibility of quantum factoring. For general scalability, though, a different approach has to be pursued. Here, we report the realization of a fully scalable Shor algorithm as proposed by Kitaev. For this, we demonstrate factoring the number fifteen by effectively employing and controlling seven qubits and four \"cache-qubits\", together with the implementation of generalized arithmetic operations, known as modular multipliers. The scalable algorithm has been realized with an ion-trap quantum computer exhibiting success probabilities in excess of 90%.',\n",
       " 'We study the vacuum-induced degradation of high-finesse optical cavities with mirror coatings composed of SiO$_2$-Ta$_{2}$O$_{5}$ dielectric stacks, and present methods to protect these coatings and to recover their initial quality factor. For separate coatings with reflectivities centered at 370 nm and 422 nm, a vacuum-induced continuous increase in optical loss occurs if the surface-layer coating is made of Ta$_{2}$O$_{5}$, while it does not occur if it is made of SiO$_2$. The incurred optical loss can be reversed by filling the vacuum chamber with oxygen at atmospheric pressure, and the recovery rate can be strongly accelerated by continuous laser illumination at 422 nm. Both the degradation and the recovery processes depend strongly on temperature. We find that a 1 nm-thick layer of SiO$_2$ passivating the Ta$_{2}$O$_{5}$ surface layer is sufficient to reduce the degradation rate by more than a factor of 10, strongly supporting surface oxygen depletion as the primary degradation mechanism.',\n",
       " 'Scaling-up from prototype systems to dense arrays of ions on chip, or vast networks of ions connected by photonic channels, will require developing entirely new technologies that combine miniaturized ion trapping systems with devices to capture, transmit and detect light, while refining how ions are confined and controlled. Building a cohesive ion system from such diverse parts involves many challenges, including navigating materials incompatibilities and undesired coupling between elements. Here, we review our recent efforts to create scalable ion systems incorporating unconventional materials such as graphene and indium tin oxide, integrating devices like optical fibers and mirrors, and exploring alternative ion loading and trapping techniques.',\n",
       " 'Conventional wisdom dictates that to image the position of fluorescent atoms or molecules, one should stimulate as much emission and collect as many photons as possible. That is, in this classical case, it has always been assumed that the coherence time of the system should be made short, and that the statistical scaling $\\\\sim1/\\\\sqrt{t}$ defines the resolution limit for imaging time $t$. However, here we show in contrast that given the same resources, a long coherence time permits a higher resolution image. In this quantum regime, we give a procedure for determining the position of a single two-level system, and demonstrate that the standard errors of our position estimates scale at the Heisenberg limit as $\\\\sim 1/t$, a quadratic, and notably optimal, improvement over the classical case.',\n",
       " \"Grover's quantum search and its generalization, quantum amplitude amplification, provide quadratic advantage over classical algorithms for a diverse set of tasks, but are tricky to use without knowing beforehand what fraction $Î»$ of the initial state is comprised of the target states. In contrast, fixed-point search algorithms need only a reliable lower bound on this fraction, but, as a consequence, lose the very quadratic advantage that makes Grover's algorithm so appealing. Here we provide the first version of amplitude amplification that achieves fixed-point behavior without sacrificing the quantum speedup. Our result incorporates an adjustable bound on the failure probability, and, for a given number of oracle queries, guarantees that this bound is satisfied over the broadest possible range of $Î»$.\",\n",
       " \"Performing exact inference on Bayesian networks is known to be #P-hard. Typically approximate inference techniques are used instead to sample from the distribution on query variables given the values $e$ of evidence variables. Classically, a single unbiased sample is obtained from a Bayesian network on $n$ variables with at most $m$ parents per node in time $\\\\mathcal{O}(nmP(e)^{-1})$, depending critically on $P(e)$, the probability the evidence might occur in the first place. By implementing a quantum version of rejection sampling, we obtain a square-root speedup, taking $\\\\mathcal{O}(n2^mP(e)^{-\\\\frac12})$ time per sample. We exploit the Bayesian network's graph structure to efficiently construct a quantum state, a q-sample, representing the intended classical distribution, and also to efficiently apply amplitude amplification, the source of our speedup. Thus, our speedup is notable as it is unrelativized -- we count primitive operations and require no blackbox oracle queries.\",\n",
       " 'Massive Open Online Courses are an exciting new avenue for instruction and research, yet they are full of unknowns. In the Spring of 2013, MITx released its first introductory physics MOOC through the edX platform, generating a total enrollment of 43,000 students from around the world. We describe the population of participants in terms of their age, gender, level of education, and country of origin, highlighting both the diversity of 8.02x enrollees as well as gender gap and retention. Using three midterm exams and the final as waypoints, we highlight performance by different demographic subpopulations and their retention rates. Our work is generally aimed at making a bridge between available MOOC data and topics associated with the Physics Education Research community.',\n",
       " 'Implementing a single qubit unitary is often hampered by imperfect control. Systematic amplitude errors $Îµ$, caused by incorrect duration or strength of a pulse, are an especially common problem. But a sequence of imperfect pulses can provide a better implementation of a desired operation, as compared to a single primitive pulse. We find optimal pulse sequences consisting of $L$ primitive $Ï€$ or $2Ï€$ rotations that suppress such errors to arbitrary order $\\\\mathcal{O}(Îµ^{n})$ on arbitrary initial states. Optimality is demonstrated by proving an $L=\\\\mathcal{O}(n)$ lower bound and saturating it with $L=2n$ solutions. Closed-form solutions for arbitrary rotation angles are given for $n=1,2,3,4$. Perturbative solutions for any $n$ are proven for small angles, while arbitrary angle solutions are obtained by analytic continuation up to $n=12$. The derivation proceeds by a novel algebraic and non-recursive approach, in which finding amplitude error correcting sequences can be reduced to solving polynomial equations.',\n",
       " 'We present a novel hybrid system where an optical cavity is integrated with a microfabricated planar-electrode ion trap. The trap electrodes produce a tunable periodic potential allowing the trapping of up to 50 separate ion chains spaced by 160 $Î¼$m along the cavity axis. Each chain can contain up to 20 individually addressable Yb\\\\textsuperscript{+} ions coupled to the cavity mode. We demonstrate deterministic distribution of ions between the sites of the electrostatic periodic potential and control of the ion-cavity coupling. The measured strength of this coupling should allow access to the strong collective coupling regime with $\\\\lesssim$10 ions. The optical cavity could serve as a quantum information bus between ions or be used to generate a strong wavelength-scale periodic optical potential.',\n",
       " 'Fluorescence collection sets the efficiency of state detection and the rate of entanglement generation between remote trapped ion qubits. Despite efforts to improve light collection using various optical elements, solid angle capture is limited to ~10% for implementations that are scalable to many ions. We present an approach based on fluorescence detection through a transparent trap using an integrated photodetector, combining collection efficiency approaching 50% with scalability. We microfabricate transparent surface traps with indium tin oxide and verify stable trapping of single ions. The fluorescence from a cloud of ions is detected using a photodiode sandwiched with a transparent trap.',\n",
       " 'Fermions, as a major class of quantum particles, provide platforms for quantum information processing beyond the possibilities of spins or bosons which have been studied more extensively. One particularly interesting model to study, in view of recent progress in manipulating ultracold fermion gases, is the fermionic version of measurement-based quantum computation (MBQC), which implements full quantum computation with only single site measurements on a proper fermionic many-body resource state. However, it is not known which fermionic states can be used as the resource states for MBQC and how to find them. In this paper, we generalize the framework of spin MBQC to fermions. In particular, we provide a general formalism to construct many-body entangled fermion resource states for MBQC based on the fermionic projected entangled pair state representation. We give a specific fermionic state which enables universal MBQC and demonstrate that the non-locality inherent in fermion systems can be properly taken care of with suitable measurement schemes. Such a framework opens up possibilities of finding MBQC resource states which can be more readily realized in the lab.',\n",
       " 'Previous analyses of conditional Ï†-phase gates for photonic qubits that treat cross-phase modulation (XPM) in a causal, multimode, quantum field setting suggest that a large (~Ï€rad) nonlinear phase shift is always accompanied by fidelity-degrading noise [J. H. Shapiro, Phys. Rev. A 73, 062305 (2006); J. Gea-Banacloche, Phys. Rev. A 81, 043823 (2010)]. Using an atomic V-system to model an XPM medium, we present a conditional phase gate that, for sufficiently small nonzero Ï†, has high fidelity. The gate is made cascadable by using using a special measurement, principal mode projection, to exploit the quantum Zeno effect and preclude the accumulation of fidelity-degrading departures from the principal-mode Hilbert space when both control and target photons illuminate the gate.',\n",
       " \"We model electric field noise from fluctuating patch potentials on conducting surfaces by taking into account the finite geometry of the ion trap electrodes to gain insight into the origin of anomalous heating in ion traps. The scaling of anomalous heating rates with surface distance, $d$, is obtained for several generic geometries of relevance to current ion trap designs, ranging from planar to spheroidal electrodes. The influence of patch size is studied both by solving Laplace's equation in terms of the appropriate Green's function as well as through an eigenfunction expansion. Scaling with surface distance is found to be highly dependent on the choice of geometry and the relative scale between the spatial extent of the electrode, the ion-electrode distance, and the patch size. Our model generally supports the $d^{-4}$ dependence currently found by most experiments and models, but also predicts geometry-driven deviations from this trend.\",\n",
       " \"Electrical charging of metal surfaces due to photoelectric generation of carriers is of concern in trapped ion quantum computation systems, due to the high sensitivity of the ions' motional quantum states to deformation of the trapping potential. The charging induced by typical laser frequencies involved in doppler cooling and quantum control is studied here, with microfabricated surface electrode traps made of aluminum, copper, and gold, operated at 6 K with a single Sr$^+$ ion trapped 100 $Î¼$m above the trap surface. The lasers used are at 370, 405, 460, and 674 nm, and the typical photon flux at the trap is 10$^{14}$ photons/cm$^2$/sec. Charging is detected by monitoring the ion's micromotion signal, which is related to the number of charges created on the trap. A wavelength and material dependence of the charging behavior is observed: lasers at lower wavelengths cause more charging, and aluminum exhibits more charging than copper or gold. We describe the charging dynamic based on a rate equation approach.\",\n",
       " 'An atomic ion is trapped at the tip of a single-mode optical fiber in a cryogenic (8 K) surface-electrode ion trap. The fiber serves as an integrated source of laser light, which drives the quadrupole qubit transition of $^{88}$Sr$^+$. Through \\\\emph{in situ} translation of the nodal point of the trapping field, the Gaussian beam profile of the fiber output is imaged, and the fiber-ion displacement, in units of the mode waist at the ion, is optimized to within $0.13\\\\pm0.10$ of the mode center despite an initial offset of $3.30\\\\pm0.10$. Fiber-induced charging at $125 Î¼$W is observed to be ${\\\\sim}10$ V/m at an ion height of $670 Î¼$m, with charging and discharging time constants of $1.6\\\\pm0.3$ s and $4.7\\\\pm0.6$ s respectively. This work is of importance to large-scale, ion-based quantum information processing, where optics integration in surface-electrode designs may be a crucial enabling technology.',\n",
       " 'A novel approach to optics integration in ion traps is demonstrated based on a surface electrode ion trap that is microfabricated on top of a dielectric mirror. Additional optical losses due to fabrication are found to be as low as 80 ppm for light at 422 nm. The integrated mirror is used to demonstrate light collection from, and imaging of, a single 88 Sr+ ion trapped $169\\\\pm4 Î¼$m above the mirror.',\n",
       " 'We fabricate superconducting ion traps with niobium and niobium nitride and trap single 88Sr ions at cryogenic temperatures. The superconducting transition is verified and characterized by measuring the resistance and critical current using a 4-wire measurement on the trap structure, and observing change in the rf reflection. The lowest observed heating rate is 2.1(3) quanta/sec at 800 kHz at 6 K and shows no significant change across the superconducting transition, suggesting that anomalous heating is primarily caused by noise sources on the surface. This demonstration of superconducting ion traps opens up possibilities for integrating trapped ions and molecular ions with superconducting devices.',\n",
       " 'It is well known that the ground state energy of many-particle Hamiltonians involving only 2-body interactions can be obtained using constrained optimizations over density matrices which arise from reducing an N-particle state. While determining which 2-particle density matrices are \"N- representable\" is a computationally hard problem, all known extreme N-representable 2-particle reduced density matrices arise from a unique N-particle pre-image, satisfying a conjecture established in 1972. We present explicit counterexamples to this conjecture through giving Hamiltonians with 2-body interactions which have degenerate ground states that cannot be distinguished by any 2-body operator. We relate the existence of such counterexamples to quantum error correction codes and topologically ordered spin systems.',\n",
       " 'Two-dimensional crystals of trapped ions are a promising system with which to implement quantum simulations of challenging problems such as spin frustration. Here, we present a design for a surface-electrode elliptical ion trap which produces a 2-D ion crystal and is amenable to microfabrication, which would enable higher simulated coupling rates, as well as interactions based on magnetic forces generated by on-chip currents. Working in an 11 K cryogenic environment, we experimentally verify to within 5% a numerical model of the structure of ion crystals in the trap. We also explore the possibility of implementing quantum simulation using magnetic forces, and calculate J-coupling rates on the order of 10^3 / s for an ion crystal height of 10 microns, using a current of 1 A.',\n",
       " 'We present a model as well as experimental results for a surface electrode radio-frequency Paul trap that has a circular electrode geometry well-suited for trapping of single ions and two-dimensional planar ion crystals. The trap design is compatible with microfabrication and offers a simple method by which the height of the trapped ions above the surface may be changed \\\\emph{in situ}. We demonstrate trapping of single and few Sr+ ions over an ion height range of 200-1000 microns for several hours under Doppler laser cooling, and use these to characterize the trap, finding good agreement with our model.',\n",
       " 'The tensor product representation of quantum states leads to a promising variational approach to study quantum phase and quantum phase transitions, especially topological ordered phases which are impossible to handle with conventional methods due to their long range entanglement. However, an important issue arises when we use tensor product states (TPS) as variational states to find the ground state of a Hamiltonian: can arbitrary variations in the tensors that represent ground state of a Hamiltonian be induced by local perturbations to the Hamiltonian? Starting from a tensor product state which is the exact ground state of a Hamiltonian with $\\\\mathbb{Z}_2$ topological order, we show that, surprisingly, not all variations of the tensors correspond to the variation of the ground state caused by local perturbations of the Hamiltonian. Even in the absence of any symmetry requirement of the perturbed Hamiltonian, one necessary condition for the variations of the tensors to be physical is that they respect certain $\\\\mathbb{Z}_2$ symmetry. We support this claim by calculating explicitly the change in topological entanglement entropy with different variations in the tensors. This finding will provide important guidance to numerical variational study of topological phase and phase transitions. It is also a crucial step in using TPS to study universal properties of a quantum phase and its topological order.',\n",
       " '  Entanglement, as studied in quantum information science, and non-local quantum correlations, as studied in condensed matter physics, are fundamentally akin to each other. However, their relationship is often hard to quantify due to the lack of a general approach to study both on the same footing. In particular, while entanglement and non-local correlations are properties of states, both arise from symmetries of global operators that commute with the system Hamiltonian. Here, we introduce a framework for completely classifying the local and non-local properties of all such global operators, given the Hamiltonian and a bi-partitioning of the system. This framework is limited to descriptions based on stabilizer quantum codes, but may be generalized. We illustrate the use of this framework to study entanglement and non-local correlations by analyzing global symmetries in topological order, distribution of entanglement and entanglement entropy.',\n",
       " 'We demonstrate quantum control techniques for a single trapped ion in a cryogenic, surface-electrode trap. A narrow optical transition of Sr+ along with the ground and first excited motional states of the harmonic trapping potential form a two-qubit system. The optical qubit transition is susceptible to magnetic field fluctuations, which we stabilize with a simple and compact method using superconducting rings. Decoherence of the motional qubit is suppressed by the cryogenic environment. AC Stark shift correction is accomplished by controlling the laser phase in the pulse sequencer, eliminating the need for an additional laser. Quantum process tomography is implemented on atomic and motional states using conditional pulse sequences. With these techniques we demonstrate a Cirac-Zoller Controlled-NOT gate in a single ion with a mean fidelity of 91(1)%.',\n",
       " '  Graphs are closely related to quantum error-correcting codes: every stabilizer code is locally equivalent to a graph code, and every codeword stabilized code can be described by a graph and a classical code. For the construction of good quantum codes of relatively large block length, concatenated quantum codes and their generalizations play an important role. We develop a systematic method for constructing concatenated quantum codes based on \"graph concatenation\", where graphs representing the inner and outer codes are concatenated via a simple graph operation called \"generalized local complementation.\" Our method applies to both binary and non-binary concatenated quantum codes as well as their generalizations.',\n",
       " '  We report a demonstration and quantitative characterization of one-dimensional cavity cooling of a single trapped 88Sr+ ion in the resolved sideband regime. We measure the spectrum of cavity transitions, the rates of cavity heating and cooling, and the steady-state cooling limit. The cavity cooling dynamics and cooling limit of 22.5(3) motional quanta, limited by the moderate coupling between the ion and the cavity, are consistent with a simple model [Phys. Rev. A 64, 033405] without any free parameters, validating the rate equation model for cavity cooling.',\n",
       " '  Many-body entangled quantum states studied in condensed matter physics can be primary resources for quantum information, allowing any quantum computation to be realized using measurements alone, on the state. Such a universal state would be remarkably valuable, if only it were thermodynamically stable and experimentally accessible, by virtue of being the unique ground state of a physically reasonable Hamiltonian made of two-body, nearest neighbor interactions. We introduce such a state, composed of six-state particles on a hexagonal lattice, and describe a general method for analyzing its properties based on its projected entangled pair state representation.',\n",
       " '  Dense array of ions in microfabricated traps represent one possible way to scale up ion trap quantum computing. The ability to address individual ions is an important component of such a scheme. We demonstrate individual addressing of trapped ions in a microfabricated surface-electrode trap using a magnetic field gradient generated on-chip. A frequency splitting of 310(2) kHz for two ions separated by 5 um is achieved. Selective single qubit operations are performed on one of two trapped ions with an average of 2.2+/-1.0% crosstalk. Coherence time as measured by the spin-echo technique is unaffected by the field gradient.',\n",
       " '  Quantum simulations of spin systems could enable the solution of problems which otherwise require infeasible classical resources. Such a simulation may be implemented using a well-controlled system of effective spins, such as a two-dimensional lattice of locally interacting ions. We propose here a layered planar rf trap design that can be used to create arbitrary two-dimensional lattices of ions. The design also leads naturally to ease of microfabrication. As a first experimental demonstration, we confine strontium-88 ions in a mm-scale lattice trap and verify numerical models of the trap by measuring the motional frequencies. We also confine 440 nm diameter charged microspheres and observe ion-ion repulsion between ions in neighboring lattice sites. Our design, when scaled to smaller ion-ion distances, is appropriate for quantum simulation schemes, e.g. that of Porras and Cirac (PRL 92 207901 (2004)). We note, however, that in practical realizations of the trap, an increase in the secular frequency with decreasing ion spacing may make a coupling rate that is large relative to the decoherence rate in such a trap difficult to achieve.',\n",
       " '  The codeword stabilized (CWS) quantum codes formalism presents a unifying approach to both additive and nonadditive quantum error-correcting codes (arXiv:0708.1021 [quant-ph]), but only for binary states. Here we generalize the CWS framework to the nonbinary case (of both prime and nonprime dimension) and map the search for nonbinary quantum codes to a corresponding search problem for classical nonbinary codes with specific error patterns. We show that while the additivity properties of nonbinary CWS codes are similar to the binary case, the structural properties of the nonbinary codes differ substantially from the binary case, even for prime dimensions. In particular, we identify specific structure patterns of stabilizer groups, based on which efficient constructions might be possible for codes that encode more dimensions than any stabilizer codes of the same length and distance; similar methods cannot be applied in the binary case. Understanding of these structural properties can help prune the search space and facilitate the identification of good nonbinary CWS codes.',\n",
       " '  Electric field noise from fluctuating patch potentials is a significant problem for a broad range of precision experiments, including trapped ion quantum computation and single spin detection. Recent results demonstrated strong suppression of this noise by cryogenic cooling, suggesting an underlying thermal process. We present measurements characterizing the temperature and frequency dependence of the noise from 7 to 100 K, using a single Sr+ ion trapped 75 um above the surface of a gold plated surface electrode ion trap. The noise amplitude is observed to have an approximate 1/f spectrum around 1 MHz, and grows rapidly with temperature as T^beta for beta from 2 to 4. The data are consistent with microfabricated cantilever measurements of non-contact friction but do not extrapolate to the DC measurements with neutral atoms or contact potential probes.',\n",
       " '  The codeword stabilized (\"CWS\") quantum codes formalism presents a unifying approach to both additive and nonadditive quantum error-correcting codes (arXiv:0708.1021). This formalism reduces the problem of constructing such quantum codes to finding a binary classical code correcting an error pattern induced by a graph state. Finding such a classical code can be very difficult. Here, we consider an algorithm which maps the search for CWS codes to a problem of identifying maximum cliques in a graph. While solving this problem is in general very hard, we prove three structure theorems which reduce the search space, specifying certain admissible and optimal ((n,K,d)) additive codes. In particular, we find there does not exist any ((7,3,3)) CWS code though the linear programming bound does not rule it out. The complexity of the CWS search algorithm is compared with the contrasting method introduced by Aggarwal and Calderbank (arXiv:cs/0610159).',\n",
       " '  A long-standing open problem in fault-tolerant quantum computation has been to find a universal set of transversal gates. As three of us proved in arXiv: 0706.1382, such a set does not exist for binary stabilizer codes. Here we generalize our work to show that for subsystem stabilizer codes in $d$ dimensional Hilbert space, such a universal set of transversal gates cannot exist for even one encoded qudit, for any dimension $d$, prime or nonprime. This result strongly supports the idea that other primitives, such as quantum teleportation, are necessary for universal fault-tolerant quantum computation, and may be an important factor for fault tolerance noise thresholds.',\n",
       " \"  Teleportation is a crucial element in fault-tolerant quantum computation and a complete understanding of its capacity is very important for the practical implementation of optimal fault-tolerant architectures. It is known that stabilizer codes support a natural set of gates that can be more easily implemented by teleportation than any other gates. These gates belong to the so called $\\\\mathcal{C}_k$ hierarchy introduced by Gottesman and Chuang (Nature \\\\textbf{402}, 390). Moreover, a subset of $\\\\mathcal{C}_k$ gates, called semi-Clifford operations, can be implemented by an even simpler architecture than the traditional teleportation setup (Phys. Rev. \\\\textbf{A62}, 052316). However, the precise set of gates in $\\\\mathcal{C}_k$ remains unknown, even for a fixed number of qubits $n$, which prevents us from knowing exactly what teleportation is capable of. In this paper we study the structure of $\\\\mathcal{C}_k$ in terms of semi-Clifford operations, which send by conjugation at least one maximal abelian subgroup of the $n$-qubit Pauli group into another one. We show that for $n=1,2$, all the $\\\\mathcal{C}_k$ gates are semi-Clifford, which is also true for $\\\\{n=3,k=3\\\\}$. However, this is no longer true for $\\\\{n>2,k>3\\\\}$. To measure the capability of this teleportation primitive, we introduce a quantity called `teleportation depth', which characterizes how many teleportation steps are necessary, on average, to implement a given gate. We calculate upper bounds for teleportation depth by decomposing gates into both semi-Clifford $\\\\mathcal{C}_k$ gates and those $\\\\mathcal{C}_k$ gates beyond semi-Clifford operations, and compare their efficiency.\",\n",
       " '  Dense arrays of trapped ions provide one way of scaling up ion trap quantum information processing. However, miniaturization of ion traps is currently limited by sharply increasing motional state decoherence at sub-100 um ion-electrode distances. We characterize heating rates in cryogenically cooled surface-electrode traps, with characteristic sizes in 75 um to 150 um range. Upon cooling to 6 K, the measured rates are suppressed by 7 orders of magnitude, two orders of magnitude below previously published data of similarly sized traps operated at room temperature. The observed noise depends strongly on fabrication process, which suggests further improvements are possible.',\n",
       " '  We demonstrate loading by laser ablation of $^{88}$Sr$^+$ ions into a mm-scale surface-electrode ion trap. The laser used for ablation is a pulsed, frequency-tripled Nd:YAG with pulse energies of 1-10 mJ and durations of 3-5 ns. An additional laser is not required to photoionize the ablated material. The efficiency and lifetime of several candidate materials for the laser ablation target are characterized by measuring the trapped ion fluorescence signal for a number of consecutive loads. Additionally, laser ablation is used to load traps with a trap depth (40 meV) below where electron impact ionization loading is typically successful ($\\\\gtrsim$ 500 meV).',\n",
       " '  Certain quantum codes allow logic operations to be performed on the encoded data, such that a multitude of errors introduced by faulty gates can be corrected. An important class of such operations are {\\\\em transversal}, acting bitwise between corresponding qubits in each code block, thus allowing error propagation to be carefully limited. If any quantum operation could be implemented using a set of such gates, the set would be {\\\\em universal}; codes with such a universal, transversal gate set have been widely desired for efficient fault-tolerant quantum computation. We study the structure of GF(4)-additive quantum codes and prove that no universal set of transversal logic operations exists for these codes. This result strongly supports the idea that additional primitive operations, based for example on quantum teleportation, are necessary to achieve universal fault-tolerant computation on additive codes.',\n",
       " '  We produce large numbers of low-energy ions by photoionization of laser-cooled atoms inside a surface-electrode-based Paul trap. The isotope-selective trap loading rate of $4\\\\times10^{5}$ Yb$^{+}$ ions/s exceeds that attained by photoionization (electron impact ionization) of an atomic beam by four (six) orders of magnitude. Traps as shallow as 0.13 eV are easily loaded with this technique. The ions are confined in the same spatial region as the laser-cooled atoms, which will allow the experimental investigation of interactions between cold ions and cold atoms or Bose-Einstein condensates.',\n",
       " \"  The equivalence of stabilizer states under local transformations is of fundamental interest in understanding properties and uses of entanglement. Two stabilizer states are equivalent under the usual stochastic local operations and classical communication criterion if and only if they are equivalent under local unitary (LU) operations. More surprisingly, under certain conditions, two LU equivalent stabilizer states are also equivalent under local Clifford (LC) operations, as was shown by Van den Nest et al. [Phys. Rev. \\\\textbf{A71}, 062323]. Here, we broaden the class of stabilizer states for which LU equivalence implies LC equivalence ($LU\\\\Leftrightarrow LC$) to include all stabilizer states represented by graphs with neither cycles of length 3 nor 4. To compare our result with Van den Nest et al.'s, we show that any stabilizer state of distance $Î´=2$ is beyond their criterion. We then further prove that $LU\\\\Leftrightarrow LC$ holds for a more general class of stabilizer states of $Î´=2$. We also explicitly construct graphs representing $Î´>2$ stabilizer states which are beyond their criterion: we identify all 58 graphs with up to 11 vertices and construct graphs with $2^m-1$ ($m\\\\geq 4$) vertices using quantum error correcting codes which have non-Clifford transversal gates.\",\n",
       " \"  The assumption of maximum parallelism support for the successful realization of scalable quantum computers has led to homogeneous, ``sea-of-qubits'' architectures. The resulting architectures overcome the primary challenges of reliability and scalability at the cost of physically unacceptable system area. We find that by exploiting the natural serialization at both the application and the physical microarchitecture level of a quantum computer, we can reduce the area requirement while improving performance. In particular we present a scalable quantum architecture design that employs specialization of the system into memory and computational regions, each individually optimized to match hardware support to the available parallelism. Through careful application and system analysis, we find that our new architecture can yield up to a factor of thirteen savings in area due to specialization. In addition, by providing a memory hierarchy design for quantum computers, we can increase time performance by a factor of eight. This result brings us closer to the realization of a quantum processor that can solve meaningful problems.\",\n",
       " '  We demonstrate a method for loading surface electrode ion traps by electron impact ionization. The method relies on the property of surface electrode geometries that the trap depth can be increased at the cost of more micromotion. By introducing a buffer gas, we can counteract the rf heating assocated with the micromotion and benefit from the larger trap depth. After an initial loading of the trap, standard compensation techniques can be used to cancel the stray fields resulting from charged dielectric and allow for the loading of the trap at ultra-high vacuum.',\n",
       " '  Quantum simulation uses a well-known quantum system to predict the behavior of another quantum system. Certain limitations in this technique arise, however, when applied to specific problems, as we demonstrate with a theoretical and experimental study of an algorithm to find the low-lying spectrum of a Hamiltonian. While the number of elementary quantum gates does scale polynomially with the size of the system, it increases inversely to the desired error bound $Îµ$. Making such simulations robust to decoherence using fault-tolerance constructs requires an additional factor of $1/ Îµ$ gates. These constraints are illustrated by using a three qubit nuclear magnetic resonance system to simulate a pairing Hamiltonian, following the algorithm proposed by Wu, Byrd, and Lidar.',\n",
       " '  We present an efficient family of quantum circuits for a fundamental primitive in quantum information theory, the Schur transform. The Schur transform on n d-dimensional quantum systems is a transform between a standard computational basis to a labelling related to the representation theory of the symmetric and unitary groups. If we desire to implement the Schur transform to an accuracy of epsilon, then our circuit construction uses a number of gates which is polynomial in n, d and log(1/epsilon). The important insights we use to perform this construction are the selection of the appropriate subgroup adapted basis and the Wigner-Eckart theorem. Our efficient circuit construction renders numerous protocols in quantum information theory computationally tractable and is an important new efficient quantum circuit family which goes significantly beyond the standard paradigm of the quantum Fourier transform.',\n",
       " '  Recent experimental advances have demonstrated technologies capable of supporting scalable quantum computation. A critical next step is how to put those technologies together into a scalable, fault-tolerant system that is also feasible. We propose a Quantum Logic Array (QLA) microarchitecture that forms the foundation of such a system. The QLA focuses on the communication resources necessary to efficiently support fault-tolerant computations. We leverage the extensive groundwork in quantum error correction theory and provide analysis that shows that our system is both asymptotically and empirically fault tolerant. Specifically, we use the QLA to implement a hierarchical, array-based design and a logarithmic expense quantum-teleportation communication protocol. Our goal is to overcome the primary scalability challenges of reliability, communication, and quantum resource distribution that plague current proposals for large-scale quantum computing.',\n",
       " '  The role of mixed state entanglement in liquid-state nuclear magnetic resonance (NMR) quantum computation is not yet well-understood. In particular, despite the success of quantum information processing with NMR, recent work has shown that quantum states used in most of those experiments were not entangled. This is because these states, derived by unitary transforms from the thermal equilibrium state, were too close to the maximally mixed state. We are thus motivated to determine whether a given NMR state is entanglable - that is, does there exist a unitary transform that entangles the state? The boundary between entanglable and nonentanglable thermal states is a function of the spin system size $N$ and its temperature $T$. We provide new bounds on the location of this boundary using analytical and numerical methods; our tightest bound scales as $N \\\\sim T$, giving a lower bound requiring at least $N \\\\sim 22,000$ proton spins to realize an entanglable thermal state at typical laboratory NMR magnetic fields. These bounds are tighter than known bounds on the entanglability of effective pure states.',\n",
       " '  Nuclear Magnetic Resonance (NMR) has provided a valuable experimental testbed for quantum information processing (QIP). Here, we briefly review the use of nuclear spins as qubits, and discuss the current status of NMR-QIP. Advances in the techniques available for control are described along with the various implementations of quantum algorithms and quantum simulations that have been performed using NMR. The recent application of NMR control techniques to other quantum computing systems are reviewed before concluding with a description of the efforts currently underway to transition to solid state NMR systems that hold promise for scalable architectures.',\n",
       " '  The Schur basis on n d-dimensional quantum systems is a generalization of the total angular momentum basis that is useful for exploiting symmetry under permutations or collective unitary rotations. We present efficient (size poly(n,d,log(1/Îµ)) for accuracy Îµ) quantum circuits for the Schur transform, which is the change of basis between the computational and the Schur bases. These circuits are based on efficient circuits for the Clebsch-Gordan transformation. We also present an efficient circuit for a limited version of the Schur transform in which one needs only to project onto different Schur subspaces. This second circuit is based on a generalization of phase estimation to any nonabelian finite group for which there exists a fast quantum Fourier transform.',\n",
       " '  Systematic errors in quantum operations can be the dominating source of imperfection in achieving control over quantum systems. This problem, which has been well studied in nuclear magnetic resonance, can be addressed by replacing single operations with composite sequences of pulsed operations, which cause errors to cancel by symmetry. Remarkably, this can be achieved without knowledge of the amount of error epsilon. Independent of the initial state of the system, current techniques allow the error to be reduced to O(epsilon^3). Here, we extend the composite pulse technique to cancel errors to O(epsilon^n), for arbitrary n.',\n",
       " '  We define a multi-partite entanglement measure for stabilizer states, which can be computed efficiently from a set of generators of the stabilizer group. Our measure applies to qubits, qudits and continuous variables.',\n",
       " '  Fifty years of developments in nuclear magnetic resonance (NMR) have resulted in an unrivaled degree of control of the dynamics of coupled two-level quantum systems. This coherent control of nuclear spin dynamics has recently been taken to a new level, motivated by the interest in quantum information processing. NMR has been the workhorse for the experimental implementation of quantum protocols, allowing exquisite control of systems up to seven qubits in size. Here, we survey and summarize a broad variety of pulse control and tomographic techniques which have been developed for and used in NMR quantum computation. Many of these will be useful in other quantum systems now being considered for implementation of quantum information processing tasks.',\n",
       " '  We report the realization of a nuclear magnetic resonance computer with three quantum bits that simulates an adiabatic quantum optimization algorithm. Adiabatic quantum algorithms offer new insight into how quantum resources can be used to solve hard problems. This experiment uses a particularly well suited three quantum bit molecule and was made possible by introducing a technique that encodes general instances of the given optimization problem into an easily applicable Hamiltonian. Our results indicate an optimal run time of the adiabatic algorithm that agrees well with the prediction of a simple decoherence model.',\n",
       " \"  The number of steps any classical computer requires in order to find the prime factors of an $l$-digit integer $N$ increases exponentially with $l$, at least using algorithms known at present. Factoring large integers is therefore conjectured to be intractable classically, an observation underlying the security of widely used cryptographic codes. Quantum computers, however, could factor integers in only polynomial time, using Shor's quantum factoring algorithm. Although important for the study of quantum computers, experimental demonstration of this algorithm has proved elusive. Here we report an implementation of the simplest instance of Shor's algorithm: factorization of ${N=15}$ (whose prime factors are 3 and 5). We use seven spin-1/2 nuclei in a molecule as quantum bits, which can be manipulated with room temperature liquid state nuclear magnetic resonance techniques. This method of using nuclei to store quantum information is in principle scalable to many quantum bit systems, but such scalability is not implied by the present work. The significance of our work lies in the demonstration of experimental and theoretical techniques for precise control and modelling of complex quantum computers. In particular, we present a simple, parameter-free but predictive model of decoherence effects in our system.\",\n",
       " '  Quantum compiling addresses the problem of approximating an arbitrary quantum gate with a string of gates drawn from a particular finite set. It has been shown that this is possible for almost all choices of base sets and furthermore that the number of gates required for precision epsilon is only polynomial in log 1/epsilon. Here we prove that using certain sets of base gates quantum compiling requires a string length that is linear in log 1/epsilon, a result which matches the lower bound from counting volume up to constant factor.',\n",
       " \"  Current experiments in liquid-state nuclear magnetic resonance quantum computing are limited by low initial polarization. To address this problem, we have investigated the use of optical pumping techniques to enhance the polarization of a 2-qubit NMR quantum computer (13C and 1H in 13CHCl3). To efficiently use the increased polarization, we have generalized the procedure for effective pure state preparation. With this new, more flexible scheme, an effective pure state was prepared with polarization-enhancement of a factor of 10 compared to the thermal state. An implementation of Grover's quantum search algorithm was demonstrated using this new technique.\",\n",
       " '  We present a quantum digital signature scheme whose security is based on fundamental principles of quantum physics. It allows a sender (Alice) to sign a message in such a way that the signature can be validated by a number of different people, and all will agree either that the message came from Alice or that it has been tampered with. To accomplish this task, each recipient of the message must have a copy of Alice\\'s \"public key,\" which is a set of quantum states whose exact identity is known only to Alice. Quantum public keys are more difficult to deal with than classical public keys: for instance, only a limited number of copies can be in circulation, or the scheme becomes insecure. However, in exchange for this price, we achieve unconditionally secure digital signatures. Sending an m-bit message uses up O(m) quantum bits for each recipient of the public key. We briefly discuss how to securely distribute quantum public keys, and show the signature scheme is absolutely secure using one method of key distribution. The protocol provides a model for importing the ideas of classical public key cryptography into the quantum world.',\n",
       " '  This in an introduction on quantum computing and on the use of NMR to build quantum computers, geared towards an NMR audience.',\n",
       " '  Quantum process tomography is a procedure by which the unknown dynamical evolution of an open quantum system can be fully experimentally characterized. We demonstrate explicitly how this procedure can be implemented with a nuclear magnetic resonance quantum computer. This allows us to measure the fidelity of a controlled-not logic gate and to experimentally investigate the error model for our computer. Based on the latter analysis, we test an important assumption underlying nearly all models of quantum error correction, the independence of errors on different qubits.',\n",
       " '  Although the conditions for performing arbitrary unitary operations to simulate the dynamics of a closed quantum system are well understood, the same is not true of the more general class of quantum operations (also known as superoperators) corresponding to the dynamics of open quantum systems. We propose a framework for the generation of Markovian quantum dynamics and study the resources needed for universality. For the case of a single qubit, we show that a single nonunitary process is necessary and sufficient to generate all unital Markovian quantum dynamics, whereas a set of processes parametrized by one continuous parameter is needed in general. We also obtain preliminary results for the unital case in higher dimensions.',\n",
       " '  Although the initial proposal for ion trap quantum computation made use of an auxiliary internal level to perform logic between ions, this resource is not necessary in principle. Instead, one may perform such operations directly using sideband laser pulses, operating with an arbitrary (sufficiently small) Lamb-Dicke parameter. We explore the potential of this technique, showing how to perform logical operations between the internal state of an ion and the collective motional state and giving explicit constructions for a controlled-not gate between ions.',\n",
       " \"  We report the realization of a nuclear magnetic resonance (NMR) quantum computer which combines the quantum Fourier transform (QFT) with exponentiated permutations, demonstrating a quantum algorithm for order-finding. This algorithm has the same structure as Shor's algorithm and its speed-up over classical algorithms scales exponentially. The implementation uses a particularly well-suited five quantum bit molecule and was made possible by a new state initialization procedure and several quantum control techniques.\",\n",
       " '  The clock synchronization problem is to determine the time difference $Î”$ between two spatially separated clocks. When message delivery times between the two clocks are uncertain, $O(2^{2n})$ classical messages must be exchanged between the clocks to determine $n$ digits of $Î”$. On the other hand, as we show, there exists a quantum algorithm to obtain $n$ digits of $Î”$ while communicating only O(n) quantum messages.',\n",
       " '  We present a general method to construct fault-tolerant quantum logic gates with a simple primitive, which is an analog of quantum teleportation. The technique extends previous results based on traditional quantum teleportation (Gottesman and Chuang, Nature {\\\\bf 402}, 390, 1999) and leads to straightforward and systematic construction of many fault-tolerant encoded operations, including the $Ï€/8$ and Toffoli gates. The technique can also be applied to the construction of remote quantum operations that cannot be directly performed.',\n",
       " \"  We report the experimental implementation of Grover's quantum search algorithm on a quantum computer with three quantum bits. The computer consists of molecules of $^{13}$C-labeled CHFBr$_2$, in which the three weakly coupled spin-1/2 nuclei behave as the bits and are initialized, manipulated, and read out using magnetic resonance techniques. This quantum computation is made possible by the introduction of two techniques which significantly reduce the complexity of the experiment and by the surprising degree of cancellation of systematic errors which have previously limited the total possible number of quantum gates.\",\n",
       " '  We present a method to create a variety of interesting gates by teleporting quantum bits through special entangled states. This allows, for instance, the construction of a quantum computer based on just single qubit operations, Bell measurements, and GHZ states. We also present straightforward constructions of a wide variety of fault-tolerant quantum gates.',\n",
       " '  Liquid crystals offer several advantages as solvents for molecules used for nuclear magnetic resonance quantum computing (NMRQC). The dipolar coupling between nuclear spins manifest in the NMR spectra of molecules oriented by a liquid crystal permits a significant increase in clock frequency, while short spin-lattice relaxation times permit fast recycling of algorithms, and save time in calibration and signal-enhancement experiments. Furthermore, the use of liquid crystal solvents offers scalability in the form of an expanded library of spin-bearing molecules suitable for NMRQC. These ideas are demonstrated with the successful execution of a 2-qubit Grover search using a molecule ($^{13}$C$^{1}$HCl$_3$) oriented in a liquid crystal and a clock speed eight times greater than in an isotropic solvent. Perhaps more importantly, five times as many logic operations can be executed within the coherence time using the liquid crystal solvent.',\n",
       " \"  Realistic physical implementations of quantum computers can entail tradeoffs which depart from the ideal model of quantum computation. Although these tradeoffs have allowed successful demonstration of certain quantum algorithms, a crucial question is whether they fundamentally limit the computational capacity of such machines. We study the limitations of a quantum computation model in which only ensemble averages of measurement observables are accessible. Furthermore, we stipulate that input qubits may only be prepared in highly random, ``hot'' mixed states. In general, these limitations are believed to dramatically detract from the computational power of the system. However, we construct a class of algorithms for this limited model, which, surprisingly, are polynomially equivalent to the ideal case. This class includes the well known Deutsch-Jozsa algorithm.\",\n",
       " '  We report the first use of \"logical labeling\" to perform a quantum computation with a room-temperature bulk system. This method entails the selection of a subsystem which behaves as if it were at zero temperature - except for a decrease in signal strength - conditioned upon the state of the remaining system. No averaging over differently prepared molecules is required. In order to test this concept, we execute a quantum search algorithm in a subspace of two nuclear spins, labeled by a third spin, using solution nuclear magnetic resonance (NMR), and employing a novel choice of reference frame to uncouple nuclei.',\n",
       " '  We present an efficient scheme which couples any designated pair of spins in heteronuclear spin systems. The scheme is based on the existence of Hadamard matrices. For a system of $n$ spins with pairwise coupling, the scheme concatenates $cn$ intervals of system evolution and uses at most $c n^2$ pulses where $c \\\\approx 1$. Our results demonstrate that, in many systems, selective recoupling is possible with linear overhead, contrary to common speculation that exponential effort is always required.',\n",
       " '  Using nuclear magnetic resonance techniques, we experimentally investigated the effects of applying a two bit phase error detection code to preserve quantum information in nuclear spin systems. Input states were stored with and without coding, and the resulting output states were compared with the originals and with each other. The theoretically expected result, net reduction of distortion and conditional error probabilities to second order, was indeed observed, despite imperfect coding operations which increased the error probabilities by approximately 5%. Systematic study of the deviations from the ideal behavior provided quantitative measures of different sources of error, and good agreement was found with a numerical model. Theoretical questions in quantum error correction in bulk nuclear spin systems including fidelity measures, signal strength and syndrome measurements are discussed.',\n",
       " \"  Nuclear magnetic resonance techniques are used to realize a quantum algorithm experimentally. The algorithm allows a simple NMR quantum computer to determine global properties of an unknown function requiring fewer function ``calls'' than is possible using a classical computer.\",\n",
       " '  In bulk quantum computation one can manipulate a large number of indistinguishable quantum computers by parallel unitary operations and measure expectation values of certain observables with limited sensitivity. The initial state of each computer in the ensemble is known but not pure. Methods for obtaining effective pure input states by a series of manipulations have been described by Gershenfeld and Chuang (logical labeling) and Cory et al. (spatial averaging) for the case of quantum computation with nuclear magnetic resonance. We give a different technique called temporal averaging. This method is based on classical randomization, requires no ancilla qubits and can be implemented in nuclear magnetic resonance without using gradient fields. We introduce several temporal averaging algorithms suitable for both high temperature and low temperature bulk quantum computing and analyze the signal to noise behavior of each.',\n",
       " '  We show how to construct quantum gate arrays that can be programmed to perform different unitary operations on a data register, depending on the input to some program register. It is shown that a universal quantum gate array - a gate array which can be programmed to perform any unitary operation - exists only if one allows the gate array to operate in a probabilistic fashion. The universal quantum gate array we construct requires an exponentially smaller number of gates than a classical universal gate array.',\n",
       " '  Traditional quantum error correction involves the redundant encoding of k quantum bits using n quantum bits to allow the detection and correction of any t bit error. The smallest general t=1 code requires n=5 for k=1. However, the dominant error process in a physical system is often well known, thus inviting the question: given a specific error model, can more efficient codes be devised? We demonstrate new codes which correct just amplitude damping errors which allow, for example, a t=1, k=1 code using effectively n=4.6. Our scheme is based on using bosonic states of photons in a finite number of optical modes. We present necessary and sufficient conditions for the codes, and describe construction algorithms, physical implementation, and performance bounds.',\n",
       " '  We give an explicit prescription for experimentally determining the evolution operators which completely describe the dynamics of a quantum mechanical black box -- an arbitrary open quantum system. We show necessary and sufficient conditions for this to be possible, and illustrate the general theory by considering specifically one and two quantum bit systems. These procedures may be useful in the comparative evaluation of experimental quantum measurement, communication, and computation systems.',\n",
       " \"  Decoherence and loss will limit the practicality of quantum cryptography and computing unless successful error correction techniques are developed. To this end, we have discovered a new scheme for perfectly detecting and rejecting the error caused by loss (amplitude damping to a reservoir at T=0), based on using a dual-rail representation of a quantum bit. This is possible because (1) balanced loss does not perform a ``which-path'' measurement in an interferometer, and (2) balanced quantum nondemolition measurement of the ``total'' photon number can be used to detect loss-induced quantum jumps without disturbing the quantum coherence essential to the quantum bit. Our results are immediately applicable to optical quantum computers using single photonics devices.\",\n",
       " '  The construction of large, coherent quantum systems necessary for quantum computation remains an entreating but elusive goal, due to the ubiquitous nature of decoherence. Recent progress in quantum error correction schemes have given new hope to this field, but thus far, the codes presented in the literature assume a restricted number of errors and error free encoding, decoding, and measurement. We investigate a specific scenario without these assumptions; in particular, we evaluate a scheme to preserve a single quantum bit against phase damping using a three-qubit encoding based on Shor. By applying a new formalism which gives simple operators for decoherence and noisy logic gates, we find the fidelity of the stored qubit as a function of time, including decoherence which occurs not only during storage but also during processing. We generalize our results to include any source of error, and derive an upper limit on the allowable decoherence per timestep. Physically, our results suggest the feasibility of engineering artificial metastable states through repeated error correction.',\n",
       " '  We investigate the impact of loss (amplitude damping) and decoherence (phase damping) on the performance of a simple quantum computer which solves the one-bit Deutsch problem. The components of this machine are beamsplitters and nonlinear optical Kerr cells, but errors primarily originate from the latter. We develop models to describe the effect of these errors on a quantum optical Fredkin gate. The results are used to analyze possible error correction strategies in a complete quantum computer. We find that errors due to loss can be avoided perfectly by appropriate design techniques, while decoherence can be partially dealt with using projective error correction.',\n",
       " '  Recent progress in quantum cryptography and quantum computers has given hope to their imminent practical realization. An essential element at the heart of the application of these quantum systems is a quantum error correction scheme. We propose a new technique based on the use of coding in order to detect and correct errors due to imperfect transmission lines in quantum cryptography or memories in quantum computers. We give a particular example of how to detect a decohered qubit in order to transmit or preserve with high fidelity the original qubit.',\n",
       " \"  We propose an implementation of a quantum computer to solve Deutsch's problem, which requires exponential time on a classical computer but only linear time with quantum parallelism. By using a dual-rail qubit representation as a simple form of error correction, our machine can tolerate some amount of decoherence and still give the correct result with high probability. The design which we employ also demonstrates a signature for quantum parallelism which unambiguously delineates the desired quantum behavior from the merely classical. The experimental demonstration of our proposal using quantum optical components calls for the development of several key technologies common to single photonics.\",\n",
       " \"The design of data markets has gained in importance as firms increasingly use predictions from machine learning models to make their operations more effective, yet need to externally acquire the necessary training data to fit such models. A property of such markets that has been given limited consideration thus far is the externality faced by a firm when data is allocated to other, competing firms. Addressing this is likely necessary for progress towards the practical implementation of such markets. In this work, we consider the case with $n$ competing firms and a monopolistic data seller. We demonstrate that modeling the utility of firms solely through the increase in prediction accuracy experienced reduces the complex, combinatorial problem of allocating and pricing multiple data sets to an auction of a single digital (freely replicable) good. Crucially, this is what enables us to model the negative externalities experienced by a firm resulting from other firms' allocations. We obtain forms of the welfare-maximizing and revenue-maximizing auctions for such settings. We highlight how the form of the firms' private information -- whether they know the externalities they exert on others or that others exert on them -- affects the structure of the optimal mechanisms. We find that in all cases, the optimal allocation rules turn out to be single thresholds (one per firm), in which the seller allocates all information or none of it to a firm. We note the framework and results introduced hold more broadly for the auction of digital goods with externalities.\",\n",
       " \"Smart buildings have great potential for shaping an energy-efficient, sustainable, and more economic future for our planet as buildings account for approximately 40% of the global energy consumption. A key challenge for large-scale plug and play deployment of the smart building technology is the ability to learn a good control policy in a short period of time, i.e. having a low sample complexity for the learning control agent. Motivated by this problem and to remedy the issue of high sample complexity in the general context of cyber-physical systems, we propose an event-triggered paradigm for learning and control with variable-time intervals, as opposed to the traditional constant-time sampling. The events occur when the system state crosses the a priori-parameterized switching manifolds; this crossing triggers the learning as well as the control processes. Policy gradient and temporal difference methods are employed to learn the optimal switching manifolds which define the optimal control policy. We propose two event-triggered learning algorithms for stochastic and deterministic control policies. We show the efficacy of our proposed approach via designing a smart learning thermostat for autonomous micro-climate control in buildings. The event-triggered algorithms are implemented on a single-zone building to decrease buildings' energy consumption as well as to increase occupants' comfort. Simulation results confirm the efficacy and improved sample efficiency of the proposed event-triggered approach for online learning and control.\",\n",
       " 'We propose a means to relate properties of an interconnected system to its separate component systems in the presence of cascade-like phenomena. Building on a theory of interconnection reminiscent of the behavioral approach to systems theory, we introduce the notion of generativity, and its byproduct, generative effects. Cascade effects, enclosing contagion phenomena and cascading failures, are seen as instances of generative effects. The latter are precisely the instances where properties of interest are not preserved or behave very badly when systems interact. The goal is to overcome that obstruction. We will show how to extract mathematical objects from the systems, that encode their generativity: their potential to generate new phenomena upon interaction. Those objects may then be used to link the properties of the interconnected system to its separate systems. Such a link will be executed through the use of exact sequences from commutative algebra.',\n",
       " 'We revisit the behavioral approach to systems theory and make explicit the abstract pattern that governs it. Our end goal is to use that pattern to understand interaction-related phenomena that emerge when systems interact. Rather than thinking of a system as a pair $(\\\\mathbb{U}, \\\\mathcal{B})$, we begin by thinking of it as an injective map from $\\\\mathcal{B} \\\\rightarrow \\\\mathbb{U}$. This relative perspective naturally brings about the sought structure, which we summarize in three points. First, the separation of behavioral equations and behavior is developed through two spaces, one of syntax and another of semantics, linked by an interpretation map. Second, the notion of interconnection and variable sharing is shown to be a construction of the same nature as that of gluing topological spaces or taking amalgamated sums of algebraic objects. Third, the notion of interconnection instantiates to both the syntax space and the semantics space, and the interpretation map is shown to preserve the interconnection when going from syntax to semantics. This pattern, in its generality, is made precise by borrowing very basic constructs from the language of categories and functors.',\n",
       " 'We argue that the mathematical structure, enabling certain cascading and emergent phenomena to intuitively emerge, coincides with Galois connections. We introduce the notion of generative effects to formally capture such phenomena. We establish that these effects arise, via a notion of a veil, from either concealing mechanisms in a system or forgetting characteristics from it. The goal of the work is to initiate a mathematical base that enables us to further study such phenomena. In particular, generative effects can be further linked to a certain loss of exactness. Homological algebra, and related algebraic methods, may then be used to characterize the effects.',\n",
       " 'We review selected results related to robustness of networked systems in finite and asymptotically large size regimes, under static and dynamical settings. In the static setting, within the framework of flow over finite networks, we discuss the effect of physical constraints on robustness to loss in link capacities. In the dynamical setting, we review several settings in which small gain type analysis provides tight robustness guarantees for linear dynamics over finite networks towards worst-case and stochastic disturbances. We also discuss network flow dynamic settings where nonlinear techniques facilitate in understanding the effect on robustness of constraints on capacity and information, substituting information with control action, and cascading failure. We also contrast the latter with a representative contagion model. For asymptotically large networks, we discuss the role of network properties in connecting microscopic shocks to emergent macroscopic fluctuations under linear dynamics as well as for economic networks at equilibrium. Through the review of these results, the paper aims to achieve two objectives. First, to highlight selected settings in which the role of interconnectivity structure of a network on its robustness is well-understood. Second, to highlight a few additional settings in which existing system theoretic tools give tight robustness guarantees, and which are also appropriate avenues for future network-theoretic investigations.',\n",
       " 'We address the problem of learning the parameters of a mean square stable switched linear systems(SLS) with unknown latent space dimension, or \\\\textit{order}, from its noisy input--output data. In particular, we focus on learning a good lower order approximation of the underlying model allowed by finite data. This is achieved by constructing Hankel-like matrices from data and obtaining suitable approximations via SVD truncation where the threshold for SVD truncation is purely data dependent. By exploiting tools from theory of model reduction for SLS, we find that the system parameter estimates are close to a balanced truncated realization of the underlying system with high probability.',\n",
       " \"Personal data is essential in showing users targeted ads - the economic backbone of the web. Still, there are major inefficiencies in how data is transacted online: (1) users don't decide what information is released nor get paid for this privacy loss; (2) algorithmic advertisers are stuck in inefficient long-term contracts where they purchase user data without knowing the value it provides. This paper proposes a system, Zorro, which aims to rectify aforementioned two problems.\\n  As the main contribution, we provide a natural, 'absolute' definition of 'Value of Data' (VoD) - for any quantity of interest, it is the delta between an individual's value and population mean. The challenge remains how to operationalize this definition, independently of a buyer's model for VoD. We propose a model-agnostic solution, relying on matrix estimation, and use it to estimate click-through-rate (CTR), as an example.\\n  Regarding (2), Zorro empowers advertisers to measure value of user data on a query-by-query basis and based only on the increase in accuracy it provides in estimating CTR. In contrast advertisers currently engage in inefficient long-term data contracts with third party data sellers. We highlight two results on a large ad-click dataset: (i) our system has R^2=0.58, in line with best-in-class results for related problems (e.g. content recommendation). Crucially, our system is model-agnostic - we estimate CTR without accessing an advertiser's proprietary models, a required property of any such pricing system;(ii) our experiments show selling user data has incremental value ranging from 30%-69% depending on ad category. Roughly, this translates to at least USD 16 Billion loss in value for advertisers if user data is not provided.\\n  Regarding (1), in addition to allowing users to get paid for data sharing, we extend our mathematical framework to when users provide explicit intent.\",\n",
       " 'We address the problem of learning the parameters of a stable linear time invariant (LTI) system or linear dynamical system (LDS) with unknown latent space dimension, or order, from a single time--series of noisy input-output data. We focus on learning the best lower order approximation allowed by finite data. Motivated by subspace algorithms in systems theory, where the doubly infinite system Hankel matrix captures both order and good lower order approximations, we construct a Hankel-like matrix from noisy finite data using ordinary least squares. This circumvents the non-convexities that arise in system identification, and allows accurate estimation of the underlying LTI system. Our results rely on careful analysis of self-normalized martingale difference terms that helps bound identification error up to logarithmic factors of the lower bound. We provide a data-dependent scheme for order selection and find an accurate realization of system parameters, corresponding to that order, by an approach that is closely related to the Ho-Kalman subspace algorithm. We demonstrate that the proposed model order selection procedure is not overly conservative, i.e., for the given data length it is not possible to estimate higher order models or find higher order approximations with reasonable accuracy.',\n",
       " 'Spurred by the growth of transportation network companies and increasing data capabilities, vehicle routing and ride-matching algorithms can improve the efficiency of private transportation services. However, existing routing solutions do not address where drivers should travel after dropping off a passenger and before receiving the next passenger ride request, i.e., during the between-ride period. We address this problem by developing an efficient algorithm to find the optimal policy for drivers between rides in order to maximize driver profits. We model the road network as a graph, and we show that the between-ride routing problem is equivalent to a stochastic shortest path problem, an infinite dynamic program with no discounting. We prove under reasonable assumptions that an optimal routing policy exists that avoids cycles; policies of this type can be efficiently found. We present an iterative approach to find an optimal routing policy. Our approach can account for various factors, including the frequency of passenger ride requests at different locations, traffic conditions, and surge pricing. We demonstrate the effectiveness of the approach by implementing it on road network data from Boston and New York City.',\n",
       " 'This paper addresses two fundamental problems in the context of jump linear systems (JLS). The first problem is concerned with characterizing the minimal state space dimension solely from input-output pairs and without any knowledge of the number of mode switches. The second problem is concerned with characterizing the number of discrete modes of the JLS. For the first problem, we develop a linear system theory based approach and construct an appropriate Hankel-like matrix. The rank of this matrix gives us the state space dimension. For the second problem we show that minimal number of modes corresponds to the minimal rank of a positive semi-definite matrix obtained via a non--convex formulation.',\n",
       " \"In this work, we aim to design a data marketplace; a robust real-time matching mechanism to efficiently buy and sell training data for Machine Learning tasks. While the monetization of data and pre-trained models is an essential focus of industry today, there does not exist a market mechanism to price training data and match buyers to sellers while still addressing the associated (computational and other) complexity. The challenge in creating such a market stems from the very nature of data as an asset: (i) it is freely replicable; (ii) its value is inherently combinatorial due to correlation with signal in other data; (iii) prediction tasks and the value of accuracy vary widely; (iv) usefulness of training data is difficult to verify a priori without first applying it to a prediction task. As our main contributions we: (i) propose a mathematical model for a two-sided data market and formally define the key associated challenges; (ii) construct algorithms for such a market to function and analyze how they meet the challenges defined. We highlight two technical contributions: (i) a new notion of 'fairness' required for cooperative games with freely replicable goods; (ii) a truthful, zero regret mechanism to auction a class of combinatorial goods based on utilizing Myerson's payment function and the Multiplicative Weights algorithm. These might be of independent interest.\",\n",
       " 'We consider the problem of robustness in large consensus networks that occur in many areas such as distributed optimization. Robustness, in this context, is the scaling of performance measures, e.g. H2-norm, as a function of network dimension. We provide a formal framework to quantify the relation between such performance scaling and the convergence speed of the network. Specifically, we provide upper and lower bounds for the convergence speed in terms of robustness and discuss how these bounds scale with the network topology. The main contribution of this work is that we obtain tight bounds, that hold regardless of network topology. The work here also encompasses some results in convergence time analysis in previous literature.',\n",
       " 'This paper examines the dependence of network performance measures on network size and considers scaling results for large networks. We connect two performance measures that are well studied, but appear to be unrelated. The first measure is concerned with energy metrics, namely the $\\\\Hcal_2$--norm of a network, which arises in control theory applications. The second measure is concerned with the notion of \"tail risk\" which arises in economic and financial networks. We study the question of why such performance measures may deteriorate at a faster rate than the growth rate of the network. We first focus on the energy metric and its well known connection to controllability Gramian of the underlying dynamical system. We show that undirected networks exhibit the most graceful energy growth rates as network size grows. This rate is quantified completely by the proximity of spectral radius to unity or distance to instability. In contrast, we show that the simple characterization of energy in terms of network spectrum does not exist for directed networks. We demonstrate that, for any fixed distance to instability, energy of a directed network can grow at an exponentially faster rate. We provide general methods for manipulating networks to reduce energy. In particular, we prove that certain operations that increase the symmetry in a network cannot increase energy (in an order sense). Secondly, we focus on tail risk in economic and financial networks. In contrast to $\\\\Hcal_2$--norm which arises from computing the expectation of energy in the network, tail risk focuses on tail probability behavior of network variables. Although the two measures differ substantially we show that they are precisely connected through the system Gramian. This surprising result explains why topology considerations rather than specific performance measures dictate the large scale behavior of networks.',\n",
       " 'In coalitional games, traditional coalitional game theory does not apply if different participants hold different opinions about the payoff function that corresponds to each subset of the coalition. In this paper, we propose a framework in which players can exchange opinions about their views of payoff functions and then decide the distribution of the value of the grand coalition. When all players are truth-telling, the problem of opinion consensus is decoupled from the coalitional game, but interesting dynamics will arise when players are strategic in the consensus phase. Assuming that all players are rational, the model implies that, if influential players are risk-averse, an efficient fusion of the distributed data is achieved at pure strategy Nash equilibrium, meaning that the average opinion will not drift. Also, without the assumption that all players are rational, each player can use an algorithmic R-learning process, which gives the same result as the pure strategy Nash equilibrium with rational players.',\n",
       " \"This paper analyzes the impact of peer effects on electricity consumption of a network of rational, utility-maximizing users. Users derive utility from consuming electricity as well as consuming less energy than their neighbors. However, a disutility is incurred for consuming more than their neighbors. To maximize the profit of the load-serving entity that provides electricity to such users, we develop a two-stage game-theoretic model, where the entity sets the prices in the first stage. In the second stage, consumers decide on their demand in response to the observed price set in the first stage so as to maximize their utility. To this end, we derive theoretical statements under which such peer effects reduce aggregate user consumption. Further, we obtain expressions for the resulting electricity consumption and profit of the load serving entity for the case of perfect price discrimination and a single price under complete information, and approximations under incomplete information. Simulations suggest that exposing only a selected subset of all users to peer effects maximizes the entity's profit.\",\n",
       " 'Load-serving entities which procure electricity from the wholesale electricity market to service end-users face significant quantity and price risks due to the volatile nature of electricity demand and quasi-fixed residential tariffs at which electricity is sold. This paper investigates strategies for load serving entities to hedge against such price risks. Specifically, we compute profit-maximizing portfolios of forward contract and call options as a function of the uncertain aggregate user demand. We compare the profit to the case of Demand Response, where users are offered monetary incentives to temporarily reduce their consumption during periods of supply shortages. Using smart meter data of residential customers in California, we simulate optimal portfolios and derive conditions under which Demand Response outperforms call options and forward contracts.',\n",
       " 'Residential Demand Response has emerged as a viable tool to alleviate supply and demand imbalances of electricity, particularly during times when the electric grid is strained due a shortage of supply. Demand Response providers bid reduction capacity into the wholesale electricity market by asking their customers under contract to temporarily reduce their consumption in exchange for a monetary incentive. To contribute to the analysis of consumer behavior in response to such incentives, this paper formulates Demand Response as a Mechanism Design problem, where a Demand Response Provider elicits private information of its rational, profit-maximizing customers who derive positive expected utility by participating in reduction events. By designing an incentive compatible and individually rational mechanism to collect users\\' price elasticities of demand, the Demand Response provider can target the most susceptible users to incentives. We measure reductions by comparing the materialized consumption to the projected consumption, which we model as the \"10-in-10\"-baseline, the regulatory standard set by the California Independent System Operator. Due to the suboptimal performance of this baseline, we show, using consumption data of residential customers in California, that Demand Response Providers receive payments for \"virtual reductions\", which exist due to the inaccuracies of the baseline rather than actual reductions. Improving the accuracy of the baseline diminishes the contribution of these virtual reductions.',\n",
       " 'We investigate the ability of a homogeneous collection of deferrable energy loads to behave as a battery; that is, to absorb and release energy in a controllable fashion up to fixed and predetermined limits on volume, charge rate and discharge rate. We derive explicit bounds on the battery capacity that can be offered, and show that there is a fundamental trade-off between the abilities of collective load to absorb and release energy at high aggregate rates. Finally, we introduce a new class of dynamic priority-driven feedback policies that balance these abilities, and characterize the batteries that they can emulate.',\n",
       " 'We investigate the ability of a homogeneous collection of deferrable energy loads to behave as a battery; that is, to absorb and release energy in a controllable fashion up to fixed and predetermined limits on volume, charge rate and discharge rate. We derive bounds on the battery capacity that can be realized and show that there are fundamental trade-offs between battery parameters. By characterizing the state trajectories under scheduling policies that emulate two illustrative batteries, we show that the trade-offs occur because the states that allow the loads to absorb and release energy at high aggregate rates are conflicting.',\n",
       " 'In this paper, we investigate the use of variable speed limits for resilient operation of transportation networks, which are modeled as dynamical flow networks under local routing decisions. In such systems, some external inflow is injected to the so-called origin nodes of the network. The total inflow arriving at each node is routed to its operational outgoing links based on their current particle densities. The density on each link has first order dynamics driven by the difference of its incoming and outgoing flows. A link irreversibly fails if it reaches its jam density. Such failures may propagate in the network and cause a systemic failure. We show that larger link capacities do not necessarily help in preventing systemic failures under local routing. Accordingly, we propose the use of variable speed limits to operate the links below their capacities, when necessary, to compensate for the lack of global information and coordination in routing decisions. Our main result shows that systemic failures under feasible external inflows can always be averted through a proper selection of speed limits if the routing decisions are sufficiently responsive to local congestion and the network is initially uncongested. This is an attractive feature as it is much easier in practice to adjust the speed limits than to build more physical capacity or to alter routing decisions that are determined by social behavior.',\n",
       " \"This paper analyzes stability conditions for wholesale electricity markets under real-time retail pricing and realistic consumption models with memory, which explicitly take into account previous electricity prices and consumption levels. By passing on the current retail price of electricity from supplier to consumer and feeding the observed consumption back to the supplier, a closed-loop dynamical system for electricity prices and consumption arises whose stability is to be investigated. Under mild assumptions on the generation cost of electricity and consumers' backlog disutility functions, we show that, for consumer models with price memory only, market stability is achieved if the ratio between the consumers' marginal backlog disutility and the suppliers' marginal cost of supply remains below a fixed threshold. Further, consumer models with price and consumption memory can result in greater stability regions and faster convergence to the equilibrium compared to models with price memory alone, if consumption deviations from nominal demand are adequately penalized.\",\n",
       " 'In this paper, we are concerned with the resilience of locally routed network flows with finite link capacities. In this setting, an external inflow is injected to the so-called origin nodes. The total inflow arriving at each node is routed locally such that none of the outgoing links are overloaded unless the node receives an inflow greater than its total outgoing capacity. A link irreversibly fails if it is overloaded or if there is no operational link in its immediate downstream to carry its flow. For such systems, resilience is defined as the minimum amount of reduction in the link capacities that would result in the failure of all the outgoing links of an origin node. We show that such networks do not necessarily become more resilient as additional capacity is built in the network. Moreover, when the external inflow does not exceed the network capacity, selective reductions of capacity at certain links can actually help averting the cascading failures, without requiring any change in the local routing policies. This is an attractive feature as it is often easier in practice to reduce the available capacity of some critical links than to add physical capacity or to alter routing policies, e.g., when such policies are determined by social behavior, as in the case of road traffic networks. The results can thus be used for real-time monitoring of distance-to-failure in such networks and devising a feasible course of actions to avert systemic failures.',\n",
       " 'Voltage control plays an important role in the operation of electricity distribution networks, especially when there is a large penetration of renewable energy resources. In this paper, we focus on voltage control through reactive power compensation and study how different information structures affect the control performance. In particular, we first show that using only voltage measurements to determine reactive power compensation is insufficient to maintain voltage in the acceptable range. Then we propose two fully decentralized and robust algorithms by adding additional information, which can stabilize the voltage in the acceptable range. The one with higher complexity can further minimize a cost of reactive power compensation in a particular form. Both of the two algorithms use only local measurements and local variables and require no communication. In addition, the two algorithms are robust against heterogeneous update rates and delays.',\n",
       " 'We introduce a new class of (dynamical) systems that inherently capture cascading effects (viewed as consequential effects) and are naturally amenable to combinations. We develop an axiomatic general theory around those systems, and guide the endeavor towards an understanding of cascading failure. The theory evolves as an interplay of lattices and fixed points, and its results may be instantiated to commonly studied models of cascade effects.\\n  We characterize the systems through their fixed points, and equip them with two operators. We uncover properties of the operators, and express global systems through combinations of local systems. We enhance the theory with a notion of failure, and understand the class of shocks inducing a system to failure. We develop a notion of mu-rank to capture the energy of a system, and understand the minimal amount of effort required to fail a system, termed resilience. We deduce a dual notion of fragility and show that the combination of systems sets a limit on the amount of fragility inherited.',\n",
       " 'Consider a stationary discrete random process with alphabet size d, which is assumed to be the output process of an unknown stationary Hidden Markov Model (HMM). Given the joint probabilities of finite length strings of the process, we are interested in finding a finite state generative model to describe the entire process. In particular, we focus on two classes of models: HMMs and quasi-HMMs, which is a strictly larger class of models containing HMMs. In the main theorem, we show that if the random process is generated by an HMM of order less or equal than k, and whose transition and observation probability matrix are in general position, namely almost everywhere on the parameter space, both the minimal quasi-HMM realization and the minimal HMM realization can be efficiently computed based on the joint probabilities of all the length N strings, for N > 4 lceil log_d(k) rceil +1. In this paper, we also aim to compare and connect the two lines of literature: realization theory of HMMs, and the recent development in learning latent variable models with tensor decomposition techniques.',\n",
       " 'We propose a dynamical model for cascading failures in single-commodity network flows. In the proposed model, the network state consists of flows and activation status of the links. Network dynamics is determined by a, possibly state-dependent and adversarial, disturbance process that reduces flow capacity on the links, and routing policies at the nodes that have access to the network state, but are oblivious to the presence of disturbance. Under the proposed dynamics, a link becomes irreversibly inactive either due to overload condition on itself or on all of its immediate downstream links. The coupling between link activation and flow dynamics implies that links to become inactive successively are not necessarily adjacent to each other, and hence the pattern of cascading failure under our model is qualitatively different than standard cascade models. The magnitude of a disturbance process is defined as the sum of cumulative capacity reductions across time and links of the network, and the margin of resilience of the network is defined as the infimum over the magnitude of all disturbance processes under which the links at the origin node become inactive. We propose an algorithm to compute an upper bound on the margin of resilience for the setting where the routing policy only has access to information about the local state of the network. For the limiting case when the routing policies update their action as fast as network dynamics, we identify sufficient conditions on network parameters under which the upper bound is tight under an appropriate routing policy. Our analysis relies on making connections between network parameters and monotonicity in network state evolution under proposed dynamics.',\n",
       " 'The primary concerns of this paper are twofold: to understand the economic value of storage in the presence of ramp constraints and exogenous electricity prices, and to understand the implications of the associated optimal storage management policy on qualitative and quantitative characteristics of storage response to real-time prices. We present an analytic characterization of the optimal policy, along with the associated finite-horizon time-averaged value of storage. We also derive an analytical upperbound on the infinite-horizon time-averaged value of storage. This bound is valid for any achievable realization of prices when the support of the distribution is fixed, and highlights the dependence of the value of storage on ramp constraints and storage capacity. While the value of storage is a non-decreasing function of price volatility, due to the finite ramp rate, the value of storage saturates quickly as the capacity increases, regardless of volatility. To study the implications of the optimal policy, we first present computational experiments that suggest that optimal utilization of storage can, in expectation, induce a considerable amount of price elasticity near the average price, but little or no elasticity far from it. We then present a computational framework for understanding the behavior of storage as a function of price and the amount of stored energy, and for characterization of the buy/sell phase transition region in the price-state plane. Finally, we study the impact of market-based operation of storage on the required reserves, and show that the reserves may need to be expanded to accommodate market-based storage.',\n",
       " \"We study a model for cascade effects over finite networks based on a deterministic binary linear threshold model. Our starting point is a networked coordination game where each agent's payoff is the sum of the payoffs coming from pairwise interactions with each of the neighbors. We first establish that the best response dynamics in this networked game is equivalent to the linear threshold dynamics with heterogeneous thresholds over the agents. While the previous literature has studied such linear threshold models under the assumption that each agent may change actions at most once, a study of best response dynamics in such networked games necessitates an analysis that allows for multiple switches in actions. In this paper, we develop such an analysis and construct a combinatorial framework to understand the behavior of the model. To this end, we establish that the agents behavior cycles among different actions in the limit and provide three sets of results.\\n  We first characterize the limiting behavioral properties of the dynamics. We determine the length of the limit cycles and reveal bounds on the time steps required to reach such cycles for different network structures. We then study the complexity of decision/counting problems that arise within the context. Specifically, we consider the tractability of counting the number of limit cycles and fixed-points, and deciding the reachability of action profiles. We finally propose a measure of network resilience that captures the nature of the involved dynamics. We prove bounds and investigate the resilience of different network structures under this measure.\",\n",
       " 'In this paper, we examine in an abstract framework, how a tradeoff between efficiency and robustness arises in different dynamic oligopolistic market architectures. We consider a market in which there is a monopolistic resource provider and agents that enter and exit the market following a random process. Self-interested and fully rational agents dynamically update their resource consumption decisions over a finite time horizon, under the constraint that the total resource consumption requirements are met before each individual\\'s deadline. We then compare the statistics of the stationary aggregate demand processes induced by the non-cooperative and cooperative load scheduling schemes. We show that although the non-cooperative load scheduling scheme leads to an efficiency loss - widely known as the \"price of anarchy\" - the stationary distribution of the corresponding aggregate demand process has a smaller tail. This tail, which corresponds to rare and undesirable demand spikes, is important in many applications of interest. On the other hand, when the agents can cooperate with each other in optimizing their total cost, a higher market efficiency is achieved at the cost of a higher probability of demand spikes. We thus posit that the origins of endogenous risk in such systems may lie in the market architecture, which is an inherent characteristic of the system.',\n",
       " 'Robustness of routing policies for networks is a central problem which is gaining increased attention with a growing awareness to safeguard critical infrastructure networks against natural and man-induced disruptions. Routing under limited information and the possibility of cascades through the network adds serious challenges to this problem. This abstract considers the framework of dynamical networks introduced in our earlier work [1,2], where the network is modeled by a system of ordinary differential equations derived from mass conservation laws on directed acyclic graphs with a single origin-destination pair and a constant inflow at the origin. The rate of change of the particle density on each link of the network equals the difference between the inflow and the outflow on that link. The latter is modeled to depend on the current particle density on that link through a flow function. The novel modeling element in this paper is that every link is assumed to have finite capacity for particle density and that the flow function is modeled to be strictly increasing as density increases from zero up to the maximum density capacity, and is discontinuous at the maximum density capacity, with the flow function value being zero at that point. This feature, in particular, allows for the possibility of spill-backs in our model. In this paper, we present our results on resilience of such networks under distributed routing, towards perturbations that reduce link-wise flow functions.',\n",
       " \"This paper examines the value of storage in securing reliability of a system with uncertain supply and demand, and supply friction. The storage is frictionless as a supply source, but once used, it cannot be filled up instantaneously. The focus application is a power supply network in which the base supply and demand are assumed to match perfectly, while deviations from the base are modeled as random shocks with stochastic arrivals. Due to friction, the random surge shocks cannot be tracked by the main supply sources. Storage, when available, can be used to compensate, fully or partially, for the surge in demand or loss of supply. The problem of optimal utilization of storage with the objective of maximizing system reliability is formulated as minimization of the expected discounted cost of blackouts over an infinite horizon. It is shown that when the stage cost is linear in the size of the blackout, the optimal policy is myopic in the sense that all shocks are compensated by storage up to the available level of storage. However, when the stage cost is strictly convex, it may be optimal to curtail some of the demand and allow a small current blackout in the interest of maintaining a higher level of reserve to avoid a large blackout in the future. The value of storage capacity in improving system's reliability, as well as the effects of the associated optimal policies under different stage costs on the probability distribution of blackouts are examined.\",\n",
       " \"We propose a general methodology for performing statistical inference within a `rare-events regime' that was recently suggested by Wagner, Viswanath and Kulkarni. Our approach allows one to easily establish consistent estimators for a very large class of canonical estimation problems, in a large alphabet setting. These include the problems studied in the original paper, such as entropy and probability estimation, in addition to many other interesting ones. We particularly illustrate this approach by consistently estimating the size of the alphabet and the range of the probabilities. We start by proposing an abstract methodology based on constructing a probability measure with the desired asymptotic properties. We then demonstrate two concrete constructions by casting the Good-Turing estimator as a pseudo-empirical measure, and by using the theory of mixture model estimation.\",\n",
       " \"The paper proposes a framework for modeling and analysis of the dynamics of supply, demand, and clearing prices in power system with real-time retail pricing and information asymmetry. Real-time retail pricing is characterized by passing on the real-time wholesale electricity prices to the end consumers, and is shown to create a closed-loop feedback system between the physical layer and the market layer of the power system. In the absence of a carefully designed control law, such direct feedback between the two layers could increase volatility and lower the system's robustness to uncertainty in demand and generation. A new notion of generalized price-elasticity is introduced, and it is shown that price volatility can be characterized in terms of the system's maximal relative price elasticity, defined as the maximal ratio of the generalized price-elasticity of consumers to that of the producers. As this ratio increases, the system becomes more volatile, and eventually, unstable. As new demand response technologies and distributed storage increase the price-elasticity of demand, the architecture under examination is likely to lead to increased volatility and possibly instability. This highlights the need for assessing architecture systematically and in advance, in order to optimally strike the trade-offs between volatility, economic efficiency, and system reliability.\",\n",
       " 'Strong resilience properties of dynamical flow networks are analyzed for distributed routing policies. The latter are characterized by the property that the way the inflow at a non-destination node gets split among its outgoing links is allowed to depend only on local information about the current particle densities on the outgoing links. The strong resilience of the network is defined as the infimum sum of link-wise flow capacity reductions under which the network cannot maintain the asymptotic total inflow to the destination node to be equal to the inflow at the origin. A class of distributed routing policies that are locally responsive to local information is shown to yield the maximum possible strong resilience under such local information constraints for an acyclic dynamical flow network with a single origin-destination pair. The maximal strong resilience achievable is shown to be equal to the minimum node residual capacity of the network. The latter depends on the limit flow of the unperturbed network and is defined as the minimum, among all the non-destination nodes, of the sum, over all the links outgoing from the node, of the differences between the maximum flow capacity and the limit flow of the unperturbed network. We propose a simple convex optimization problem to solve for equilibrium limit flows of the unperturbed network that minimize average delay subject to strong resilience guarantees, and discuss the use of tolls to induce such an equilibrium limit flow in transportation networks. Finally, we present illustrative simulations to discuss the connection between cascaded failures and the resilience properties of the network.',\n",
       " \"Robustness of distributed routing policies is studied for dynamical flow networks, with respect to adversarial disturbances that reduce the link flow capacities. A dynamical flow network is modeled as a system of ordinary differential equations derived from mass conservation laws on a directed acyclic graph with a single origin-destination pair and a constant inflow at the origin. Routing policies regulate the way the inflow at a non-destination node gets split among its outgoing links as a function of the current particle density, while the outflow of a link is modeled to depend on the current particle density on that link through a flow function. The dynamical flow network is called partially transferring if the total inflow at the destination node is asymptotically bounded away from zero, and its weak resilience is measured as the minimum sum of the link-wise magnitude of all disturbances that make it not partially transferring. The weak resilience of a dynamical flow network with arbitrary routing policy is shown to be upper-bounded by the network's min-cut capacity, independently of the initial flow conditions. Moreover, a class of distributed routing policies that rely exclusively on local information on the particle densities, and are locally responsive to that, is shown to yield such maximal weak resilience. These results imply that locality constraints on the information available to the routing policies do not cause loss of weak resilience. Some fundamental properties of dynamical flow networks driven by locally responsive distributed policies are analyzed in detail, including global convergence to a unique limit flow.\",\n",
       " \"Stability of Wardrop equilibria is analyzed for dynamical transportation networks in which the drivers' route choices are influenced by information at multiple temporal and spatial scales. The considered model involves a continuum of indistinguishable drivers commuting between a common origin/destination pair in an acyclic transportation network. The drivers' route choices are affected by their, relatively infrequent, perturbed best responses to global information about the current network congestion levels, as well as their instantaneous local observation of the immediate surroundings as they transit through the network. A novel model is proposed for the drivers' route choice behavior, exhibiting local consistency with their preference toward globally less congested paths as well as myopic decisions in favor of locally less congested paths. The simultaneous evolution of the traffic congestion on the network and of the aggregate path preference is modeled by a system of coupled ordinary differential equations. The main result shows that, if the frequency of updates of path preferences is sufficiently small as compared to the frequency of the traffic flow dynamics, then the state of the transportation network ultimately approaches a neighborhood of the Wardrop equilibrium. The presented results may be read as a further evidence in support of Wardrop's postulate of equilibrium, showing robustness of it with respect to non-persistent perturbations. The proposed analysis combines techniques from singular perturbation theory, evolutionary game theory, and cooperative dynamical systems.\",\n",
       " '  We demonstrate the use of a new, control-oriented notion of finite state approximation for a particular class of hybrid systems. Specifically, we consider the problem of designing a stabilizing binary output feedback switching controller for a pair of unstable homogeneous second order systems. The constructive approach presented in this note, in addition to yielding an explicit construction of a deterministic finite state approximate model of the hybrid plant, allows us to efficiently establish a useable upper bound on the quality of approximation, and leads to a discrete optimization problem whose solution immediately provides a certifiably correct-by-design controller for the original system. The resulting controller consists of a finite state observer for the plant and a corresponding full state feedback switching control law.',\n",
       " '  A set of N independent Gaussian linear time invariant systems is observed by M sensors whose task is to provide the best possible steady-state causal minimum mean square estimate of the state of the systems, in addition to minimizing a steady-state measurement cost. The sensors can switch between systems instantaneously, and there are additional resource constraints, for example on the number of sensors which can observe a given system simultaneously. We first derive a tractable relaxation of the problem, which provides a bound on the achievable performance. This bound can be computed by solving a convex program involving linear matrix inequalities. Exploiting the additional structure of the sites evolving independently, we can decompose this program into coupled smaller dimensional problems. In the scalar case with identical sensors, we give an analytical expression of an index policy proposed in a more general context by Whittle. In the general case, we develop open-loop periodic switching policies whose performance matches the bound arbitrarily closely.',\n",
       " '  We extend a relaxation technique due to Bertsimas and Nino-Mora for the restless bandit problem to the case where arbitrary costs penalize switching between the bandits. We also construct a one-step lookahead policy using the solution of the relaxation. Computational experiments and a bound for approximate dynamic programming provide some empirical support for the heuristic.',\n",
       " \"We investigate the emergence of global alignment in colonies of dividing rod-shaped cells under confinement. Using molecular dynamics simulations and continuum modeling, we demonstrate that geometrical anisotropies in the confining environment give rise to imbalance in the normal stresses, which, in turn, drives a collective rearrangement of the cells. This behavior crucially relies on the colony's solid-like mechanical response at short time scales and can be recovered within the framework of active hydrodynamics upon modeling bacterial colonies as growing viscoelastic gels characterized by Maxwell-like stress relaxation.\",\n",
       " 'Detection of anomalous behaviors in data centers is crucial to predictive maintenance and data safety. With data centers, we mean any computer network that allows users to transmit and exchange data and information. In particular, we focus on the Tier-1 data center of the Italian Institute for Nuclear Physics (INFN), which supports the high-energy physics experiments at the Large Hadron Collider (LHC) in Geneva. The center provides resources and services needed for data processing, storage, analysis, and distribution. Log records in the data center is a stochastic and non-stationary phenomenon in nature. We propose a real-time approach to monitor and classify log records based on sliding time windows, and a time-varying evolving fuzzy-rule-based classification model. The most frequent log pattern according to a control chart is taken as the normal system status. We extract attributes from time windows to gradually develop and update an evolving Gaussian Fuzzy Classifier (eGFC) on the fly. The real-time anomaly monitoring system has to provide encouraging results in terms of accuracy, compactness, and real-time operation.',\n",
       " 'The popularity of drones is rapidly increasing across the different sectors of the economy. Aerial capabilities and relatively low costs make drones the perfect solution to improve the efficiency of those operations that are typically carried out by humans (e.g., building inspection, photo collection). The potential of drone applications can be pushed even further when they are operated in fleets and in a fully autonomous manner, acting de facto as a drone swarm. Besides automating field operations, a drone swarm can serve as an ad-hoc cloud infrastructure built on top of computing and storage resources available across the swarm members and other connected elements. Even in the absence of Internet connectivity, this cloud can serve the workloads generated by the swarm members themselves, as well as by the field agents operating within the area of interest. By considering the practical example of a swarm-powered 3D reconstruction application, we present a new optimization problem for the efficient generation and execution, on top of swarm-powered ad-hoc cloud infrastructure, of multi-node computing workloads subject to data geolocation and clustering constraints. The objective is the minimization of the overall computing times, including both networking delays caused by the inter-drone data transmission and computation delays. We prove that the problem is NP-hard and present two combinatorial formulations to model it. Computational results on the solution of the formulations show that one of them can be used to solve, within the configured time-limit, more than 50% of the considered real-world instances involving up to two hundred images and six drones.',\n",
       " 'The KATRIN (Karlsruhe Tritium Neutrino) experiment investigates the energetic endpoint of the tritium beta-decay spectrum to determine the effective mass of the electron anti-neutrino. The collaboration has reported a first mass measurement result at this TAUP-2019 conference. The TRISTAN project aims at detecting a keV-sterile neutrino signature by measuring the entire tritium beta-decay spectrum with an upgraded KATRIN system. One of the greatest challenges is to handle the high signal rates generated by the strong activity of the KATRIN tritium source while maintaining a good energy resolution. Therefore, a novel multi-pixel silicon drift detector and read-out system are being designed to handle rates of about 100 Mcps with an energy resolution better than 300 eV (FWHM). This report presents succinctly the KATRIN experiment, the TRISTAN project, then the results of the first 7-pixels prototype measurement campaign and finally describes the construction of the first TRISTAN module composed of 166 SDD-pixels as well as its implementation in KATRIN experiment.',\n",
       " 'The Massive and Distant Clusters of WISE Survey (MaDCoWS) provides a catalog of high-redshift ($0.7\\\\lesssim z\\\\lesssim 1.5$) infrared-selected galaxy clusters. However, the verification of the ionized intracluster medium, indicative of a collapsed and nearly virialized system, is made challenging by the high redshifts of the sample members. The main goal of this work is to test the capabilities of the Atacama Compact Array (ACA; also known as the Morita Array) Band 3 observations, centered at about 97.5 GHz, to provide robust validation of cluster detections via the thermal Sunyaev-Zeldovich (SZ) effect. Using a pilot sample that comprises ten MaDCoWS galaxy clusters, accessible to ACA and representative of the median sample richness, we infer the masses of the selected galaxy clusters and respective detection significance by means of a Bayesian analysis of the interferometric data. Our test of the \"Verification with the ACA - Localization and Cluster Analysis\" (VACA LoCA) program demonstrates that the ACA can robustly confirm the presence of the virialized intracluster medium in galaxy clusters previously identified in full-sky surveys. In particular, we obtain a significant detection of the SZ effect for seven out of the ten VACA LoCA clusters. We note that this result is independent of the assumed pressure profile. However, the limited angular dynamic range of the ACA in Band 3 alone, short observational integration times, and possible contamination from unresolved sources limit the detailed characterization of the cluster properties and the inference of the cluster masses within scales appropriate for the robust calibration of mass-richness scaling relations.',\n",
       " \"Let $\\\\mathcal{E}/\\\\mathbb{F}_q$ be an elliptic curve, and $P$ a point in $\\\\mathcal{E}(\\\\mathbb{F}_q)$ of prime order $\\\\ell$. VÃ©lu's formulae let us compute a quotient curve $\\\\mathcal{E}' = \\\\mathcal{E}/\\\\langle{P}\\\\rangle$ and rational maps defining a quotient isogeny $Ï†: \\\\mathcal{E} \\\\to \\\\mathcal{E}'$ in $\\\\tilde{O}(\\\\ell)$ $\\\\mathbb{F}_q$-operations, where the $\\\\tilde{O}$ is uniform in $q$.This article shows how to compute $\\\\mathcal{E}'$, and $Ï†(Q)$ for $Q$ in $\\\\mathcal{E}(\\\\mathbb{F}_q)$, using only $\\\\tilde{O}(\\\\sqrt{\\\\ell})$ $\\\\mathbb{F}_q$-operations, where the $\\\\tilde{O}$ is again uniform in $q$.As an application, this article speeds up some computations used in the isogeny-based cryptosystems CSIDH and CSURF.\",\n",
       " \"Objective: Global Maxwell Tomography (GMT) is a recently introduced volumetric technique for noninvasive estimation of electrical properties (EP) from magnetic resonance measurements. Previous work evaluated GMT using ideal radiofrequency (RF) excitations. The aim of this simulation study was to assess GMT performance with a realistic RF coil. Methods: We designed a transmit-receive RF coil with $8$ decoupled channels for $7$T head imaging. We calculated the RF transmit field ($B_1^+$) inside heterogeneous head models for different RF shimming approaches, and used them as input for GMT to reconstruct EP for all voxels. Results: Coil tuning/decoupling remained relatively stable when the coil was loaded with different head models. Mean error in EP estimation changed from $7.5\\\\%$ to $9.5\\\\%$ and from $4.8\\\\%$ to $7.2\\\\%$ for relative permittivity and conductivity, respectively, when changing head model without re-tuning the coil. Results slightly improved when an SVD-based RF shimming algorithm was applied, in place of excitation with one coil at a time. Despite errors in EP, RF transmit field ($B_1^+$) and absorbed power could be predicted with less than $0.5\\\\%$ error over the entire head. GMT could accurately detect a numerically inserted tumor. Conclusion: This work demonstrates that GMT can reliably reconstruct EP in realistic simulated scenarios using a tailored 8-channel RF coil design at $7$T. Future work will focus on construction of the coil and optimization of GMT's robustness to noise, to enable in-vivo GMT experiments. Significance: GMT could provide accurate estimations of tissue EP, which could be used as biomarkers and could enable patient-specific estimation of RF power deposition, which is an unsolved problem for ultra-high-field magnetic resonance imaging.\",\n",
       " 'The fragility of modern machine learning models has drawn a considerable amount of attention from both academia and the public. While immense interests were in either crafting adversarial attacks as a way to measure the robustness of neural networks or devising worst-case analytical robustness verification with guarantees, few methods could enjoy both scalability and robustness guarantees at the same time. As an alternative to these attempts, randomized smoothing adopts a different prediction rule that enables statistical robustness arguments and can scale to large networks. However, in this paper, we point out for the first time the side effects of current randomized smoothing workflows. Specifically, we articulate and prove two major points: 1) the decision boundaries shrink with the adoption of randomized smoothing prediction rule; 2) noise augmentation does not necessarily resolve the shrinking issue and can even create additional issues.',\n",
       " 'The notion of gauge invariance plays a predominant role in the most fundamental theories of Nature. However, understanding non-perturbartive effects is a challenging open problem with implications in other disciplines, such as condensed matter and quantum information. Here, motivated by the recent experimental progress in the field of quantum simulations with ultracold atoms, and using numerical tools developed in quantum information, we explore the interplay of topology with local and global symmetries in a fermionic Ising gauge theory. We find an Aharonov-Bohm instability, resulting in the spontaneous generation of a topologically-ordered $Ï€$-flux phase that can coexist with a symmetry-protected topological phase and, interestingly, intertwine with it by a topological flux-threading phenomenon. This instability leads to deconfined phases in lattice gauge theories without the need of plaquette interactions, identifying a promising route for future quantum-simulation experiments. We show that, at finite chemical potentials, the deconfined phase occurs via the generation of topological solitons in the gauge-field configuration, in which fermions can localise forming $\\\\mathbb{Z}_2$-charged quasi-particles.',\n",
       " 'In turbulence phenomena, including the quantum turbulence in superfluids, an energy flux flows from large to small length scales, composing a cascade of energy. It is a well-known fact that for multi-scale energy flow, dissipation can be scale-dependent. In particular, the existence of a range of scales where there is no energy accumulation, the inertial range, is an indication of universal behavior in turbulence. There are intrinsic difficulties associated with the measurement of the energy flux during the time evolution of turbulence. Here we present a procedure to measure the energy flux during the time evolution of turbulence in a sample of a trapped Bose-Einstein condensate. The energy flux is evaluated using the energy spectrum and the continuity equation. We identified intervals of momentum where the flux is constant using two different procedures. The identification of a region with constant flux in turbulent BECs is a manifestation of the universal character of turbulence in these quantum systems. These measurements pave the way for studies of energy conservation and dissipation in a scale-dependent manner in trapped atomic superfluids, and also analogies with the related processes that take place in ordinary fluids.',\n",
       " \"The complex organic chemistry harbored by the atmosphere of Titan has been investigated in depth by Cassini observations. Among them, a series of solar occultations performed by the VIMS instrument throughout the 13 years of Cassini revealed a strong absorption centered at 3.4 $Î¼$m. Several molecules present in Titan's atmosphere create spectral features in that wavelength region, but their individual contributions are difficult to disentangle. In this work, we quantify the contribution of the various molecular species to the 3.4 $Î¼$m band using a radiative transfer model. Ethane and propane are a significant component of the band but they are not enough to fit the shape perfectly, then we need something else. Polycyclic Aromatic Hydrocarbons (PAHs) and more complex polyaromatic hydrocarbons like Hydrogenated Amorphous Carbons (HACs) are the most plausible candidates because they are rich in C-H bonds. PAHs signature have already been detected above ~900 km, and they are recognized as aerosols particles precursors. High similarities between individual spectra impede abundances determinations.\",\n",
       " 'Some highly irradiated close-in exoplanets orbit stars showing anomalously low stellar chromospheric emission. We attribute this to absorption by circumstellar gas replenished by mass loss from ablating planets. Here we report statistics validating this hypothesis. Among ~3000 nearby, bright, main sequence stars ~40 show depressed chromospheric emission indicative of undiscovered mass-losing planets. The Dispersed Matter Planet Project uses high precision, high cadence radial velocity measurements to detect these planets. We summarise results for two planetary systems (DMPP-1 and DMPP-3) and fully present observations revealing a Mp sin i = 0.469 M$_{\\\\rm J}$ planet in a 5.207 d orbit around the $Î³$-Doradus pulsator HD 11231 (DMPP-2). We have detected short period planets wherever we have made more than 60 RV measurements, demonstrating that we have originated a very efficient method for detecting nearby compact planetary systems. Our shrouded, ablating planetary systems may be a short-lived phase related to the Neptunian desert: i.e. the dearth of intermediate-mass planets at short orbital periods. The circumstellar gas facilitates compositional analysis; allowing empirical exogeology in the cases of sublimating rocky planets. Dispersed Matter Planet Project discoveries will be important for establishing the empirical mass-radius-composition relationship(s) for low mass planets.',\n",
       " 'The emergence of high repetition-rate X-ray free-electron lasers (XFELs) powered by superconducting accelerator technology enables the measurement of significantly more experimental data per day than was previously possible. The European XFEL will soon provide 27,000 pulses per second, more than two orders of magnitude more than any other XFEL. The increased pulse rate is a key enabling factor for single-particle X-ray diffractive imaging, which relies on averaging the weak diffraction signal from single biological particles. Taking full advantage of this new capability requires that all experimental steps, from sample preparation and delivery to the acquisition of diffraction patterns, are compatible with the increased pulse repetition rate. Here, we show that single-particle imaging can be performed using X-ray pulses at megahertz repetition rates. The obtained results pave the way towards exploiting high repetition-rate X-ray free-electron lasers for single-particle imaging at their full repetition rate.',\n",
       " 'Earth mass exoplanets are difficult to detect. The Dispersed Matter Planet Project (DMPP) identifies stars which are likely to host the most detectable low mass exoplanets. The star DMPP-3 (HD 42936) shows signs of circumstellar absorption, indicative of mass loss from ablating planets. Here we report the radial velocity (RV) discovery of a highly eccentric 507 d binary companion and a hot super-Earth planet in a 6.67 d orbit around the primary star. DMPP-3A is a solar type star while DMPP-3B is just massive enough to fuse hydrogen. The binary, with semi-major axis 1.22 $\\\\pm$ 0.02 AU, is considerably tighter than others known to host planets orbiting only one of the component stars. The configuration of the DMPP-3 planetary system is rare and indicates dynamical interactions, though the evolutionary history is not entirely clear. DMPP-3Ab is possibly the residual core of a giant planet precursor, consistent with the inferred circumstellar gas shroud.',\n",
       " 'Verifying robustness of neural networks given a specified threat model is a fundamental yet challenging task. While current verification methods mainly focus on the L_p-norm-ball threat model of the input instances, robustness verification against semantic adversarial attacks inducing large L_p-norm perturbations such as color shifting and lighting adjustment are beyond their capacity. To bridge this gap, we propose Semantify-NN, a model-agnostic and generic robustness verification approach against semantic perturbations for neural networks. By simply inserting our proposed semantic perturbation layers (SP-layers) to the input layer of any given model, Semantify-NN is model-agnostic, and any $L_p$-norm-ball based verification tools can be used to verify the model robustness against semantic perturbations. We illustrate the principles of designing the SP-layers and provide examples including semantic perturbations to image classification in the space of hue, saturation, lightness, brightness, contrast and rotation, respectively. Experimental results on various network architectures and different datasets demonstrate the superior verification performance of Semantify-NN over L_p-norm-based verification frameworks that naively convert semantic perturbation to L_p-norm. To the best of our knowledge, Semantify-NN is the first framework to support robustness verification against a wide range of semantic perturbations.',\n",
       " 'Many fields of science and engineering rely on running simulations with complex and computationally expensive models to understand the involved processes in the system of interest. Nevertheless, the high cost involved hamper reliable and exhaustive simulations. Very often such codes incorporate heuristics that ironically make them less tractable and transparent. This paper introduces an active learning methodology for adaptively constructing surrogate models, i.e. emulators, of such costly computer codes in a multi-output setting. The proposed technique is sequential and adaptive, and is based on the optimization of a suitable acquisition function. It aims to achieve accurate approximations, model tractability, as well as compact and expressive simulated datasets. In order to achieve this, the proposed Active Multi-Output Gaussian Process Emulator (AMOGAPE) combines the predictive capacity of Gaussian Processes (GPs) with the design of an acquisition function that favors sampling in low density and fluctuating regions of the approximation functions. Comparing different acquisition functions, we illustrate the promising performance of the method for the construction of emulators with toy examples, as well as for a widely used remote sensing transfer code.',\n",
       " 'The rapid growth of deep learning applications in real life is accompanied by severe safety concerns. To mitigate this uneasy phenomenon, much research has been done providing reliable evaluations of the fragility level in different deep neural networks. Apart from devising adversarial attacks, quantifiers that certify safeguarded regions have also been designed in the past five years. The summarizing work of Salman et al. unifies a family of existing verifiers under a convex relaxation framework. We draw inspiration from such work and further demonstrate the optimality of deterministic CROWN (Zhang et al. 2018) solutions in a given linear programming problem under mild constraints. Given this theoretical result, the computationally expensive linear programming based method is shown to be unnecessary. We then propose an optimization-based approach \\\\textit{FROWN} (\\\\textbf{F}astened C\\\\textbf{ROWN}): a general algorithm to tighten robustness certificates for neural networks. Extensive experiments on various networks trained individually verify the effectiveness of FROWN in safeguarding larger robust regions.',\n",
       " 'Photoinduced isomerization reactions, including ring-opening reactions, lie at the heart of many processes in nature. The mechanisms of such reactions are determined by a delicate interplay of coupled electronic and nuclear dynamics unfolding on the femtosecond scale, followed by the slower redistribution of energy into different vibrational degrees of freedom. Here we apply time-resolved photoelectron spectroscopy with a seeded extreme ultraviolet free electron laser to trace the ultrafast ring opening of gas phase thiophenone molecules following photoexcitation at 265 nm. When combined with cutting edge ab initio electronic structure and molecular dynamics calculations of both the excited and ground state molecules, the results provide unprecedented insights into both electronic and nuclear dynamics of this fundamental class of reactions. The initial ring opening and non-adiabatic coupling to the electronic ground state is shown to be driven by ballistic SC bond extension and to be complete within 350 femtoseconds. Theory and experiment also allow clear visualization of the rich ground-state dynamics involving formation of, and interconversion between, several ring opened isomers and the reformed cyclic structure, and fragmentation (CO loss) over much longer timescales.',\n",
       " 'Metis is the first solar coronagraph designed for a space mission capable of performing simultaneous imaging of the off-limb solar corona in both visible and UV light. The observations obtained with Metis aboard the Solar Orbiter ESA-NASA observatory will enable us to diagnose, with unprecedented temporal coverage and spatial resolution, the structures and dynamics of the full corona from 1.7 $R_\\\\odot$ to about 9 $R_\\\\odot$. Due to the uniqueness of the Solar Orbiter mission profile, Metis will be able to observe the solar corona from a close vantage point (down to 0.28 AU), achieving out-of-ecliptic views with the increase of the orbit inclination over time. Moreover, observations near perihelion, during the phase of lower rotational velocity of the solar surface relative to the spacecraft, will allow longer-term studies of the coronal features. Thanks to a novel occultation design and a combination of a UV interference coating of the mirrors and a spectral bandpass filter, Metis images the solar corona simultaneously in the visible light band, between 580 and 640 nm, and in the UV H I Lyman-Î± line at 121.6 nm. The coronal images in both the UV Lyman-Î± and polarised visible light are obtained at high spatial resolution with a spatial scale down to about 2000 km and 15000 km at perihelion, in the cases of the visible and UV light, respectively. A temporal resolution down to 1 second can be achieved when observing coronal fluctuations in visible light. The Metis measurements will allow for complete characterisation of the main physical parameters and dynamics of the electron and neutral hydrogen/proton plasma components of the corona in the region where the solar wind undergoes acceleration and where the onset and initial propagation of coronal mass ejections take place, thus significantly improving our understanding of the region connecting the Sun to the heliosphere.',\n",
       " 'We revisit the analysis of the bright multiplanet system K2-93, discovered with data taken by the K2 mission. This system contains five identified planets ranging in size from sub-Neptune to Jupiter size. The K2 data available at the discovery of the system only showed single transits for the three outer planets, which allowed weak constraints to be put on their periods. As these planets are interesting candidates for future atmospheric studies, a better characterization of the host star and tighter constraints on their orbital periods are essential. Using new data from the K2 mission taken after the discovery of the system, we perform an asteroseismic characterization of the host star. We are able to place strong constraints on the stellar parameters and obtain a value for the stellar mass of $1.22^{+0.03}_{-0.02}\\\\, \\\\rm M_{\\\\odot}$, a stellar radius of $1.30\\\\pm 0.01\\\\, \\\\rm R_{\\\\odot}$, and an age of $2.07^{+0.36}_{-0.27}$ Gyr. Put together with the additional transits identified for two of the three outer planets, we constrain the orbital periods of the outer planets and provide updated estimates for the stellar reflex velocities induced by the planets.',\n",
       " 'We present isochrone ages and initial bulk metallicities ($\\\\rm [Fe/H]_{bulk}$, by accounting for diffusion) of 163,722 stars from the GALAH Data Release 2, mainly composed of main sequence turn-off stars and subgiants ($\\\\rm 7000 K>T_{eff}>4000 K$ and $\\\\rm log g>3$ dex). The local age-metallicity relationship (AMR) is nearly flat but with significant scatter at all ages; the scatter is even higher when considering the observed surface abundances. After correcting for selection effects, the AMR appear to have intrinsic structures indicative of two star formation events, which we speculate are connected to the thin and thick disks in the solar neighborhood. We also present abundance ratio trends for 16 elements as a function of age, across different $\\\\rm [Fe/H]_{bulk}$ bins. In general, we find the trends in terms of [X/Fe] vs age from our far larger sample to be compatible with studies based on small ($\\\\sim$ 100 stars) samples of solar twins but we now extend it to both sub- and super-solar metallicities. The $Î±$-elements show differing behaviour: the hydrostatic $Î±$-elements O and Mg show a steady decline with time for all metallicities while the explosive $Î±$-elements Si, Ca and Ti are nearly constant during the thin disk epoch (ages $\\\\lessapprox $ 12 Gyr). The s-process elements Y and Ba show increasing [X/Fe] with time while the r-process element Eu have the opposite trend, thus favouring a primary production from sources with a short time-delay such as core-collapse supernovae over long-delay events such as neutron star mergers.',\n",
       " 'One of the reasons for the success of convolutional networks is their equivariance/invariance under translations. However, rotatable data such as molecules, living cells, everyday objects, or galaxies require processing with equivariance/invariance under rotations in cases where the rotation of the coordinate system does not affect the meaning of the data (e.g. object classification). On the other hand, estimation/processing of rotations is necessary in cases where rotations are important (e.g. motion estimation). There has been recent progress in methods and theory in all these regards. Here we provide an overview of existing methods, both for 2D and 3D rotations (and translations), and identify commonalities and links between them, in the hope that our insights will be useful for choosing and perfecting the methods.',\n",
       " 'deal.II is a state-of-the-art finite element library focused on generality, dimension-independent programming, parallelism, and extensibility. Herein, we outline its primary design considerations and its sophisticated features such as distributed meshes, $hp$-adaptivity, support for complex geometries, and matrix-free algorithms. But deal.II is more than just a software library: It is also a diverse and worldwide community of developers and users, as well as an educational platform. We therefore also discuss some of the technical and social challenges and lessons learned in running a large community software project over the course of two decades.',\n",
       " 'It has been argued that current saturation in graphene field-effect transistors (GFETs) is needed to get the highest possible maximum oscillation frequency (fmax). This paper numerically investigates whether velocity saturation can help to get better current saturation and if that correlates with enhanced fmax. For such a purpose, we used a drift-diffusion simulator that includes several factors that influence output conductance, especially at short channel lengths and-or large drain bias: short-channel electrostatics, saturation velocity, graphene-dielectric interface traps, and self-heating effects. As a testbed for our investigation, we analyzed fabricated GFETs with high extrinsin cutoff frequency fT,x (34 GHz) and fmax (37 GHz). Our simulations allow for a microscopic (local) analysis of the channel parameteres such as carrier concentration, drift and saturation velocities. For biases far away from the Dirac voltage, where the channel behaves as unipolar, we confirmed that the higher is the drift velocity, as close as possible to the saturation velocity, the greater fmax is. However, the largest fmax is recorded at biases near the crossover between unipolar and bipolar behavior, where it does not hold that the highest drift velocity maximizes fmax. In fact, the position and magnitude of the largest fmax depend on the complex interplay between the carrier concentration and total velocity which, in turn, are impacted by the self-heating. Importantly, this effect was found to severely limit radio-frequency performance, reducing the maximum fmax from around 60 to 40 GHz.',\n",
       " 'Here we report on a novel High Harmonic Generation (HHG) light source designed for space charge free ultrafast photoelectron spectroscopy (PES) on solids. The ultimate overall energy resolution achieved on a polycrystalline Au sample is ~22 meV at 40 K. These results have been obtained at a photon energy of 16.9 eV with a pulse bandwidth of ~19 meV, by varying, up to 200 kHz, the photon pulses repetition rate and the photon fluence on the sample. These features set a new benchmark for tunable narrowband HHG sources. By comparing the PES energy resolution and the photon pulse bandwidth with a pulse duration of ~105 fs, as retrieved from time-resolved (TR) angle resolved (AR) PES experiments on Bi$_2$Se$_3$, we validate a way for a space charge free photoelectric process close to Fourier transform limit conditions for ultrafast TR-PES experiments on solids.',\n",
       " 'In this work, we provide an effective model to evaluate the one-electron dipole matrix elements governing optical excitations and the photoemission process of single-layer (SL) and bilayer (BL) transition metal dichalcogenides. By utilizing a $\\\\vec{k} \\\\cdot \\\\vec{p}$ Hamiltonian, we calculate the photoemission intensity as observed in angle-resolved photoemission from the valence bands around the $\\\\bar{K}$-valley of MoS$_2$. In SL MoS$_2$ we find a significant masking of intensity outside the first Brillouin zone, which originates from an in-plane interference effect between photoelectrons emitted from the Mo $d$ orbitals. In BL MoS$_2$ an additional inter-layer interference effect leads to a distinctive modulation of intensity with photon energy. Finally, we use the semiconductor Bloch equations to model the optical excitation in a time- and angle-resolved pump-probe photoemission experiment. We find that the momentum dependence of an optically excited population in the conduction band leads to an observable dichroism in both SL and BL MoS$_2$.',\n",
       " 'Inversion-symmetric crystals are optically isotropic and thus naively not expected to show dichroism effects in optical absorption and photoemission processes. Here, we find a strong linear dichroism effect (up to 42.4%) in the conduction band of inversion-symmetric bilayer MoS$_2$, when measuring energy- and momentum-resolved snapshots of excited electrons by time- and angle-resolved photoemission spectroscopy. We model the polarization-dependent photoemission intensity in the transiently-populated conduction band using the semiconductor Bloch equations and show that the observed dichroism emerges from intralayer single-particle effects within the isotropic part of the dispersion. This leads to optical excitations with an anisotropic momentum-dependence in an otherwise inversion symmetric material.',\n",
       " 'Sterile neutrinos emerge in minimal extensions of the Standard Model which can solve a number of open questions in astroparticle physics. For example, sterile neutrinos in the keV-mass range are viable dark matter candidates. Their existence would lead to a kink-like distortion in the tritium $Î²$-decay spectrum. In this work we report about the instrumentation of the Troitsk nu-mass experiment with a 7-pixel TRISTAN prototype detector and measurements in both differential and integral mode. The combination of the two modes is a key requirement for a precise sterile neutrino search, as both methods are prone to largely different systematic uncertainties. Thanks to the excellent performance of the TRISTAN detector at high rates, a sterile neutrino search up to masses of about 6 keV could be performed, which enlarges the previous accessible mass range by a factor of 3. Upper limits on the neutrino mixing amplitude in the mass range < 5.6 keV (differential) and < 6.6 keV (integral) are presented. These results demonstrate the feasibility of a sterile neutrino search as planned in the upgrade of the KATRIN experiment with the final TRISTAN detector and read-out system.',\n",
       " 'Deep neural networks are known to be fragile to small adversarial perturbations. This issue becomes more critical when a neural network is interconnected with a physical system in a closed loop. In this paper, we show how to combine recent works on neural network certification tools (which are mainly used in static settings such as image classification) with robust control theory to certify a neural network policy in a control loop. Specifically, we give a sufficient condition and an algorithm to ensure that the closed loop state and control constraints are satisfied when the persistent adversarial perturbation is l-infinity norm bounded. Our method is based on finding a positively invariant set of the closed loop dynamical system, and thus we do not require the differentiability or the continuity of the neural network policy. Along with the verification result, we also develop an effective attack strategy for neural network control systems that outperforms exhaustive Monte-Carlo search significantly. We show that our certification algorithm works well on learned models and achieves 5 times better result than the traditional Lipschitz-based method to certify the robustness of a neural network policy on a cart pole control problem.',\n",
       " 'The Sr~{\\\\sc i} 4607~Ã… spectral line shows one of the strongest scattering polarization signals in the visible solar spectrum. The amplitude of this polarization signal is expected to vary at granular spatial scales, due to the combined action of the Hanle effect and the local anisotropy of the radiation field. Observing these variations would be of great interest because it would provide precious information on the small-scale activity of the solar photosphere. At present, few detections of such spatial variations have been reported. This is due to the difficulty of these measurements, which require combining high spatial ($\\\\sim$ 0.1\"), spectral ($\\\\leq$ 20 mÃ…), and temporal resolution (< 1 min) with increased polarimetric sensitivity ($\\\\sim$ 10$^-$$^4$). Aims. We aim to detect spatial variations at granular scales of the scattering polarization peak of the Sr~{\\\\sc i} 4607~Ã… line at different limb distances, and to study the correlation with the continuum intensity. Methods.Using the Zurich IMaging POLarimeter (ZIMPOL) system mounted at the GREGOR telescope and spectrograph in Tenerife, Spain, we carried out spectro-polarimetric measurements to obtain the four Stokes parameters in the Sr~{\\\\sc i} line at different limb distances, from $Î¼=0.2$ to $Î¼=0.8$, on the solar disk. Results.Spatial variations of the scattering polarization signal in the Sr~{\\\\sc i} 4607~Ã… line, with a spatial resolution of about 0.66\", are clearly observed at every $Î¼$. The spatial scale of these variations is comparable to the granular size. A statistical analysis reveals that the linear scattering polarization amplitude in this Sr~{\\\\sc i} spectral line is positively correlated with the intensity in the continuum, corresponding to the granules, at every $Î¼$.',\n",
       " 'This white paper briefly summarizes stellar science opportunities enabled by ultra-high resolution (sub-100 Î¼ arc-sec) astronomical imaging in the visible (U/V) wavebands. Next generation arrays of Imaging Cherenkov telescopes, to be constructed in the next decade, can provide unprecedented visible band imaging of several thousand bright (m< 6), hot (O/B/A) stars using a modern implementation of Stellar Intensity Interferometry (SII). This white paper describes the astrophysics/astronomy science opportunities that may be uncovered in this new observation space during the next decade.',\n",
       " 'A grand-challenge problem at the forefront of physics is to understand how energy is transported and transformed in plasmas. This fundamental research priority encapsulates the conversion of plasma-flow and electromagnetic energies into particle energy, either as heat or some other form of energisation. The smallest characteristic scales, at which electron dynamics determines the plasma behaviour, are the next frontier in space and astrophysical plasma research. The analysis of astrophysical processes at these scales lies at the heart of the field of electron-astrophysics. Electron scales are the ultimate bottleneck for dissipation of plasma turbulence, which is a fundamental process not understood in the electron-kinetic regime. Since electrons are the most numerous and most mobile plasma species in fully ionised plasmas and are strongly guided by the magnetic field, their thermal properties couple very efficiently to global plasma dynamics and thermodynamics.',\n",
       " 'Recent advances in telescope design, photodetector efficiency, and high-speed electronic data recording and synchronization have created the observational capability to achieve unprecedented angular resolution for several thousand bright (m< 6) and hot (O/B/A) stars by means of a modern implementation of Stellar Intensity Interferometry (SII). This technology, when deployed on future arrays of large diameter optical telescopes, has the ability to image astrophysical objects with an angular resolution better than 40 Î¼ arc-sec. This paper describes validation tests of the SII technique in the laboratory using various optical sensors and correlators, and SII measurements on nearby stars that have recently been completed as a technology demonstrator. The paper describes ongoing and future developments that will advance the impact and instrumental resolution of SII during the upcoming decade.',\n",
       " 'CSIDH is a recent quantum-resistant primitive based on the difficulty of finding isogeny paths between supersingular curves. Recently, two constant-time versions of CSIDH have been proposed: first by Meyer, Campos and Reith, and then by Onuki, Aikawa, Yamazaki and Takagi. While both offer protection against timing attacks and simple power consumption analysis, they are vulnerable to more powerful attacks such as fault injections. In this work, we identify and repair two oversights in these algorithms that compromised their constant-time character. By exploiting Edwards arithmetic and optimal addition chains, we produce the fastest constant-time version of CSIDH to date. We then consider the stronger attack scenario of fault injection, which is relevant for the security of CSIDH static keys in embedded hardware. We propose and evaluate a dummy-free CSIDH algorithm. While these CSIDH variants are slower, their performance is still within a small constant factor of less-protected variants. Finally, we discuss derandomized CSIDH algorithms.',\n",
       " 'The Simons Observatory (SO) is a ground-based cosmic microwave background (CMB) experiment sited on Cerro Toco in the Atacama Desert in Chile that promises to provide breakthrough discoveries in fundamental physics, cosmology, and astrophysics. Supported by the Simons Foundation, the Heising-Simons Foundation, and with contributions from collaborating institutions, SO will see first light in 2021 and start a five year survey in 2022. SO has 287 collaborators from 12 countries and 53 institutions, including 85 students and 90 postdocs.\\n  The SO experiment in its currently funded form (\\'SO-Nominal\\') consists of three 0.4 m Small Aperture Telescopes (SATs) and one 6 m Large Aperture Telescope (LAT). Optimized for minimizing systematic errors in polarization measurements at large angular scales, the SATs will perform a deep, degree-scale survey of 10% of the sky to search for the signature of primordial gravitational waves. The LAT will survey 40% of the sky with arc-minute resolution. These observations will measure (or limit) the sum of neutrino masses, search for light relics, measure the early behavior of Dark Energy, and refine our understanding of the intergalactic medium, clusters and the role of feedback in galaxy formation.\\n  With up to ten times the sensitivity and five times the angular resolution of the Planck satellite, and roughly an order of magnitude increase in mapping speed over currently operating (\"Stage 3\") experiments, SO will measure the CMB temperature and polarization fluctuations to exquisite precision in six frequency bands from 27 to 280 GHz. SO will rapidly advance CMB science while informing the design of future observatories such as CMB-S4.',\n",
       " 'The thermal Sunyaev-Zeldovich (SZ) effect presents a relatively new tool for characterizing galaxy cluster merger shocks, traditionally studied through X-ray observations. Widely regarded as the \"textbook example\" of a cluster merger bow shock, the western shock front in the Bullet Cluster (1E0657-56) represents the ideal test case for such an SZ study. We aim to reconstruct a parametric model for the shock SZ signal by directly and jointly fitting deep, high-resolution interferometric data from the Atacama Large Millimeter/submillimeter Array (ALMA) and Atacama Compact Array (ACA) in Fourier space. The ALMA+ACA data are primarily sensitive to the electron pressure difference across the shock front. To estimate the shock Mach number $M$, this difference can be combined with the value for the upstream electron pressure derived from an independent Chandra X-ray analysis. In the case of instantaneous electron-ion temperature equilibration, we find $M=2.08^{+0.12}_{-0.12}$, in $\\\\approx 2.4Ïƒ$ tension with the independent constraint from Chandra, $M_X=2.74\\\\pm0.25$. The assumption of purely adiabatic electron temperature change across the shock leads to $M=2.53^{+0.33}_{-0.25}$, in better agreement with the X-ray estimate $M_X=2.57\\\\pm0.23$ derived for the same heating scenario. We have demonstrated that interferometric observations of the SZ effect provide constraints on the properties of the shock in the Bullet Cluster that are highly complementary to X-ray observations. The combination of X-ray and SZ data yields a powerful probe of the shock properties, capable of measuring $M$ and addressing the question of electron-ion equilibration in cluster shocks. Our analysis is however limited by systematics related to the overall cluster geometry and the complexity of the post-shock gas distribution. To overcome these limitations, a joint analysis of SZ and X-ray data is needed.',\n",
       " 'The All-sky Medium Energy Gamma-ray Observatory (AMEGO) is a probe class mission concept that will provide essential contributions to multimessenger astrophysics in the late 2020s and beyond. AMEGO combines high sensitivity in the 200 keV to 10 GeV energy range with a wide field of view, good spectral resolution, and polarization sensitivity. Therefore, AMEGO is key in the study of multimessenger astrophysical objects that have unique signatures in the gamma-ray regime, such as neutron star mergers, supernovae, and flaring active galactic nuclei. The order-of-magnitude improvement compared to previous MeV missions also enables discoveries of a wide range of phenomena whose energy output peaks in the relatively unexplored medium-energy gamma-ray band.',\n",
       " 'We present estimates of the turbulent energy cascade rate, derived from a Hall-MHD third-order law. We compute the contribution from the Hall term and the MHD term to the energy flux. We use MMS data accumulated in the magnetosheath and the solar wind, and compare the results with previously established simulation results. We find that in observation, the MHD contribution is dominant at inertial scales, as in the simulations, but the Hall term becomes significant in observations at larger scales than in the simulations. Possible reasons are offered for this unanticipated result.',\n",
       " 'The recent development of novel extreme ultraviolet (XUV) coherent light sources bears great potential for a better understanding of the structure and dynamics of matter. Promising routes are advanced coherent control and nonlinear spectroscopy schemes in the XUV energy range, yielding unprecedented spatial and temporal resolution. However, their implementation has been hampered by the experimental challenge of generating XUV pulse sequences with precisely controlled timing and phase properties. In particular, direct control and manipulation of the phase of individual pulses within a XUV pulse sequence opens exciting new possibilities for coherent control and multidimensional spectroscopy schemes, but has not been accomplished. Here, we overcome these constraints in a highly time-stabilized and phase-modulated XUV-pump, XUV-probe experiment which directly probes the evolution and dephasing of an inner subshell electronic coherence. This new approach, avoiding any XUV optics for direct pulse manipulation, opens up extensive applications of advanced nonlinear optics and spectroscopy at XUV wavelengths.',\n",
       " 'The vulnerability to adversarial attacks has been a critical issue for deep neural networks. Addressing this issue requires a reliable way to evaluate the robustness of a network. Recently, several methods have been developed to compute $\\\\textit{robustness quantification}$ for neural networks, namely, certified lower bounds of the minimum adversarial perturbation. Such methods, however, were devised for feed-forward networks, e.g. multi-layer perceptron or convolutional networks. It remains an open problem to quantify robustness for recurrent networks, especially LSTM and GRU. For such networks, there exist additional challenges in computing the robustness quantification, such as handling the inputs at multiple steps and the interaction between gates and states. In this work, we propose $\\\\textit{POPQORN}$ ($\\\\textbf{P}$ropagated-$\\\\textbf{o}$ut$\\\\textbf{p}$ut $\\\\textbf{Q}$uantified R$\\\\textbf{o}$bustness for $\\\\textbf{RN}$Ns), a general algorithm to quantify robustness of RNNs, including vanilla RNNs, LSTMs, and GRUs. We demonstrate its effectiveness on different network architectures and show that the robustness quantification on individual steps can lead to new insights.',\n",
       " 'Conventional impurity doping of deep nanoscale silicon (dns-Si) used in ultra large scale integration (ULSI) faces serious challenges below the 14 nm technology node. We report on a new fundamental effect in theory and experiment, namely the electronic structure of dns-Si experiencing energy offsets of ca. 1 eV as a function of SiO$_2$- vs. Si$_3$N$_4$-embedding with a few monolayers (MLs). An interface charge transfer (ICT) from dns-Si specific to the anion type of the dielectric is at the core of this effect and arguably nested in quantum-chemical properties of oxygen (O) and nitrogen (N) vs. Si. We investigate the size up to which this energy offset defines the electronic structure of dns-Si by density functional theory (DFT), considering interface orientation, embedding layer thickness, and approximants featuring two Si nanocrystals (NCs); one embedded in SiO$_2$ and the other in Si$_3$N$_4$. Working with synchrotron ultraviolet photoelectron spectroscopy (UPS), we use SiO$_2$- vs. Si$_3$N$_4$-embedded Si nanowells (NWells) to obtain their energy of the top valence band states. These results confirm our theoretical findings and gauge an analytic model for projecting maximum dns-Si sizes for NCs, nanowires (NWires) and NWells where the energy offset reaches full scale, yielding to a clear preference for electrons or holes as majority carriers in dns-Si. Our findings can replace impurity doping for n/p-type dns-Si as used in ultra-low power electronics and ULSI, eliminating dopant-related issues such as inelastic carrier scattering, thermal ionization, clustering, out-diffusion and defect generation. As far as majority carrier preference is concerned, the elimination of those issues effectively shifts the lower size limit of Si-based ULSI devices to the crystalization limit of Si of ca. 1.5 nm and enables them to work also under cryogenic conditions.',\n",
       " 'We study how Reinforcement Learning can be employed to optimally control parameters in evolutionary algorithms. We control the mutation probability of a (1+1) evolutionary algorithm on the OneMax function. This problem is modeled as a Markov Decision Process and solved with Value Iteration via the known transition probabilities. It is then solved via Q-Learning, a Reinforcement Learning algorithm, where the exact transition probabilities are not needed. This approach also allows previous expert or empirical knowledge to be included into learning. It opens new perspectives, both formally and computationally, for the problem of parameter control in optimization.',\n",
       " \"We show that a black hole surrounded by scalar dark matter develops scalar hair. This is the generalization of a phenomenon pointed out by Jacobson, that a minimally coupled scalar with a non-trivial time dependence far away from the black hole would endow the black hole with hair. In our case, the time dependence arises from the oscillation of a scalar field with a non-zero mass. We systematically explore the scalar profile around the black hole for different scalar masses. In the small mass limit, the scalar field has a $1/r$ component at large radius $r$, consistent with Jacobson's result. In the large mass limit (with the Compton wavelength of order of the horizon or smaller), the scalar field has a $1/r^{3/4}$ profile yielding a pile-up close to the horizon, while distinctive nodes occur for intermediate masses. Thus, the dark matter profile around a black hole, while challenging to measure, contains information about the dark matter particle mass. As an application, we consider the case of the supermassive black hole at the center of M87, recently imaged by the Event Horizon Telescope. Its horizon size is roughly the Compton wavelength of a scalar particle of mass $10^{-20}$ eV. We consider the implications of the expected scalar pile-up close to the horizon, for fuzzy dark matter at a mass of $10^{-20}$ eV or below.\",\n",
       " 'Theory suggests that there are two primary modes of accretion through which dark matter halos acquire the gas to form and fuel galaxies, hot and cold mode accretion. In cold mode accretion, gas streams along cosmic web filaments to the center of the halo, allowing for the efficient delivery of star-forming fuel. Recently, two QSO-illuminated HI Lyman alpha (LyÎ±) emitting objects were reported to have properties of cold, rotating structures (Martin et al. 2015, Martin et al. 2016). However, the spatial and spectral resolution available was insufficient to constrain radial flows associated with connecting filaments. With the Keck Cosmic Web Imager (KCWI) we now have eight times the spatial resolution, permitting the detection of these in-spiraling flows. In order to detect these inflows, we introduce a suite of models which incorporate zonal radial flows, demonstrate their performance on a numerical simulation that exhibits coldflow accretion, and show that they are an excellent match to KCWI velocity maps of two LyÎ± emitters observed around high-redshift quasars. These Multi-Filament Inflow models kinematically isolate zones of radial inflow that correspond to extended filamentary emission. The derived gas flux and inflow path is sufficient to fuel the inferred central galaxy star formation rate and angular momentum. Thus, our kinematic emission maps provide strong evidence for the inflow of gas from the cosmic web building galaxies at the peak of star formation.',\n",
       " 'Recent developments in compact object astrophysics, especially the discovery of merging neutron stars by LIGO, the imaging of the black hole in M87 by the Event Horizon Telescope (EHT) and high precision astrometry of the Galactic Center at close to the event horizon scale by the GRAVITY experiment motivate the development of numerical source models that solve the equations of general relativistic magnetohydrodynamics (GRMHD). Here we compare GRMHD solutions for the evolution of a magnetized accretion flow where turbulence is promoted by the magnetorotational instability from a set of nine GRMHD codes: Athena++, BHAC, Cosmos++, ECHO, H-AMR, iharm3D, HARM-Noble, IllinoisGRMHD and KORAL. Agreement between the codes improves as resolution increases, as measured by a consistently applied, specially developed set of code performance metrics. We conclude that the community of GRMHD codes is mature, capable, and consistent on these test problems.',\n",
       " '(Abridged) The Maunakea Spectroscopic Explorer (MSE) is an end-to-end science platform for the design, execution and scientific exploitation of spectroscopic surveys. It will unveil the composition and dynamics of the faint Universe and impact nearly every field of astrophysics across all spatial scales, from individual stars to the largest scale structures in the Universe. Major pillars in the science program for MSE include (i) the ultimate Gaia follow-up facility for understanding the chemistry and dynamics of the distant Milky Way, including the outer disk and faint stellar halo at high spectral resolution (ii) galaxy formation and evolution at cosmic noon, via the type of revolutionary surveys that have occurred in the nearby Universe, but now conducted at the peak of the star formation history of the Universe (iii) derivation of the mass of the neutrino and insights into inflationary physics through a cosmological redshift survey that probes a large volume of the Universe with a high galaxy density. MSE is positioned to become a critical hub in the emerging international network of front-line astronomical facilities, with scientific capabilities that naturally complement and extend the scientific power of Gaia, the Large Synoptic Survey Telescope, the Square Kilometer Array, Euclid, WFIRST, the 30m telescopes and many more.',\n",
       " 'Sr I 4607Ã…spectral line shows one of the strongest scattering polarization signals in the visible solar spectrum. The amplitudes of these signals are expected to vary at granular spatial scales. This variation can be due to changes in the magnetic field intensity and orientation (Hanle effect) as well as due to spatial and temporal variations in the plasma properties. Measuring the spatial variation of such polarization signal would allow us to study the properties of the magnetic fields at subgranular region. But, the observations are challenging since both high spatial resolution and high spectropolarimetric sensitivity are required at the same time. To the aim of measuring these spatial variations at granular scale, we carried out a spectro-polarimetric measurement with the Zurich IMaging POLarimeter (ZIMPOL), at the GREGOR solar telescope at different limb distances on solar disk. Our results show a spatial variation of scattering linear polarization signals in Sr I 4607Ã…line at the granular scale at every $Î¼$, starting from 0.2 to 0.8. The correlation between the polarization signal amplitude and the continuum intensity imply statistically that the scattering polarization is higher at the granular regions than in the intergranular lanes.',\n",
       " \"The XENON experiment is looking for non-baryonic particle dark matter in the universe. The setup is a dual phase time projection chamber (TPC) filled with 3200 kg of ultra-pure liquid xenon. The setup is operated at the Laboratori Nazionali del Gran Sasso (LNGS) in Italy. We present a full overview of the computing scheme for data distribution and job management in XENON1T. The software package Rucio, which is developed by the ATLAS collaboration, facilitates data handling on Open Science Grid (OSG) and European Grid Infrastructure (EGI) storage systems. A tape copy at the Center for High Performance Computing (PDC) is managed by the Tivoli Storage Manager (TSM). Data reduction and Monte Carlo production are handled by CI Connect which is integrated into the OSG network. The job submission system connects resources at the EGI, OSG, SDSC's Comet, and the campus HPC resources for distributed computing. The previous success in the XENON1T computing scheme is also the starting point for its successor experiment XENONnT, which starts to take data in autumn 2019.\",\n",
       " 'We report the Transiting Exoplanet Survey Satellite (TESS) discovery of three terrestrial-sized planets transiting L 98-59 (TOI-175, TIC 307210830) -- a bright M dwarf at a distance of 10.6 pc. Using the Gaia-measured distance and broad-band photometry we find that the host star is an M3 dwarf. Combined with the TESS transits from three sectors, the corresponding stellar parameters yield planet radii ranging from 0.8REarth to 1.6REarth. All three planets have short orbital periods, ranging from 2.25 to 7.45 days with the outer pair just wide of a 2:1 period resonance. Diagnostic tests produced by the TESS Data Validation Report and the vetting package DAVE rule out common false positive sources. These analyses, along with dedicated follow-up and the multiplicity of the system, lend confidence that the observed signals are caused by planets transiting L 98-59 and are not associated with other sources in the field. The L 98-59 system is interesting for a number of reasons: the host star is bright (V = 11.7 mag, K = 7.1 mag) and the planets are prime targets for further follow-up observations including precision radial-velocity mass measurements and future transit spectroscopy with the James Webb Space Telescope; the near resonant configuration makes the system a laboratory to study planetary system dynamical evolution; and three planets of relatively similar size in the same system present an opportunity to study terrestrial planets where other variables (age, metallicity, etc.) can be held constant. L 98-59 will be observed in 4 more TESS sectors, which will provide a wealth of information on the three currently known planets and have the potential to reveal additional planets in the system.',\n",
       " 'This paper briefly reviews a number of fundamental measurements that need to be made in order to characterize turbulence in space plasmas such as the solar wind. It has long been known that many of these quantities require simultaneous multipoint measurements to attain a proper characterization that would reveal the fundamental physics of plasma turbulence. The solar wind is an ideal plasma for such an investigation, and it now appears to be technologically feasible to carry out such an investigation, following the pioneering Cluster and MMS missions. Quantities that need to be measured using multipoint measurements include the two-point, two-time second correlation function of velocity, magnetic field and density, and higher order statistical objects such as third and fourth order structure functions. Some details of these requirements are given here, with a eye towards achieving closure on fundamental questions regarding the cascade rate, spectral anisotropy, characteristic coherent structures, intermittency, and dissipation mechanisms that describe plasma turbuelence, as well as its variability with plasma parameters in the solar wind. The motivation for this discussion is the current planning for a proposed Helioswarm mission that would be designed to make these measurements,leading to breakthrough understanding of the physics of space and astrophysical turbulence.',\n",
       " \"Modern astronomy has finally been able to observe protoplanetary disks in reasonable resolution and detail, unveiling the processes happening during planet formation. These observed processes are understood under the framework of disk-planet interaction, a process studied analytically and modeled numerically for over 40 years. Long a theoreticians' game, the wealth of observational data has been allowing for increasingly stringent tests of the theoretical models. Modeling efforts are crucial to support the interpretation of direct imaging analyses, not just for potential detections but also to put meaningful upper limits on mass accretion rates and other physical quantities in current and future large-scale surveys. This white paper addresses the questions of what efforts on the computational side are required in the next decade to advance our theoretical understanding, explain the observational data, and guide new observations. We identified the nature of accretion, ab initio planet formation, early evolution, and circumplanetary disks as major fields of interest in computational planet formation. We recommend that modelers relax the approximations of alpha-viscosity and isothermal equations of state, on the grounds that these models use flawed assumptions, even if they give good visual qualitative agreement with observations. We similarly recommend that population synthesis move away from 1D hydrodynamics. The computational resources to reach these goals should be developed during the next decade, through improvements in algorithms and the hardware for hybrid CPU/GPU clusters. Coupled with high angular resolution and great line sensitivity in ground based interferometers, ELTs and JWST, these advances in computational efforts should allow for large strides in the field in the next decade.\",\n",
       " 'The Maunakea Spectroscopic Explorer (MSE) is a planned 11.25-m aperture facility with a 1.5 square degree field of view that will be fully dedicated to multi-object spectroscopy. A rebirth of the 3.6m Canada-France-Hawaii Telescope on Maunakea, MSE will use 4332 fibers operating at three different resolving powers (R ~ 2500, 6000, 40000) across a wavelength range of 0.36-1.8mum, with dynamical fiber positioning that allows fibers to match the exposure times of individual objects. MSE will enable spectroscopic surveys with unprecedented scale and sensitivity by collecting millions of spectra per year down to limiting magnitudes of g ~ 20-24 mag, with a nominal velocity precision of ~100 m/s in high-resolution mode. This white paper describes science cases for stellar astrophysics and exoplanet science using MSE, including the discovery and atmospheric characterization of exoplanets and substellar objects, stellar physics with star clusters, asteroseismology of solar-like oscillators and opacity-driven pulsators, studies of stellar rotation, activity, and multiplicity, as well as the chemical characterization of AGB and extremely metal-poor stars.',\n",
       " 'In this note we describe the application of existing smart contract technologies with the aim to construct a new digital representation of a financial derivative contract. We compare several existing DLT based technologies. We provide a detailed description of two separate prototypes which are able to be executed on a centralized and on a DLT platform respectively. Beyond that we highlight some insights on legal aspects as well as on common integration challenges regarding existing process and system landscapes. For a further introductory note and motivation on the theoretical concept we refer to https://www.law.ox.ac.uk/business-law-blog/blog/2018/12/smart-derivative-contract-constructing-digital-financial-derivative . A very detailed methodological overview of the concept of a smart derivative contract can be found in doi:10.2139/ssrn.3163074.',\n",
       " 'PDS 110 is a young disk-hosting star in the Orion OB1A association. Two dimming events of similar depth and duration were seen in 2008 (WASP) and 2011 (KELT), consistent with an object in a closed periodic orbit. In this paper we present data from a ground-based observing campaign designed to measure the star both photometrically and spectroscopically during the time of predicted eclipse in September 2017. Despite high-quality photometry, the predicted eclipse did not occur, although coherent structure is present suggesting variable amounts of stellar flux or dust obscuration. We also searched for RV oscillations caused by any hypothetical companion and can rule out close binaries to 0.1 $M_\\\\odot$. A search of Sonneberg plate archive data also enabled us to extend the photometric baseline of this star back more than 50 years, and similarly does not re-detect any deep eclipses. Taken together, they suggest that the eclipses seen in WASP and KELT photometry were due to aperiodic events. It would seem that PDS 110 undergoes stochastic dimmings that are shallower and shorter-duration than those of UX Ori variables, but may have a similar mechanism.',\n",
       " \"Nearly all young stars are initially surrounded by `protoplanetary' discs of gas and dust, and in the case of single stars at least 30\\\\% of these discs go on to form planets. The process of protoplanetary disc formation can result in initial misalignments, where the disc orbital plane is different to the stellar equator in single star systems, or to the binary orbital plane in systems with two stars. A quirk of the dynamics means that initially misaligned `circumbinary' discs -- those that surround two stars -- are predicted to evolve to one of two possible stable configurations, one where the disc and binary orbital planes are coplanar, and one where they are perpendicular (a `polar' configuration). Prior work has found coplanar circumbinary discs, but no polar examples were known until now. Here we report the first discovery of a protoplanetary circumbinary disc in the polar configuration, supporting the predictions that such discs should exist. The disc shows some characteristics that are similar to discs around single stars, and that are attributed to dust growth. Thus, the first stages of planet formation appear able to proceed in polar circumbinary discs.\",\n",
       " 'With the discovery of non-zero value of $Î¸_{13}$ mixing angle, the next generation of long-baseline neutrino (LBN) experiments offers the possibility of obtaining statistically significant samples of muon and electron neutrinos and anti-neutrinos with large oscillation effects. In this document we intend to highlight the importance of Near Detector facilities in LBN experiments to both constrain the systematic uncertainties affecting oscillation analyses but also to perform, thanks to their close location, measurements of broad benefit for LBN physics goals. A strong European contribution to these efforts is possible.',\n",
       " 'The future of exoplanet science is bright, as TESS once again demonstrates with the discovery of its longest-period confirmed planet to date. We hereby present HD 21749b (TOI 186.01), a sub-Neptune in a 36-day orbit around a bright (V = 8.1) nearby (16 pc) K4.5 dwarf. TESS measures HD21749b to be 2.61$^{+0.17}_{-0.16}$ $R_{\\\\oplus}$, and combined archival and follow-up precision radial velocity data put the mass of the planet at $22.7^{+2.2}_{-1.9}$ $M_{\\\\oplus}$. HD 21749b contributes to the TESS Level 1 Science Requirement of providing 50 transiting planets smaller than 4 $R_{\\\\oplus}$ with measured masses. Furthermore, we report the discovery of HD 21749c (TOI 186.02), the first Earth-sized ($R_p = 0.892^{+0.064}_{-0.058} R_{\\\\oplus}$) planet from TESS. The HD21749 system is a prime target for comparative studies of planetary composition and architecture in multi-planet systems.',\n",
       " \"With deep neural networks providing state-of-the-art machine learning models for numerous machine learning tasks, quantifying the robustness of these models has become an important area of research. However, most of the research literature merely focuses on the \\\\textit{worst-case} setting where the input of the neural network is perturbed with noises that are constrained within an $\\\\ell_p$ ball; and several algorithms have been proposed to compute certified lower bounds of minimum adversarial distortion based on such worst-case analysis. In this paper, we address these limitations and extend the approach to a \\\\textit{probabilistic} setting where the additive noises can follow a given distributional characterization. We propose a novel probabilistic framework PROVEN to PRObabilistically VErify Neural networks with statistical guarantees -- i.e., PROVEN certifies the probability that the classifier's top-1 prediction cannot be altered under any constrained $\\\\ell_p$ norm perturbation to a given input. Importantly, we show that it is possible to derive closed-form probabilistic certificates based on current state-of-the-art neural network robustness verification frameworks. Hence, the probabilistic certificates provided by PROVEN come naturally and with almost no overhead when obtaining the worst-case certified lower bounds from existing methods such as Fast-Lin, CROWN and CNN-Cert. Experiments on small and large MNIST and CIFAR neural network models demonstrate our probabilistic approach can achieve up to around $75\\\\%$ improvement in the robustness certification with at least a $99.99\\\\%$ confidence compared with the worst-case robustness certificate delivered by CROWN.\",\n",
       " 'Verifying robustness of neural network classifiers has attracted great interests and attention due to the success of deep neural networks and their unexpected vulnerability to adversarial perturbations. Although finding minimum adversarial distortion of neural networks (with ReLU activations) has been shown to be an NP-complete problem, obtaining a non-trivial lower bound of minimum distortion as a provable robustness guarantee is possible. However, most previous works only focused on simple fully-connected layers (multilayer perceptrons) and were limited to ReLU activations. This motivates us to propose a general and efficient framework, CNN-Cert, that is capable of certifying robustness on general convolutional neural networks. Our framework is general -- we can handle various architectures including convolutional layers, max-pooling layers, batch normalization layer, residual blocks, as well as general activation functions; our approach is efficient -- by exploiting the special structure of convolutional layers, we achieve up to 17 and 11 times of speed-up compared to the state-of-the-art certification algorithms (e.g. Fast-Lin, CROWN) and 366 times of speed-up compared to the dual-LP approach while our algorithm obtains similar or even better verification bounds. In addition, CNN-Cert generalizes state-of-the-art algorithms e.g. Fast-Lin and CROWN. We demonstrate by extensive experiments that our method outperforms state-of-the-art lower-bound-based certification algorithms in terms of both bound quality and speed.',\n",
       " 'The transition from monolayers to multilayered structures in bacterial colonies is a fundamental step in biofilm development. Observed across different morphotypes and species, this transition is triggered within freely growing bacterial microcolonies comprising a few hundred cells. Using a combination of numerical simulations and analytical modeling, here we demonstrate that this transition originates from the competition between growth-induced in-plane active stresses and vertical restoring forces, due to the cell-substrate interactions. Using a simple chain-like colony of laterally confined cells, we show that the transition is triggered by the mechanical instability of individual cells, thus it is localized and mechanically deterministic. Asynchronous cell division renders the process stochastic, so that all the critical parameters that control the onset of the transition are continuously distributed random variables. Upon modeling cell division as a Poisson process, we can approximately calculate the probability distribution function of the position and time associated with the first extrusion. The rate of such a Poisson process can be identified as the order parameter of the transition, thus highlighting its mixed deterministic/stochastic nature.',\n",
       " 'The electron-phonon coupling strength in the spin-split valence band maximum of single-layer MoS$_2$ is studied using angle-resolved photoemission spectroscopy and density functional theory-based calculations. Values of the electron-phonon coupling parameter $Î»$ are obtained by measuring the linewidth of the spin-split bands as a function of temperature and fitting the data points using a Debye model. The experimental values of $Î»$ for the upper and lower spin-split bands at K are found to be 0.05 and 0.32, respectively, in excellent agreement with the calculated values for a free-standing single-layer MoS$_2$. The results are discussed in the context of spin and phase-space restricted scattering channels, as reported earlier for single-layer WS$_2$ on Au(111). The fact that the absolute valence band maximum in single-layer MoS$_2$ at K is almost degenerate with the local valence band maximum at $Î“$ can potentially be used to tune the strength of the electron-phonon interaction in this material.',\n",
       " 'Finding minimum distortion of adversarial examples and thus certifying robustness in neural network classifiers for given data points is known to be a challenging problem. Nevertheless, recently it has been shown to be possible to give a non-trivial certified lower bound of minimum adversarial distortion, and some recent progress has been made towards this direction by exploiting the piece-wise linear nature of ReLU activations. However, a generic robustness certification for general activation functions still remains largely unexplored. To address this issue, in this paper we introduce CROWN, a general framework to certify robustness of neural networks with general activation functions for given input data points. The novelty in our algorithm consists of bounding a given activation function with linear and quadratic functions, hence allowing it to tackle general activation functions including but not limited to four popular choices: ReLU, tanh, sigmoid and arctan. In addition, we facilitate the search for a tighter certified lower bound by adaptively selecting appropriate surrogates for each neuron activation. Experimental results show that CROWN on ReLU networks can notably improve the certified lower bounds compared to the current state-of-the-art algorithm Fast-Lin, while having comparable computational efficiency. Furthermore, CROWN also demonstrates its effectiveness and flexibility on networks with general activation functions, including tanh, sigmoid and arctan.',\n",
       " \"Recent observational and analytical studies suggested that a new regime of kinetic turbulence may exist in plasma environments with low electron beta (Chen and Boldyrev, 2017). Such a regime, termed inertial kinetic-AlfvÃ©n turbulence, is relevant for the solar corona, Earth's magnetosheath, and other astrophysical systems where the electron and ion plasma beta parameters satisfy the condition $Î²_e\\\\ll Î²_i\\\\lesssim 1$. In this paper we present kinetic numerical simulations that confirm existence of the iKAW regime. Specifically, the simulations demonstrate a transition at scales below electron inertial length $d_e$ when $Î²_e\\\\ll Î²_i\\\\lesssim 1$. Spectral slopes and other statistical properties of turbulence at sub-$d_e$ scales are consistent with the phenomenological theory of inertial kinetic-AlfvÃ©n turbulence proposed by Chen and Boldyrev (2017) and with the recent observations in the Earth's magnetosheath.\",\n",
       " 'CLEVER (Cross-Lipschitz Extreme Value for nEtwork Robustness) is an Extreme Value Theory (EVT) based robustness score for large-scale deep neural networks (DNNs). In this paper, we propose two extensions on this robustness score. First, we provide a new formal robustness guarantee for classifier functions that are twice differentiable. We apply extreme value theory on the new formal robustness guarantee and the estimated robustness is called second-order CLEVER score. Second, we discuss how to handle gradient masking, a common defensive technique, using CLEVER with Backward Pass Differentiable Approximation (BPDA). With BPDA applied, CLEVER can evaluate the intrinsic robustness of neural networks of a broader class -- networks with non-differentiable input transformations. We demonstrate the effectiveness of CLEVER with BPDA in experiments on a 121-layer Densenet model trained on the ImageNet dataset.',\n",
       " 'Sterile neutrinos are a minimal extension of the Standard Model of Particle Physics. If their mass is in the kilo-electron-volt regime, they are viable dark matter candidates. One way to search for sterile neutrinos in a laboratory-based experiment is via tritium-beta decay, where the new neutrino mass eigenstate would manifest itself as a kink-like distortion of the $Î²$-decay spectrum. The objective of the TRISTAN project is to extend the KATRIN setup with a new multi-pixel silicon drift detector system to search for a keV-scale sterile neutrino signal. In this paper we describe the requirements of such a new detector, and present first characterization measurement results obtained with a 7-pixel prototype system.',\n",
       " 'Electromagnetic fields coupled with mechanical degrees of freedom have recently shown exceptional and innovative applications, ultimately leading to mesoscopic optomechanical devices operating in the quantum regime of motion. Simultaneously, micromechanical elements have provided new ways to enhance and manipulate the optical properties of passive photonic elements. Following this concept, in this article we show how combining a chiral metasurface with a GaAs suspended micromembrane can offer new scenarios for controlling the polarization state of near-infrared light beams. Starting from the uncommon properties of chiral metasurface to statically realize target polarization states and circular and linear dichroism, we report mechanically induced, ~300 kHz polarization modulation, which favorably compares, in terms of speed, with liquid-crystals commercial devices. Moreover, we demonstrate how the mechanical resonance can be non-trivially affected by the input light polarization (and chiral state) via a thermoelastic effect triggered by intracavity photons. This work inaugurates the field of Polarization Optomechanics, which could pave the way to fast polarimetric devices, polarization modulators and dynamically tunable chiral state generators and detectors, as well as giving access to new form of polarization nonlinearities and control.',\n",
       " \"Background: Clinical decision support systems (CDSS) are a category of health information technologies that can assist clinicians to choose optimal treatments. These support systems are based on clinical trials and expert knowledge; however, the amount of data available to these systems is limited. For this reason, CDSSs could be significantly improved by using the knowledge obtained by treating patients. This knowledge is mainly contained in patient records, whose usage is restricted due to privacy and confidentiality constraints.\\n  Methods: A treatment effectiveness measure, containing valuable information for treatment prescription, was defined and a method to extract this measure from patient records was developed. This method uses an advanced cryptographic technology, known as secure Multiparty Computation (henceforth referred to as MPC), to preserve the privacy of the patient records and the confidentiality of the clinicians' decisions.\\n  Results: Our solution enables to compute the effectiveness measure of a treatment based on patient records, while preserving privacy. Moreover, clinicians are not burdened with the computational and communication costs introduced by the privacy-preserving techniques that are used. Our system is able to compute the effectiveness of 100 treatments for a specific patient in less than 24 minutes, querying a database containing 20,000 patient records.\\n  Conclusion: This paper presents a novel and efficient clinical decision support system, that harnesses the potential and insights acquired from treatment data, while preserving the privacy of patient records and the confidentiality of clinician decisions.\",\n",
       " 'The chemical composition of an exoplanet is a key ingredient in constraining its formation history. Iron is the most abundant transition metal, but has never been directly detected in an exoplanet due to its highly refractory nature. KELT-9b (HD 195689b) is the archetype of the class of ultra-hot Jupiters that straddle the transition between stars and gas-giant exoplanets and serve as distinctive laboratories for studying atmospheric chemistry, because of its high equilibrium temperature of 4050 +/- 180 K. These properties imply that its atmosphere is a tightly constrained chemical system that is expected to be nearly in chemical equilibrium and cloud-free. It was previously predicted that the spectral lines of iron will be detectable in the visible range of wavelengths. At these high temperatures, iron and several other transition metals are not sequestered in molecules or cloud particles and exist solely in their atomic forms. Here, we report the direct detection of atomic neutral and singly-ionized iron (Fe and Fe+), and singly-ionized titanium (Ti+) in KELT-9b via the cross-correlation technique applied to high-resolution spectra obtained during the primary transit of the exoplanet.',\n",
       " 'The spin structure of the valence and conduction bands at the $\\\\overline{\\\\text{K}}$ and $\\\\overline{\\\\text{K}}$\\' valleys of single-layer WS$_2$ on Au(111) is determined by spin- and angle-resolved photoemission and inverse photoemission. The bands confining the direct band gap of 1.98 eV are out-of-plane spin polarized with spin-dependent energy splittings of 417 meV in the valence band and 16 meV in the conduction band. The sequence of the spin-split bands is the same in the valence and in the conduction bands and opposite at the $\\\\overline{\\\\text{K}}$ and the $\\\\overline{\\\\text{K}}$\\' high-symmetry points. The first observation explains \"dark\" excitons discussed in optical experiments, the latter points to coupled spin and valley physics in electron transport. The experimentally observed band dispersions are discussed along with band structure calculations for a freestanding single layer and for a single layer on Au(111).',\n",
       " \"The X-ray Integral Field Unit (X-IFU) is the high resolution X-ray spectrometer of the ESA Athena X-ray observatory. Over a field of view of 5' equivalent diameter, it will deliver X-ray spectra from 0.2 to 12 keV with a spectral resolution of 2.5 eV up to 7 keV on ~5 arcsecond pixels. The X-IFU is based on a large format array of super-conducting molybdenum-gold Transition Edge Sensors cooled at about 90 mK, each coupled with an absorber made of gold and bismuth with a pitch of 249 microns. A cryogenic anti-coincidence detector located underneath the prime TES array enables the non X-ray background to be reduced. A bath temperature of about 50 mK is obtained by a series of mechanical coolers combining 15K Pulse Tubes, 4K and 2K Joule-Thomson coolers which pre-cool a sub Kelvin cooler made of a 3He sorption cooler coupled with an Adiabatic Demagnetization Refrigerator. Frequency domain multiplexing enables to read out 40 pixels in one single channel. A photon interacting with an absorber leads to a current pulse, amplified by the readout electronics and whose shape is reconstructed on board to recover its energy with high accuracy. The defocusing capability offered by the Athena movable mirror assembly enables the X-IFU to observe the brightest X-ray sources of the sky (up to Crab-like intensities) by spreading the telescope point spread function over hundreds of pixels. Thus the X-IFU delivers low pile-up, high throughput (>50%), and typically 10 eV spectral resolution at 1 Crab intensities, i.e. a factor of 10 or more better than Silicon based X-ray detectors. In this paper, the current X-IFU baseline is presented, together with an assessment of its anticipated performance in terms of spectral resolution, background, and count rate capability. The X-IFU baseline configuration will be subject to a preliminary requirement review that is scheduled at the end of 2018.\",\n",
       " 'Neuronal activity in the brain generates synchronous oscillations of the Local Field Potential (LFP). The traditional analyses of the LFPs are based on decomposing the signal into simpler components, such as sinusoidal harmonics. However, a common drawback of such methods is that the decomposition primitives are usually presumed from the onset, which may bias our understanding of the signal\\'s structure. Here, we introduce an alternative approach that allows an impartial, high resolution, hands-off decomposition of the brain waves into a small number of discrete, frequency-modulated oscillatory processes, which we call oscillons. In particular, we demonstrate that mouse hippocampal LFP contain a single oscillon that occupies the $Î¸$-frequency band and a couple of $Î³$-oscillons that correspond, respectively, to slow and fast $Î³$-waves. Since the oscillons were identified empirically, they may represent the actual, physical structure of synchronous oscillations in neuronal ensembles, whereas Fourier-defined \"brain waves\" are nothing but poorly resolved oscillons.',\n",
       " 'We present a complete characterisation at the nanoscale of the growth and structure of single-layer tungsten disulfide (WS$_2$) epitaxially grown on Au(111). Following the growth process in real time with fast x-ray photoelectron spectroscopy, we obtain a singly-oriented layer by choosing the proper W evaporation rate and substrate temperature during the growth. Information about the morphology, size and layer stacking of the WS$_2$ layer were achieved by employing x-ray photoelectron diffraction and low-energy electron microscopy. The strong spin splitting in the valence band of WS$_2$ coupled with the single-orientation character of the layer make this material the ideal candidate for the exploitation of the spin and valley degrees of freedom.',\n",
       " 'We report on a search for Weakly Interacting Massive Particles (WIMPs) using 278.8 days of data collected with the XENON1T experiment at LNGS. XENON1T utilizes a liquid xenon time projection chamber with a fiducial mass of $(1.30 \\\\pm 0.01)$ t, resulting in a 1.0 t$\\\\times$yr exposure. The energy region of interest, [1.4, 10.6] $\\\\mathrm{keV_{ee}}$ ([4.9, 40.9] $\\\\mathrm{keV_{nr}}$), exhibits an ultra-low electron recoil background rate of $(82\\\\substack{+5 \\\\\\\\ -3}\\\\textrm{ (sys)}\\\\pm3\\\\textrm{ (stat)})$ events/$(\\\\mathrm{t}\\\\times\\\\mathrm{yr}\\\\times\\\\mathrm{keV_{ee}})$. No significant excess over background is found and a profile likelihood analysis parameterized in spatial and energy dimensions excludes new parameter space for the WIMP-nucleon spin-independent elastic scatter cross-section for WIMP masses above 6 GeV/c${}^2$, with a minimum of $4.1\\\\times10^{-47}$ cm$^2$ at 30 GeV/c${}^2$ and 90% confidence level.',\n",
       " 'Verifying the robustness property of a general Rectified Linear Unit (ReLU) network is an NP-complete problem [Katz, Barrett, Dill, Julian and Kochenderfer CAV17]. Although finding the exact minimum adversarial distortion is hard, giving a certified lower bound of the minimum distortion is possible. Current available methods of computing such a bound are either time-consuming or delivering low quality bounds that are too loose to be useful. In this paper, we exploit the special structure of ReLU networks and provide two computationally efficient algorithms Fast-Lin and Fast-Lip that are able to certify non-trivial lower bounds of minimum distortions, by bounding the ReLU units with appropriate linear functions Fast-Lin, or by bounding the local Lipschitz constant Fast-Lip. Experiments show that (1) our proposed methods deliver bounds close to (the gap is 2-3X) exact minimum distortion found by Reluplex in small MNIST networks while our algorithms are more than 10,000 times faster; (2) our methods deliver similar quality of bounds (the gap is within 35% and usually around 10%; sometimes our bounds are even better) for larger networks compared to the methods based on solving linear programming problems but our algorithms are 33-14,000 times faster; (3) our method is capable of solving large MNIST and CIFAR networks up to 7 layers with more than 10,000 neurons within tens of seconds on a single CPU core.\\n  In addition, we show that, in fact, there is no polynomial time algorithm that can approximately find the minimum $\\\\ell_1$ adversarial distortion of a ReLU network with a $0.99\\\\ln n$ approximation ratio unless $\\\\mathsf{NP}$=$\\\\mathsf{P}$, where $n$ is the number of neurons in the network.',\n",
       " 'GALAH is a large-scale magnitude-limited southern stellar spectroscopic survey. Its second data release (GALAH DR2) provides values of stellar parameters and abundances of 23 elements for 342,682 stars (Buder et al.). Here we add a description of the public release of radial velocities with a typical accuracy of 0.1 km/s for 336,215 of these stars, achievable due to the large wavelength coverage, high resolving power and good signal to noise ratio of the observed spectra, but also because convective motions in stellar atmosphere and gravitational redshift from the star to the observer are taken into account. In the process we derive medians of observed spectra which are nearly noiseless, as they are obtained from between 100 and 1116 observed spectra belonging to the same bin with a width of 50 K in temperature, 0.2 dex in gravity, and 0.1 dex in metallicity. Publicly released 1181 median spectra have a resolving power of 28,000 and trace the well-populated stellar types with metallicities between -0.6 and +0.3. Note that radial velocities from GALAH are an excellent match to the accuracy of velocity components along the sky plane derived by Gaia for the same stars. The level of accuracy achieved here is adequate for studies of dynamics within stellar clusters, associations and streams in the Galaxy. So it may be relevant for studies of the distribution of dark matter.',\n",
       " 'We present gate-controlled single, double, and triple dot operation in electrostatically gapped bilayer graphene. Thanks to the recent advancements in sample fabrication, which include the encapsulation of bilayer graphene in hexagonal boron nitride and the use of graphite gates, it has become possible to electrostatically confine carriers in bilayer graphene and to completely pinch-off current through quantum dot devices. Here, we discuss the operation and characterization of electron-hole double dots. We show a remarkable degree of control of our device, which allows the implementation of two different gate-defined electron-hole double-dot systems with very similar energy scales. In the single dot regime, we extract excited state energies and investigate their evolution in a parallel magnetic field, which is in agreement with a Zeeman-spin-splitting expected for a g-factor of two.',\n",
       " 'VS2 is a challenging material to prepare stoichiometrically in the bulk, and the single layer has not been successfully isolated before now. Here we report the first realization of single-layer VS2, which we have prepared epitaxially with high quality on Au(111) in the octahedral (1T) structure. We find that we can deplete the VS2 lattice of S by annealing in vacuum so as to create an entirely new two-dimensional compound that has no bulk analogue. The transition is reversible upon annealing in an H2S gas atmosphere. We report the structural properties of both the stoichiometric and S-depleted compounds on the basis of low-energy electron diffraction, X-ray photoelectron spectroscopy and diffraction, and scanning tunneling microscopy experiments.',\n",
       " 'Autoreactive B cells have a central role in the pathogenesis of rheumatoid arthritis (RA), and recent findings have proposed that anti-citrullinated protein autoantibodies (ACPA) may be directly pathogenic. Herein, we demonstrate the frequency of variable-region glycosylation in single-cell cloned mAbs. A total of 14 ACPA mAbs were evaluated for predicted N-linked glycosylation motifs in silico and compared to 452 highly-mutated mAbs from RA patients and controls. Variable region N-linked motifs (N-X-S/T) were strikingly prevalent within ACPA (100%) compared to somatically hypermutated (SHM) RA bone marrow plasma cells (21%), and synovial plasma cells from seropositive (39%) and seronegative RA (7%). When normalized for SHM, ACPA still had significantly higher frequency of N-linked motifs compared to all studied mAbs including highly-mutated HIV broadly-neutralizing and malaria-associated mAbs. The Fab glycans of ACPA-mAbs were highly sialylated, contributed to altered charge, but did not influence antigen binding. The analysis revealed evidence of unusual B-cell selection pressure and SHM-mediated decreased in surface charge and isoelectric point in ACPA. It is still unknown how these distinct features of anti-citrulline immunity may have an impact on pathogenesis. However, it is evident that they offer selective advantages for ACPA+ B cells, possibly also through non-antigen driven mechanisms.',\n",
       " 'Quantum teleportation establishes a correspondence between an entangled state shared by two separate par- ties that can communicate classically and the presence of a quantum channel connecting the two parties. The standard benchmark for quantum teleportation, based on the average fidelity between the input and output states, indicates that some entangled states do not lead to channels which can be certified to be quantum. It was re- cently shown that if one considers a finer-tuned witness, then all entangled states can be certified to produce a non-classical teleportation channel. Here we experimentally demonstrate a complete characterization of a new family of such witnesses, of the type proposed in Phys. Rev. Lett. 119, 110501 (2017) under different con- ditions of noise. Furthermore, we show non-classical teleportation using quantum states that can not achieve average teleportation fidelity above the classical limit. Our results have fundamental implications in quantum information protocols and may also lead to new applications and quality certification of quantum technologies.',\n",
       " 'Using GALAH survey data of nearby stars, we look at how structure in the planar (u,v) velocity distribution depends on metallicity and on viewing direction within the Galaxy. In nearby stars, with distance d < 1 kpc, the Hercules stream is most strongly seen in higher metallicity stars [Fe/H] > 0.2. The Hercules stream peak v value depends on viewed galactic longitude, which we interpret as due to the gap between the stellar stream and more circular orbits being associated with a specific angular momentum value of about 1640 km/s kpc. The association of the gap with a particular angular momentum value supports a bar resonant model for the Hercules stream.\\n  Moving groups previously identified in Hipparcos observations are easiest to see in stars nearer than 250 pc, and their visibility and peak velocities in the velocity distributions depends on both viewing direction (galactic longitude and hemisphere) and metallicity. We infer that there is fine structure in local velocity distributions that varies over distances of a few hundred pc in the Galaxy.',\n",
       " 'We propose an evolution of the Mu2e experiment, called Mu2e-II, that would leverage advances in detector technology and utilize the increased proton intensity provided by the Fermilab PIP-II upgrade to improve the sensitivity for neutrinoless muon-to-electron conversion by one order of magnitude beyond the Mu2e experiment, providing the deepest probe of charged lepton flavor violation in the foreseeable future. Mu2e-II will use as much of the Mu2e infrastructure as possible, providing, where required, improvements to the Mu2e apparatus to accommodate the increased beam intensity and cope with the accompanying increase in backgrounds.',\n",
       " 'We present a study on the growth and characterization of high-quality single-layer MoS$_2$ with a single orientation, i.e. without the presence of mirror domains. This single orientation of the MoS$_2$ layer is established by means of x-ray photoelectron diffraction. The high quality is evidenced by combining scanning tunneling microscopy with x-ray photoelectron spectroscopy measurements. Spin- and angle-resolved photoemission experiments performed on the sample revealed complete spin-polarization of the valence band states near the K and -K points of the Brillouin zone. These findings open up the possibility to exploit the spin and valley degrees of freedom for encoding and processing information in devices that are based on epitaxially grown materials.',\n",
       " 'We study the formation of epitaxial graphene on Ru(0001) using fast x-ray photoelectron spectroscopy during the growth process. The assignment of different C 1s and Ru 3d core level components and their evolution during the growth process gives a detailed insight into the graphene formation and the strongly varying graphene-Ru interaction strength within the large moire unit cell. Subsequent intercalation of oxygen can be achieved at elevated temperature and the core level spectra show a conversion of the strongly corrugated to quasi free-standing graphene, characterised by a single narrow C 1s component. This conversion and the accompanying flattening of the graphene layer is also confirmed by x-ray photoelectron diffraction. The effect of oxygen intercalation on the electronic structure is studied using angle-resolved photoemission of the valence band states. For graphene/Ru(0001), the strong graphene-substrate hybridisation disrupts the Ï€-band dispersion but oxygen intercalation fully restores the Ï€-band with a strong p-doping that shifts the Dirac point 785 meV above the Fermi level. The doping of the system is highly tunable, as the additional exposure to rubidium can convert the carrier filling to n-type with the Dirac point 970 meV below the Fermi level.',\n",
       " 'The robustness of neural networks to adversarial examples has received great attention due to security implications. Despite various attack approaches to crafting visually imperceptible adversarial examples, little has been developed towards a comprehensive measure of robustness. In this paper, we provide a theoretical justification for converting robustness analysis into a local Lipschitz constant estimation problem, and propose to use the Extreme Value Theory for efficient evaluation. Our analysis yields a novel robustness metric called CLEVER, which is short for Cross Lipschitz Extreme Value for nEtwork Robustness. The proposed CLEVER score is attack-agnostic and computationally feasible for large neural networks. Experimental results on various networks, including ResNet, Inception-v3 and MobileNet, show that (i) CLEVER is aligned with the robustness indication measured by the $\\\\ell_2$ and $\\\\ell_\\\\infty$ norms of adversarial examples from powerful attacks, and (ii) defended networks using defensive distillation or bounded ReLU indeed achieve better CLEVER scores. To the best of our knowledge, CLEVER is the first attack-independent robustness metric that can be applied to any neural network classifier.',\n",
       " 'The KATRIN (Karlsruhe Tritium Neutrino) experiment investigates the energetic endpoint of the tritium $Î²$-decay spectrum to determine the effective mass of the electron anti-neutrino with a precision of $200\\\\,\\\\mathrm{meV}$ ($90\\\\,\\\\%$ C.L.) after an effective data taking time of three years.\\n  The TRISTAN (tritium $Î²$-decay to search for sterile neutrinos) group aims to detect a sterile neutrino signature by measuring the entire tritium $Î²$-decay spectrum with an upgraded KATRIN system. One of the greatest challenges is to handle the high signal rates generated by the strong activity of the KATRIN tritium source. Therefore, a novel multi-pixel silicon drift detector is being designed, which is able to handle rates up to $10^{8}\\\\,\\\\mathrm{cps}$ with an excellent energy resolution of $<200\\\\,\\\\mathrm{eV}$ (FWHM) at $10\\\\,\\\\mathrm{keV}$.\\n  This work gives an overview of the ongoing detector development and test results of the first seven pixel prototype detectors.',\n",
       " 'The 11th Summer Workshop on Multimodal Interfaces eNTERFACE 2015 was hosted by the Numediart Institute of Creative Technologies of the University of Mons from August 10th to September 2015. During the four weeks, students and researchers from all over the world came together in the Numediart Institute of the University of Mons to work on eight selected projects structured around intelligent interfaces. Eight projects were selected and their reports are shown here.',\n",
       " 'This work aims to design the uplink (UL) of a cellular network for maximal energy efficiency (EE). Each base station (BS) is randomly deployed within a given area and is equipped with $M$ antennas to serve $K$ user equipments (UEs). A multislope (distance-dependent) path loss model is considered and linear processing is used, under the assumption that channel state information is acquired by using pilot sequences (reused across the network). Within this setting, a lower bound on the UL spectral efficiency and a realistic circuit power consumption model are used to evaluate the network EE. Numerical results are first used to compute the optimal BS density and pilot reuse factor for a Massive MIMO network with three different detection schemes, namely, maximum ratio combining, zero-forcing (ZF) and multicell minimum mean-squared error. The numerical analysis shows that the EE is a unimodal function of BS density and achieves its maximum for a relatively small density of BS, irrespective of the employed detection scheme. This is in contrast to the single-slope (distance-independent) path loss model, for which the EE is a monotonic non-decreasing function of BS density. Then, we concentrate on ZF and use stochastic geometry to compute a new lower bound on the spectral efficiency, which is then used to optimize, for a given BS density, the pilot reuse factor, number of BS antennas and UEs. Closed- form expressions are computed from which valuable insights into the interplay between optimization variables, hardware characteristics, and propagation environment are obtained.',\n",
       " 'Particle physics has an ambitious and broad experimental programme for the coming decades. This programme requires large investments in detector hardware, either to build new facilities and experiments, or to upgrade existing ones. Similarly, it requires commensurate investment in the R&D of software to acquire, manage, process, and analyse the shear amounts of data to be recorded. In planning for the HL-LHC in particular, it is critical that all of the collaborating stakeholders agree on the software goals and priorities, and that the efforts complement each other. In this spirit, this white paper describes the R&D activities required to prepare for this software upgrade.',\n",
       " 'Accurate and precise radius estimates of transiting exoplanets are critical for understanding their compositions and formation mechanisms. To know the planet, we must know the host star in as much detail as possible. We present first results from the K2-HERMES project, which uses the HERMES multi-object spectrograph on the Anglo-Australian Telescope to obtain R$\\\\sim$28,000 spectra of up to 360 stars in one exposure. This ongoing project aims to derive self-consistent spectroscopic parameters for about half of K2 target stars. We present complete stellar parameters and isochrone-derived masses and radii for 46 stars hosting 57 K2 candidate planets in Campaigns 1-3. Our revised host-star radii cast severe doubt on three candidate planets: EPIC\\\\,201407812.01, EPIC\\\\,203070421.01, and EPIC\\\\,202843107.01, all of which now have inferred radii well in excess of the largest known inflated Jovian planets.',\n",
       " 'We introduce The House Of inteRactions (THOR), a framework for visual AI research, available at http://ai2thor.allenai.org. AI2-THOR consists of near photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes and interact with objects to perform tasks. AI2-THOR enables research in many different domains including but not limited to deep reinforcement learning, imitation learning, learning by interaction, planning, visual question answering, unsupervised representation learning, object detection and segmentation, and learning models of cognition. The goal of AI2-THOR is to facilitate building visually intelligent models and push the research forward in this domain.',\n",
       " 'We investigate the effect of the formation of metal droplets on the growth dynamics of InGaN by Plasma-Assisted Molecular Beam Epitaxy (PAMBE) at low temperatures (T = 450Â°C). We find that the presence of droplets on the growth surface strongly affects the adatom incorporation dynamics, making the growth rate a decreasing function of the overall metal flux impinging on the surface as soon as the metal dose exceeds the critical amount required for the nucleation of droplets. We explain this phenomenon via a model that takes into account droplet effects on the incorporation of metal adatoms into the crystal. A relevant role is played by the vapor-liquid-solid growth mode that takes place under the droplets due to nitrogen molecules directly impinging on the droplets.',\n",
       " 'Solving inverse problems is central to geosciences and remote sensing. Radiative transfer models (RTMs) represent mathematically the physical laws which govern the phenomena in remote sensing applications (forward models). The numerical inversion of the RTM equations is a challenging and computationally demanding problem, and for this reason, often the application of a nonlinear statistical regression is preferred. In general, regression models predict the biophysical parameter of interest from the corresponding received radiance. However, this approach does not employ the physical information encoded in the RTMs. An alternative strategy, which attempts to include the physical knowledge, consists in learning a regression model trained using data simulated by an RTM code. In this work, we introduce a nonlinear nonparametric regression model which combines the benefits of the two aforementioned approaches. The inversion is performed taking into account jointly both real observations and RTM-simulated data. The proposed Joint Gaussian Process (JGP) provides a solid framework for exploiting the regularities between the two types of data. The JGP automatically detects the relative quality of the simulated and real data, and combines them accordingly. This occurs by learning an additional hyper-parameter w.r.t. a standard GP model, and fitting parameters through maximizing the pseudo-likelihood of the real observations. The resulting scheme is both simple and robust, i.e., capable of adapting to different scenarios. The advantages of the JGP method compared to benchmark strategies are shown considering RTM-simulated and real observations in different experiments. Specifically, we consider leaf area index (LAI) retrieval from Landsat data combined with simulated data generated by the PROSAIL model.',\n",
       " 'Several semiconductor quantum dot techniques have been investigated for the generation of entangled photon pairs. Among the other techniques, droplet epitaxy enables the control of the shape, size, density, and emission wavelength of the quantum emitters. However, the fraction of the entanglement-ready quantum dots that can be fabricated with this method is still limited to around 5%, and matching the energy of the entangled photons to atomic transitions (a promising route towards quantum networking) remains an outstanding challenge.\\n  Here, we overcome these obstacles by introducing a modified approach to droplet epitaxy on a high symmetry (111)A substrate, where the fundamental crystallization step is performed at a significantly higher temperature as compared to previous reports. Our method drastically improves the yield of entanglement-ready photon sources near the emission wavelength of interest, which can be as high as 95% due to the low values of fine structure splitting and radiative lifetime, together with the reduced exciton dephasing offered by the choice of GaAs/AlGaAs materials. The quantum dots are designed to emit in the operating spectral region of Rb-based slow-light media, providing a viable technology for quantum repeater stations.',\n",
       " 'The Cherenkov Telescope Array, CTA, will be the major global observatory for very high energy gamma-ray astronomy over the next decade and beyond. The scientific potential of CTA is extremely broad: from understanding the role of relativistic cosmic particles to the search for dark matter. CTA is an explorer of the extreme universe, probing environments from the immediate neighbourhood of black holes to cosmic voids on the largest scales. Covering a huge range in photon energy from 20 GeV to 300 TeV, CTA will improve on all aspects of performance with respect to current instruments.\\n  The observatory will operate arrays on sites in both hemispheres to provide full sky coverage and will hence maximize the potential for the rarest phenomena such as very nearby supernovae, gamma-ray bursts or gravitational wave transients. With 99 telescopes on the southern site and 19 telescopes on the northern site, flexible operation will be possible, with sub-arrays available for specific tasks. CTA will have important synergies with many of the new generation of major astronomical and astroparticle observatories. Multi-wavelength and multi-messenger approaches combining CTA data with those from other instruments will lead to a deeper understanding of the broad-band non-thermal properties of target sources.\\n  The CTA Observatory will be operated as an open, proposal-driven observatory, with all data available on a public archive after a pre-defined proprietary period. Scientists from institutions worldwide have combined together to form the CTA Consortium. This Consortium has prepared a proposal for a Core Programme of highly motivated observations. The programme, encompassing approximately 40% of the available observing time over the first ten years of CTA operation, is made up of individual Key Science Projects (KSPs), which are presented in this document.',\n",
       " \"Compressed bitmap indexes are used in systems such as Git or Oracle to accelerate queries. They represent sets and often support operations such as unions, intersections, differences, and symmetric differences. Several important systems such as Elasticsearch, Apache Spark, Netflix's Atlas, LinkedIn's Pivot, Metamarkets' Druid, Pilosa, Apache Hive, Apache Tez, Microsoft Visual Studio Team Services and Apache Kylin rely on a specific type of compressed bitmap index called Roaring. We present an optimized software library written in C implementing Roaring bitmaps: CRoaring. It benefits from several algorithms designed for the single-instruction-multiple-data (SIMD) instructions available on commodity processors. In particular, we present vectorized algorithms to compute the intersection, union, difference and symmetric difference between arrays. We benchmark the library against a wide range of competitive alternatives, identifying weaknesses and strengths in our software. Our work is available under a liberal open-source license.\",\n",
       " 'Next generation wireless networks aim at providing substantial improvements in spectral efficiency (SE) and energy efficiency (EE). Massive MIMO has been proved to be a viable technology to achieve these goals by spatially multiplexing several users using many base station (BS) antennas. A potential limitation of Massive MIMO in multicell systems is pilot contamination, which arises in the channel estimation process from the interference caused by reusing pilots in neighboring cells. A standard method to reduce pilot contamination, known as regular pilot (RP), is to adjust the length of pilot sequences while transmitting data and pilot symbols disjointly. An alternative method, called superimposed pilot (SP), sends a superposition of pilot and data symbols. This allows to use longer pilots which, in turn, reduces pilot contamination. We consider the uplink of a multicell Massive MIMO network using maximum ratio combining detection and compare RP and SP in terms of SE and EE. To this end, we derive rigorous closed-form achievable rates with SP under a practical random BS deployment. We prove that the reduction of pilot contamination with SP is outweighed by the additional coherent and non-coherent interference. Numerical results show that when both methods are optimized, RP achieves comparable SE and EE to SP in practical scenarios.',\n",
       " 'This work aims to design a cellular network for maximal energy efficiency (EE). In particular, we consider the uplink with multi-antenna base stations and assume that zero- forcing (ZF) combining is used for data detection with imperfect channel state information. Using stochastic geometry and a new lower bound on the average per-user spectral efficiency of the network, we optimize the pilot reuse factor, number of antennas and users per base station. Closed-form expressions are computed from which valuable insights into the interplay between the optimization variables, hardware characteristics, and propagation environment are obtained. Numerical results are used to validate the analysis and make comparisons with a network using maximum ratio (MR) combining. The results show that a Massive MIMO setup arises as the EE-optimal network configuration. In addition, ZF provides higher EE than MR while allowing a smaller pilot reuse factor and a more dense network deployment.',\n",
       " 'List of contributions from the Cherenkov Telescope Array Consortium presented at the 35th International Cosmic Ray Conference, July 12-20 2017, Busan, Korea.',\n",
       " 'The encapsulation of graphene based Hall sensors on foil is shown to be an effective method for improving the performance in terms of higher sensitivity for magnetic field detection. Two types of encapsulation were investigated: a simple encapsulation of graphene with polymethyl methacrylate (PMMA) as a proof of concept and an encapsulation with mechanically exfoliated hexagonal boron nitride (hBN). The Hall sensor with PMMA encapsulation already shows higher sensitivity compared to the one without encapsulation. However, the Hall sensor with graphene encapsulated between two stacks of hBN shows a current and a voltage normalized sensitivity of up to 2270 V/AT and 0.68 V/VT respectively, which are the highest reported sensitivity values for Hall sensors on foil so far.',\n",
       " 'We propose a new algorithm for the computation of a singular value decomposition (SVD) low-rank approximation of a matrix in the Matrix Product Operator (MPO) format, also called the Tensor Train Matrix format. Our tensor network randomized SVD (TNrSVD) algorithm is an MPO implementation of the randomized SVD algorithm that is able to compute dominant singular values and their corresponding singular vectors. In contrast to the state-of-the-art tensor-based alternating least squares SVD (ALS-SVD) and modified alternating least squares SVD (MALS-SVD) matrix approximation methods, TNrSVD can be up to 17 times faster while achieving the same accuracy. In addition, our TNrSVD algorithm also produces accurate approximations in particular cases where both ALS-SVD and MALS-SVD fail to converge. We also propose a new algorithm for the fast conversion of a sparse matrix into its corresponding MPO form, which is up to 509 times faster than the standard Tensor Train SVD (TT-SVD) method while achieving machine precision accuracy. The efficiency and accuracy of both algorithms are demonstrated in numerical experiments.',\n",
       " 'This white paper summarizes the workshop \"U.S. Cosmic Visions: New Ideas in Dark Matter\" held at University of Maryland on March 23-25, 2017.',\n",
       " 'Lensing of the CMB is now a well-developed probe of large-scale clustering over a broad range of redshifts. By exploiting the non-Gaussian imprints of lensing in the polarization of the CMB, the CORE mission can produce a clean map of the lensing deflections over nearly the full-sky. The number of high-S/N modes in this map will exceed current CMB lensing maps by a factor of 40, and the measurement will be sample-variance limited on all scales where linear theory is valid. Here, we summarise this mission product and discuss the science that it will enable. For example, the summed mass of neutrinos will be determined to an accuracy of 17 meV combining CORE lensing and CMB two-point information with contemporaneous BAO measurements, three times smaller than the minimum total mass allowed by neutrino oscillations. In the search for B-mode polarization from primordial gravitational waves with CORE, lens-induced B-modes will dominate over instrument noise, limiting constraints on the gravitational wave power spectrum amplitude. With lensing reconstructed by CORE, one can \"delens\" the observed polarization internally, reducing the lensing B-mode power by 60%. This improves to 70% by combining lensing and CIB measurements from CORE, reducing the error on the gravitational wave amplitude by 2.5 compared to no delensing (in the null hypothesis). Lensing measurements from CORE will allow calibration of the halo masses of the 40000 galaxy clusters that it will find, with constraints dominated by the clean polarization-based estimators. CORE can accurately remove Galactic emission from CMB maps with its 19 frequency channels. We present initial findings that show that residual Galactic foreground contamination will not be a significant source of bias for lensing power spectrum measurements with CORE. [abridged]',\n",
       " 'Realistic oxide materials are often semiconductors, in particular at elevated temperatures, and their surfaces contain undercoordiated atoms at structural defects such as steps and corners. Using hybrid density-functional theory and ab initio atomistic thermodynamics, we investigate the interplay of bond-making, bond-breaking, and charge-carrier trapping at the corner defects at the (100) surface of a p-doped MgO in thermodynamic equilibrium with an O2 atmosphere. We show that by manipulating the coordination of surface atoms one can drastically change and even reverse the order of stability of reduced versus oxidized surface sites.',\n",
       " 'Photonic interference is a key quantum resource for optical quantum computation, and in particular for so-called boson sampling machines. In interferometers with certain symmetries, genuine multiphoton quantum interference effectively suppresses certain sets of events, as in the original Hong-Ou-Mandel effect. Recently, it was shown that some classical and semi-classical models could be ruled out by identifying such suppressions in Fourier interferometers. Here we propose a suppression law suitable for random-input experiments in multimode Sylvester interferometers, and verify it experimentally using 4- and 8-mode integrated interferometers. The observed suppression is stronger than what is observed in Fourier interferometers of the same size, and could be relevant to certification of boson sampling machines and other experiments relying on bosonic interference.',\n",
       " 'Injuries have a great impact on professional soccer, due to their large influence on team performance and the considerable costs of rehabilitation for players. Existing studies in the literature provide just a preliminary understanding of which factors mostly affect injury risk, while an evaluation of the potential of statistical models in forecasting injuries is still missing. In this paper, we propose a multi-dimensional approach to injury forecasting in professional soccer that is based on GPS measurements and machine learning. By using GPS tracking technology, we collect data describing the training workload of players in a professional soccer club during a season. We then construct an injury forecaster and show that it is both accurate and interpretable by providing a set of case studies of interest to soccer practitioners. Our approach opens a novel perspective on injury prevention, providing a set of simple and practical rules for evaluating and interpreting the complex relations between injury risk and training performance in professional soccer.',\n",
       " 'We present a new method to locate the starting points in time of an arbitrary number of (damped) delayed signals. For a finite data sequence, the method permits to first locate the starting point of the component with the longest delay, and then --by iteration-- all the preceding ones. Numerical examples are given and noise sensitivity is tested for weak noise.',\n",
       " 'Bacterial colonies are abundant on living and nonliving surfaces and are known to mediate a broad range of processes in ecology, medicine, and industry. Although extensively researched, from single cells to demographic scales, a comprehensive biomechanical picture, highlighting the cell-to-colony dynamics, is still lacking. Here, using molecular dynamics simulations and continuous modeling, we investigate the geometrical and mechanical properties of a bacterial colony growing on a substrate with a free boundary and demonstrate that such an expanding colony self-organizes into a \"mosaic\" of microdomains consisting of highly aligned cells. The emergence of microdomains is mediated by two competing forces: the steric forces between neighboring cells, which favor cell alignment, and the extensile stresses due to cell growth that tend to reduce the local orientational order and thereby distort the system. This interplay results in an exponential distribution of the domain areas and sets a characteristic length scale proportional to the square root of the ratio between the system orientational stiffness and the magnitude of the extensile active stress. Our theoretical predictions are finally compared with experiments with freely growing E. coli microcolonies, finding quantitative agreement.',\n",
       " 'We forecast the scientific capabilities to improve our understanding of cosmic inflation of CORE, a proposed CMB space satellite submitted in response to the ESA fifth call for a medium-size mission opportunity. The CORE satellite will map the CMB anisotropies in temperature and polarization in 19 frequency channels spanning the range 60-600 GHz. CORE will have an aggregate noise sensitivity of $1.7 Î¼$K$\\\\cdot \\\\,$arcmin and an angular resolution of 5\\' at 200 GHz. We explore the impact of telescope size and noise sensitivity on the inflation science return by making forecasts for several instrumental configurations. This study assumes that the lower and higher frequency channels suffice to remove foreground contaminations and complements other related studies of component separation and systematic effects, which will be reported in other papers of the series \"Exploring Cosmic Origins with CORE.\" We forecast the capability to determine key inflationary parameters, to lower the detection limit for the tensor-to-scalar ratio down to the $10^{-3}$ level, to chart the landscape of single field slow-roll inflationary models, to constrain the epoch of reheating, thus connecting inflation to the standard radiation-matter dominated Big Bang era, to reconstruct the primordial power spectrum, to constrain the contribution from isocurvature perturbations to the $10^{-3}$ level, to improve constraints on the cosmic string tension to a level below the presumptive GUT scale, and to improve the current measurements of primordial non-Gaussianities down to the $f_{NL}^{\\\\rm local} < 1$ level. For all the models explored, CORE alone will improve significantly on the present constraints on the physics of inflation. Its capabilities will be further enhanced by combining with complementary future cosmological observations.',\n",
       " \"The advent of space-based missions like $Kepler$ has revolutionized the study of solar-type stars, particularly through the measurement and modeling of their resonant modes of oscillation. Here we analyze a sample of 66 $Kepler$ main-sequence stars showing solar-like oscillations as part of the $Kepler$ seismic LEGACY project. We use $Kepler$ short-cadence data, of which each star has at least 12 months, to create frequency power spectra optimized for asteroseismology. For each star we identify its modes of oscillation and extract parameters such as frequency, amplitude, and line width using a Bayesian Markov chain Monte Carlo `peak-bagging' approach. We report the extracted mode parameters for all 66 stars, as well as derived quantities such as frequency difference ratios, the large and small separations $Î”Î½$ and $Î´Î½_{02}$; the behavior of line widths with frequency and line widths at $Î½_{\\\\rm max}$ with $T_{\\\\rm eff}$, for which we derive parametrizations; and behavior of mode visibilities. These average properties can be applied in future peak-bagging exercises to better constrain the parameters of the stellar oscillation spectra. The frequencies and frequency ratios can tightly constrain the fundamental parameters of these solar-type stars, and mode line widths and amplitudes can test models of mode damping and excitation.\",\n",
       " \"We forecast the main cosmological parameter constraints achievable with the CORE space mission which is dedicated to mapping the polarisation of the Cosmic Microwave Background (CMB). CORE was recently submitted in response to ESA's fifth call for medium-sized mission proposals (M5). Here we report the results from our pre-submission study of the impact of various instrumental options, in particular the telescope size and sensitivity level, and review the great, transformative potential of the mission as proposed. Specifically, we assess the impact on a broad range of fundamental parameters of our Universe as a function of the expected CMB characteristics, with other papers in the series focusing on controlling astrophysical and instrumental residual systematics. In this paper, we assume that only a few central CORE frequency channels are usable for our purpose, all others being devoted to the cleaning of astrophysical contaminants. On the theoretical side, we assume LCDM as our general framework and quantify the improvement provided by CORE over the current constraints from the Planck 2015 release. We also study the joint sensitivity of CORE and of future Baryon Acoustic Oscillation and Large Scale Structure experiments like DESI and Euclid. Specific constraints on the physics of inflation are presented in another paper of the series. In addition to the six parameters of the base LCDM, which describe the matter content of a spatially flat universe with adiabatic and scalar primordial fluctuations from inflation, we derive the precision achievable on parameters like those describing curvature, neutrino physics, extra light relics, primordial helium abundance, dark matter annihilation, recombination physics, variation of fundamental constants, dark energy, modified gravity, reionization and cosmic birefringence. (ABRIDGED)\",\n",
       " 'We use asteroseismic data from the Kepler satellite to determine fundamental stellar properties of the 66 main-sequence targets observed for at least one full year by the mission. We distributed tens of individual oscillation frequencies extracted from the time series of each star among seven modelling teams who applied different methods to determine radii, masses, and ages for all stars in the sample. Comparisons among the different results reveal a good level of agreement in all stellar properties, which is remarkable considering the variety of codes, input physics and analysis methods employed by the different teams. Average uncertainties are of the order of $\\\\sim$2\\\\% in radius, $\\\\sim$4\\\\% in mass, and $\\\\sim$10\\\\% in age, making this the best-characterised sample of main-sequence stars available to date. Our predicted initial abundances and mixing-length parameters are checked against inferences from chemical enrichment laws $Î”Y / Î”Z$ and predictions from 3D atmospheric simulations. We test the accuracy of the determined stellar properties by comparing them to the Sun, angular diameter measurements, Gaia parallaxes, and binary evolution, finding excellent agreement in all cases and further confirming the robustness of asteroseismically-determined physical parameters of stars when individual frequencies of oscillation are available. Baptised as the Kepler dwarfs LEGACY sample, these stars are the solar-like oscillators with the best asteroseismic properties available for at least another decade. All data used in this analysis and the resulting stellar parameters are made publicly available for the community.',\n",
       " 'Fabrication process variations are a major source of yield degradation in the nano-scale design of integrated circuits (IC), microelectromechanical systems (MEMS) and photonic circuits. Stochastic spectral methods are a promising technique to quantify the uncertainties caused by process variations. Despite their superior efficiency over Monte Carlo for many design cases, these algorithms suffer from the curse of dimensionality; i.e., their computational cost grows very fast as the number of random parameters increases. In order to solve this challenging problem, this paper presents a high-dimensional uncertainty quantification algorithm from a big-data perspective. Specifically, we show that the huge number of (e.g., $1.5 \\\\times 10^{27}$) simulation samples in standard stochastic collocation can be reduced to a very small one (e.g., $500$) by exploiting some hidden structures of a high-dimensional data array. This idea is formulated as a tensor recovery problem with sparse and low-rank constraints; and it is solved with an alternating minimization approach. Numerical results show that our approach can simulate efficiently some ICs, as well as MEMS and photonic problems with over 50 independent random parameters, whereas the traditional algorithm can only handle several random parameters.',\n",
       " 'We report on a scanning confocal Raman spectroscopy study investigating the strain-uniformity and the overall strain and doping of high-quality chemical vapour deposited (CVD) graphene-based heterostuctures on a large number of different substrate materials, including hexagonal boron nitride (hBN), transition metal dichalcogenides, silicon, different oxides and nitrides, as well as polymers. By applying a hBN-assisted, contamination free, dry transfer process for CVD graphene, high-quality heterostructures with low doping densities and low strain variations are assembled. The Raman spectra of these pristine heterostructures are sensitive to substrate-induced doping and strain variations and are thus used to probe the suitability of the substrate material for potential high-quality graphene devices. We find that the flatness of the substrate material is a key figure for gaining, or preserving high-quality graphene.',\n",
       " 'We present Keck/DEIMOS spectroscopy of individual stars in the relatively isolated Local Group dwarf galaxies Leo A, Aquarius, and the Sagittarius dwarf irregular galaxy. The three galaxies--but especially Leo A and Aquarius--share in common delayed star formation histories relative to many other isolated dwarf galaxies. The stars in all three galaxies are supported by dispersion. We found no evidence of stellar velocity structure, even for Aquarius, which has rotating HI gas. The velocity dispersions indicate that all three galaxies are dark matter-dominated, with dark-to-baryonic mass ratios ranging from $4.4^{+1.1}_{-0.8}$ (SagDIG) to $9.6^{+2.5}_{-1.8}$ (Aquarius). Leo A and SagDIG have lower stellar metallicities than Aquarius, and they also have higher gas fractions, both of which would be expected if Aquarius were farther along in its chemical evolution. The metallicity distribution of Leo A is inconsistent with a Closed or Leaky Box model of chemical evolution, suggesting that the galaxy was pre-enriched or acquired external gas during star formation. The metallicities of stars increased steadily for all three galaxies, but possibly at different rates. The [$Î±$/Fe] ratios at a given [Fe/H] are lower than that of the Sculptor dwarf spheroidal galaxy, which indicates more extended star formation histories than Sculptor, consistent with photometrically derived star formation histories. Overall, the bulk kinematic and chemical properties for the late-forming dwarf galaxies do not diverge significantly from those of less delayed dwarf galaxies, including dwarf spheroidal galaxies.',\n",
       " 'The physical parameters of the retired A star HD 185351 were analysed in great detail by Johnson et al. (2014) using interferometry, spectroscopy and asteroseismology. Results from all independent methods are consistent with HD 185351 having a mass in excess of $1.5\\\\mathrm{M}_{\\\\odot}$. However, the study also showed that not all observational constraints could be reconciled in stellar evolutionary models, leading to mass estimates ranging from $\\\\sim 1.6-1.9\\\\mathrm{M}_{\\\\odot}$ and casting doubts on the accuracy of stellar properties determined from asteroseismology. Here we solve this discrepancy and construct a theoretical model in agreement with all observational constraints on the physical parameters of HD 185351. The effects of varying input physics are examined as well as considering the additional constraint of the observed g-mode period spacing. This quantity is found to be sensitive to the inclusion of additional mixing from the convective core during the main sequence, and can be used to calibrate the overshooting efficiency using low-luminosity red giant stars. A theoretical model with metallicity $\\\\left[\\\\mathrm{Fe/H}\\\\right]=0.16$dex, mixing-length parameter $Î±_{\\\\mathrm{MLT}}=2.00$, and convective overshooting efficiency parameter $f=0.030$ is found to be in complete agreement with all observational constraints for a stellar mass of $M\\\\simeq1.60\\\\mathrm{M}_{\\\\odot}$.',\n",
       " 'List of contributions from the Cherenkov Telescope Array (CTA) Consortium presented at the 6th International Symposium on High-Energy Gamma-Ray Astronomy (Gamma 2016), July 11-15, 2016, in Heidelberg, Germany.',\n",
       " 'Many critical EDA problems suffer from the curse of dimensionality, i.e. the very fast-scaling computational burden produced by large number of parameters and/or unknown variables. This phenomenon may be caused by multiple spatial or temporal factors (e.g. 3-D field solvers discretizations and multi-rate circuit simulation), nonlinearity of devices and circuits, large number of design or optimization parameters (e.g. full-chip routing/placement and circuit sizing), or extensive process variations (e.g. variability/reliability analysis and design for manufacturability). The computational challenges generated by such high dimensional problems are generally hard to handle efficiently with traditional EDA core algorithms that are based on matrix and vector computation. This paper presents \"tensor computation\" as an alternative general framework for the development of efficient EDA algorithms and tools. A tensor is a high-dimensional generalization of a matrix and a vector, and is a natural choice for both storing and solving efficiently high-dimensional EDA problems. This paper gives a basic tutorial on tensors, demonstrates some recent examples of EDA applications (e.g., nonlinear circuit modeling and high-dimensional uncertainty quantification), and suggests further open EDA problems where the use of tensor computation could be of advantage.',\n",
       " \"The Galactic Archaeology with HERMES (GALAH) Survey is a massive observational project to trace the Milky Way's history of star formation, chemical enrichment, stellar migration and minor mergers. Using high-resolution (R$\\\\simeq$28,000) spectra taken with the High Efficiency and Resolution Multi-Element Spectrograph (HERMES) instrument at the Anglo-Australian Telescope (AAT), GALAH will determine stellar parameters and abundances of up to 29 elements for up to one million stars. Selecting targets from a colour-unbiased catalogue built from 2MASS, APASS and UCAC4 data, we expect to observe dwarfs at 0.3 to 3 kpc and giants at 1 to 10 kpc. This enables a thorough local chemical inventory of the Galactic thin and thick disks, and also captures smaller samples of the bulge and halo. In this paper we present the plan, process and progress as of early 2016 for GALAH survey observations. In our first two years of survey observing we have accumulated the largest high-quality spectroscopic data set at this resolution, over 200,000 stars. We also present the first public GALAH data catalogue: stellar parameters (Teff, log(g), [Fe/H], [alpha/Fe]), radial velocity, distance modulus and reddening for 10680 observations of 9860 Tycho-2 stars that may be included in the first Gaia data release.\",\n",
       " 'This report, based on the Dark Sectors workshop at SLAC in April 2016, summarizes the scientific importance of searches for dark sector dark matter and forces at masses beneath the weak-scale, the status of this broad international field, the important milestones motivating future exploration, and promising experimental opportunities to reach these milestones over the next 5-10 years.',\n",
       " 'The X-ray Integral Field Unit (X-IFU) on board the Advanced Telescope for High-ENergy Astrophysics (Athena) will provide spatially resolved high-resolution X-ray spectroscopy from 0.2 to 12 keV, with 5 arc second pixels over a field of view of 5 arc minute equivalent diameter and a spectral resolution of 2.5 eV up to 7 keV. In this paper, we first review the core scientific objectives of Athena, driving the main performance parameters of the X-IFU, namely the spectral resolution, the field of view, the effective area, the count rate capabilities, the instrumental background. We also illustrate the breakthrough potential of the X-IFU for some observatory science goals. Then we briefly describe the X-IFU design as defined at the time of the mission consolidation review concluded in May 2016, and report on its predicted performance. Finally, we discuss some options to improve the instrument performance while not increasing its complexity and resource demands (e.g. count rate capability, spectral resolution).\\n  The X-IFU will be provided by an international consortium led by France, The Netherlands and Italy, with further ESA member state contributions from Belgium, Finland, Germany, Poland, Spain, Switzerland and two international partners from the United States and Japan.',\n",
       " 'We present an asteroseismic analysis of 33 solar-type stars observed in short cadence during Campaigns (C) 1-3 of the NASA K2 mission. We were able to extract both average seismic parameters and individual mode frequencies for stars with dominant frequencies up to ~3300Î¼Hz, and we find that data for some targets are good enough to allow for a measurement of the rotational splitting. Modelling of the extracted parameters is performed by using grid-based methods using average parameters and individual frequencies together with spectroscopic parameters. For the target selection in C3, stars were chosen as in C1 and C2 to cover a wide range in parameter space to better understand the performance and noise characteristics. For C3 we still detected oscillations in 73% of the observed stars that we proposed. Future K2 campaigns hold great promise for the study of nearby clusters and the chemical evolution and age-metallicity relation of nearby field stars in the solar neighbourhood. We expect oscillations to be detected in ~388 short-cadence targets if the K2 mission continues until C18, which will greatly complement the ~500 detections of solar-like oscillations made for short-cadence targets during the nominal Kepler mission. For ~30-40 of these, including several members of the Hyades open cluster, we furthermore expect that inference from interferometry should be possible.',\n",
       " 'We prove that the vortex structures of solutions to the 3D Navier-Stokes equations can change their topology without any loss of regularity. More precisely, we construct smooth high-frequency solutions to the Navier-Stokes equations where vortex lines and vortex tubes of arbitrarily complicated topologies are created and destroyed in arbitrarily small times. This instance of vortex reconnection is structurally stable and in perfect agreement with the existing computer simulations and experiments. We also provide a (non-structurally stable) scenario where the destruction of vortex structures is instantaneous.',\n",
       " 'We measure the energy emitted by extensive air showers in the form of radio emission in the frequency range from 30 to 80 MHz. Exploiting the accurate energy scale of the Pierre Auger Observatory, we obtain a radiation energy of 15.8 \\\\pm 0.7 (stat) \\\\pm 6.7 (sys) MeV for cosmic rays with an energy of 1 EeV arriving perpendicularly to a geomagnetic field of 0.24 G, scaling quadratically with the cosmic-ray energy. A comparison with predictions from state-of-the-art first-principle calculations shows agreement with our measurement. The radiation energy provides direct access to the calorimetric energy in the electromagnetic cascade of extensive air showers. Comparison with our result thus allows the direct calibration of any cosmic-ray radio detector against the well-established energy scale of the Pierre Auger Observatory.',\n",
       " 'Stochastic spectral methods have become a popular technique to quantify the uncertainties of nano-scale devices and circuits. They are much more efficient than Monte Carlo for certain design cases with a small number of random parameters. However, their computational cost significantly increases as the number of random parameters increases. This paper presents a big-data approach to solve high-dimensional uncertainty quantification problems. Specifically, we simulate integrated circuits and MEMS at only a small number of quadrature samples, then, a huge number of (e.g., $1.5 \\\\times 10^{27}$) solution samples are estimated from the available small-size (e.g., $500$) solution samples via a low-rank and tensor-recovery method. Numerical results show that our algorithm can easily extend the applicability of tensor-product stochastic collocation to IC and MEMS problems with over 50 random parameters, whereas the traditional algorithm can only handle several random parameters.',\n",
       " 'We demonstrate the use of a compound optical cavity as linear displacement detector, by measuring the thermal motion of a silicon nitride suspended membrane acting as the external mirror of a near-infrared Littrow laser diode. Fluctuations in the laser optical power induced by the membrane vibrations are collected by a photodiode integrated within the laser, and then measured with a spectrum analyzer. The dynamics of the membrane driven by a piezoelectric actuator is investigated as a function of air pressure and actuator displacement in a homodyne configuration. The high Q-factor ($\\\\sim 3.4\\\\cdot 10^4$ at $8.3 \\\\cdot 10^{-3}$ mbar) of the fundamental mechanical mode at $\\\\sim 73$ kHz guarantees a detection sensitivity high enough for direct measurement of thermal motion at room temperature ($\\\\sim 87$ pm RMS). The compound cavity system here introduced can be employed as a table-top, cost-effective linear displacement detector for cavity optomechanics. Furthermore, thanks to the strong optical nonlinearities of the laser compound cavity, these systems open new perspectives in the study of non-Markovian quantum properties at the mesoscale.',\n",
       " 'Kepler-454 (KOI-273) is a relatively bright (V = 11.69 mag), Sun-like starthat hosts a transiting planet candidate in a 10.6 d orbit. From spectroscopy, we estimate the stellar temperature to be 5687 +/- 50 K, its metallicity to be [m/H] = 0.32 +/- 0.08, and the projected rotational velocity to be v sin i <2.4 km s-1. We combine these values with a study of the asteroseismic frequencies from short cadence Kepler data to estimate the stellar mass to be 1.028+0:04-0:03 M_Sun, the radius to be 1.066 +/- 0.012 R_Sun and the age to be 5.25+1:41-1:39 Gyr. We estimate the radius of the 10.6 d planet as 2.37 +/- 0.13 R_Earth. Using 63 radial velocity observations obtained with the HARPS-N spectrograph on the Telescopio Nazionale Galileo and 36 observations made with the HIRES spectrograph at Keck Observatory, we measure the mass of this planet to be 6.8 +/- 1.4M_Earth. We also detect two additional non-transiting companions, a planet with a minimum mass of 4.46 +/- 0.12 M_J in a nearly circular 524 d orbit and a massive companion with a period >10 years and mass >12.1M_J . The twelve exoplanets with radii <2.7 R_Earth and precise mass measurements appear to fall into two populations, with those <1.6 R_Earth following an Earth-like composition curve and larger planets requiring a significant fraction of volatiles. With a density of 2.76 +/- 0.73 g cm-3, Kepler-454b lies near the mass transition between these two populations and requires the presence of volatiles and/or H/He gas.',\n",
       " 'To uniformly determine the properties of supernova remnants (SNRs) at high energies, we have developed the first systematic survey at energies from 1 to 100 GeV using data from the Fermi Large Area Telescope. Based on the spatial overlap of sources detected at GeV energies with SNRs known from radio surveys, we classify 30 sources as likely GeV SNRs. We also report 14 marginal associations and 245 flux upper limits. A mock catalog in which the positions of known remnants are scrambled in Galactic longitude, allows us to determine an upper limit of 22% on the number of GeV candidates falsely identified as SNRs. We have also developed a method to estimate spectral and spatial systematic errors arising from the diffuse interstellar emission model, a key component of all Galactic Fermi LAT analyses. By studying remnants uniformly in aggregate, we measure the GeV properties common to these objects and provide a crucial context for the detailed modeling of individual SNRs. Combining our GeV results with multiwavelength (MW) data, including radio, X-ray, and TeV, demonstrates the need for improvements to previously sufficient, simple models describing the GeV and radio emission from these objects. We model the GeV and MW emission from SNRs in aggregate to constrain their maximal contribution to observed Galactic cosmic rays.',\n",
       " 'List of contributions from the CTA Consortium presented at the 34th International Cosmic Ray Conference, 30 July - 6 August 2015, The Hague, The Netherlands.',\n",
       " 'The Auger Engineering Radio Array (AERA) is part of the Pierre Auger Observatory and is used to detect the radio emission of cosmic-ray air showers. These observations are compared to the data of the surface detector stations of the Observatory, which provide well-calibrated information on the cosmic-ray energies and arrival directions. The response of the radio stations in the 30 to 80 MHz regime has been thoroughly calibrated to enable the reconstruction of the incoming electric field. For the latter, the energy deposit per area is determined from the radio pulses at each observer position and is interpolated using a two-dimensional function that takes into account signal asymmetries due to interference between the geomagnetic and charge-excess emission components. The spatial integral over the signal distribution gives a direct measurement of the energy transferred from the primary cosmic ray into radio emission in the AERA frequency range. We measure 15.8 MeV of radiation energy for a 1 EeV air shower arriving perpendicularly to the geomagnetic field. This radiation energy -- corrected for geometrical effects -- is used as a cosmic-ray energy estimator. Performing an absolute energy calibration against the surface-detector information, we observe that this radio-energy estimator scales quadratically with the cosmic-ray energy as expected for coherent emission. We find an energy resolution of the radio reconstruction of 22% for the data set and 17% for a high-quality subset containing only events with at least five radio stations with signal.',\n",
       " 'In this note, we show that $S(n,r):=\\\\sum_{k=0}^{n} \\\\binom{n}{k}\\\\frac{k}{k+r}$ is not an integer for any positive integer $n$ and $r\\\\in \\\\{1,2,3,4,5,6\\\\}$ and for $n\\\\le r-1$. This gives a partial answer to a conjecture of [3].',\n",
       " 'This paper presents a tensor-recovery method to solve probabilistic power flow problems. Our approach generates a high-dimensional and sparse generalized polynomial-chaos expansion that provides useful statistical information. The result can also speed up other essential routines in power systems (e.g., stochastic planning, operations and controls).\\n  Instead of simulating a power flow equation at all quadrature points, our approach only simulates an extremely small subset of samples. We suggest a model to exploit the underlying low-rank and sparse structure of high-dimensional simulation data arrays, making our technique applicable to power systems with many random parameters. We also present a numerical method to solve the resulting nonlinear optimization problem.\\n  Our algorithm is implemented in MATLAB and is verified by several benchmarks in MATPOWER $5.1$. Accurate results are obtained for power systems with up to $50$ independent random parameters, with a speedup factor up to $9\\\\times 10^{20}$.',\n",
       " 'Entanglement is the key quantum resource for improving measurement sensitivity beyond classical limits. However, the production of entanglement in mesoscopic atomic systems has been limited to squeezed states, described by Gaussian statistics. Here we report on the creation and characterization of non-Gaussian many-body entangled states. We develop a general method to extract the Fisher information, which reveals that the quantum dynamics of a classically unstable system creates quantum states that are not spin squeezed but nevertheless entangled. The extracted Fisher information quantifies metrologically useful entanglement which we confirm by Bayesian phase estimation with sub shot-noise sensitivity. These methods are scalable to large particle numbers and applicable directly to other quantum systems.',\n",
       " 'Genotypic fitness landscapes are constructed by assessing the fitness of all possible combinations of a given number of mutations. In the last years, several experimental fitness landscapes have been completely resolved. As fitness landscapes are high-dimensional, their characterization relies on simple measures of their structure, which can be used as statistics in empirical applications. Here we propose two new sets of measures that explicitly capture two relevant features of fitness landscapes: epistasis and constraints. The first set contains new measures for epistasis based on the correlation of fitness effects of mutations. They have a natural interpretation, capture well the interaction between mutations, can be obtained analytically for most landscape models and can therefore be used to discriminate between different models. The second set contains measures of evolutionary constraints based on \"chains\" of forced mutations along fitness-increasing paths. Some of these measures are non-monotonic in the amount of epistatic interactions, but have instead a maximum for intermediate values. We further characterize the relationships of these measures to the ones that were previous proposed (e.g. number of peaks, roughness/slope, fraction of non-additive components, etc). Finally, we show how these measures can help uncovering the amount and the nature of epistatic interactions in two experimental landscapes.',\n",
       " 'In this paper, we propose a method for computing partial functional correspondence between non-rigid shapes. We use perturbation analysis to show how removal of shape parts changes the Laplace-Beltrami eigenfunctions, and exploit it as a prior on the spectral representation of the correspondence. Corresponding parts are optimization variables in our problem and are used to weight the functional correspondence; we are looking for the largest and most regular (in the Mumford-Shah sense) parts that minimize correspondence distortion. We show that our approach can cope with very challenging correspondence settings.',\n",
       " 'Neutrinos in the cosmic ray flux with energies near 1 EeV and above are detectable with the Surface Detector array of the Pierre Auger Observatory. We report here on searches through Auger data from 1 January 2004 until 20 June 2013. No neutrino candidates were found, yielding a limit to the diffuse flux of ultra-high energy neutrinos that challenges the Waxman-Bahcall bound predictions. Neutrino identification is attempted using the broad time-structure of the signals expected in the SD stations, and is efficiently done for neutrinos of all flavors interacting in the atmosphere at large zenith angles, as well as for \"Earth-skimming\" neutrino interactions in the case of tau neutrinos. In this paper the searches for downward-going neutrinos in the zenith angle bins $60^\\\\circ-75^\\\\circ$ and $75^\\\\circ-90^\\\\circ$ as well as for upward-going neutrinos, are combined to give a single limit. The $90\\\\%$ C.L. single-flavor limit to the diffuse flux of ultra-high energy neutrinos with an $E^{-2}$ spectrum in the energy range $1.0 \\\\times 10^{17}$ eV - $2.5 \\\\times 10^{19}$ eV is $E_Î½^2 dN_Î½/dE_Î½< 6.4 \\\\times 10^{-9}~ {\\\\rm GeV~ cm^{-2}~ s^{-1}~ sr^{-1}}$.',\n",
       " 'A measurement of the cosmic-ray spectrum for energies exceeding $4{\\\\times}10^{18}$ eV is presented, which is based on the analysis of showers with zenith angles greater than $60^{\\\\circ}$ detected with the Pierre Auger Observatory between 1 January 2004 and 31 December 2013. The measured spectrum confirms a flux suppression at the highest energies. Above $5.3{\\\\times}10^{18}$ eV, the \"ankle\", the flux can be described by a power law $E^{-Î³}$ with index $Î³=2.70 \\\\pm 0.02 \\\\,\\\\text{(stat)} \\\\pm 0.1\\\\,\\\\text{(sys)}$ followed by a smooth suppression region. For the energy ($E_\\\\text{s}$) at which the spectral flux has fallen to one-half of its extrapolated value in the absence of suppression, we find $E_\\\\text{s}=(5.12\\\\pm0.25\\\\,\\\\text{(stat)}^{+1.0}_{-1.2}\\\\,\\\\text{(sys)}){\\\\times}10^{19}$ eV.',\n",
       " \"The Mu2e experiment at Fermilab will search for charged lepton flavor violation via the coherent conversion process mu- N --> e- N with a sensitivity approximately four orders of magnitude better than the current world's best limits for this process. The experiment's sensitivity offers discovery potential over a wide array of new physics models and probes mass scales well beyond the reach of the LHC. We describe herein the preliminary design of the proposed Mu2e experiment. This document was created in partial fulfillment of the requirements necessary to obtain DOE CD-2 approval.\",\n",
       " 'Magnetite (Fe3O4) is an eligible candidate for magnetic tunnel junctions (MTJs) since it shows a high spin polarization at the Fermi level as well as a high Curie temperature of 585Â°C. In this study, Fe3O4/MgO/Co-Fe-B MTJs were manufactured. A sign change in the TMR is observed after annealing the MTJs at temperatures between 200Â°C and 280Â°C. Our findings suggest an Mg interdiffusion from the MgO barrier into the Fe3O4 as the reason for the change of the TMR. Additionally, different treatments of the magnetite interface (argon bombardment, annealing at 200Â°C in oxygen atmosphere) during the preparation of the MTJs have been studied regarding their effect on the performance of the MTJs. A maximum TMR of up to -12% could be observed using both argon bombardment and annealing in oxygen atmosphere, despite exposing the magnetite surface to atmospheric conditions before the deposition of the MgO barrier.',\n",
       " 'Pure and bright single photon sources have recently been obtained by inserting solid-state emitters in photonic nanowires or microcavities. The cavity approach presents the attractive possibility to greatly increase the source operation frequency. However, it is perceived as technologically demanding because the emitter resonance must match the cavity resonance. Here we show that the spectral matching requirement is actually strongly lifted by the intrinsic coupling of the emitter to its environment. A single photon source consisting of a single InGaAs quantum dot inserted in a micropillar cavity is studied. Phonon coupling results in a large Purcell effect even when the quantum dot is detuned from the cavity resonance. The phonon-assisted cavity enhanced emission is shown to be a good single-photon source, with a brightness exceeding $40$ \\\\% for a detuning range covering 15 cavity linewidths.',\n",
       " 'Uncertainties have become a major concern in integrated circuit design. In order to avoid the huge number of repeated simulations in conventional Monte Carlo flows, this paper presents an intrusive spectral simulator for statistical circuit analysis. Our simulator employs the recently developed generalized polynomial chaos expansion to perform uncertainty quantification of nonlinear transistor circuits with both Gaussian and non-Gaussian random parameters. We modify the nonintrusive stochastic collocation (SC) method and develop an intrusive variant called stochastic testing (ST) method to accelerate the numerical simulation. Compared with the stochastic Galerkin (SG) method, the resulting coupled deterministic equations from our proposed ST method can be solved in a decoupled manner at each time point. At the same time, ST uses fewer samples and allows more flexible time step size controls than directly using a nonintrusive SC solver. These two properties make ST more efficient than SG and than existing SC methods, and more suitable for time-domain circuit simulation. Simulation results of several digital, analog and RF circuits are reported. Since our algorithm is based on generic mathematical models, the proposed ST algorithm can be applied to many other engineering problems.',\n",
       " 'Stochastic spectral methods are efficient techniques for uncertainty quantification. Recently they have shown excellent performance in the statistical analysis of integrated circuits. In stochastic spectral methods, one needs to determine a set of orthonormal polynomials and a proper numerical quadrature rule. The former are used as the basis functions in a generalized polynomial chaos expansion. The latter is used to compute the integrals involved in stochastic spectral methods. Obtaining such information requires knowing the density function of the random input {\\\\it a-priori}. However, individual system components are often described by surrogate models rather than density functions. In order to apply stochastic spectral methods in hierarchical uncertainty quantification, we first propose to construct physically consistent closed-form density functions by two monotone interpolation schemes. Then, by exploiting the special forms of the obtained density functions, we determine the generalized polynomial-chaos basis functions and the Gauss quadrature rules that are required by a stochastic spectral simulator. The effectiveness of our proposed algorithm is verified by both synthetic and practical circuit examples.',\n",
       " 'This brief paper proposes an uncertainty quantification method for the periodic steady-state (PSS) analysis with both Gaussian and non-Gaussian variations. Our stochastic testing formulation for the PSS problem provides superior efficiency over both Monte Carlo methods and existing spectral methods. The numerical implementation of a stochastic shooting Newton solver is presented for both forced and autonomous circuits. Simulation results on some analog/RF circuits are reported to show the effectiveness of our proposed algorithms.',\n",
       " 'Due to significant manufacturing process variations, the performance of integrated circuits (ICs) has become increasingly uncertain. Such uncertainties must be carefully quantified with efficient stochastic circuit simulators. This paper discusses the recent advances of stochastic spectral circuit simulators based on generalized polynomial chaos (gPC). Such techniques can handle both Gaussian and non-Gaussian random parameters, showing remarkable speedup over Monte Carlo for circuits with a small or medium number of parameters. We focus on the recently developed stochastic testing and the application of conventional stochastic Galerkin and stochastic collocation schemes to nonlinear circuit problems. The uncertainty quantification algorithms for static, transient and periodic steady-state simulations are presented along with some practical simulation results. Some open problems in this field are discussed.',\n",
       " \"Process variations are a major concern in today's chip design since they can significantly degrade chip performance. To predict such degradation, existing circuit and MEMS simulators rely on Monte Carlo algorithms, which are typically too slow. Therefore, novel fast stochastic simulators are highly desired. This paper first reviews our recently developed stochastic testing simulator that can achieve speedup factors of hundreds to thousands over Monte Carlo. Then, we develop a fast hierarchical stochastic spectral simulator to simulate a complex circuit or system consisting of several blocks. We further present a fast simulation approach based on anchored ANOVA (analysis of variance) for some design problems with many process variations. This approach can reduce the simulation cost and can identify which variation sources have strong impacts on the circuit's performance. The simulation results of some circuit and MEMS examples are reported to show the effectiveness of our simulator\",\n",
       " 'In this work, we focus on separable convex optimization problems with box constraints and a set of triangular linear constraints. The solution is given in closed-form as a function of some Lagrange multipliers that can be computed through an iterative procedure in a finite number of steps. Graphical interpretations are given casting valuable insights into the proposed algorithm and allowing to retain some of the intuition spelled out by the water-filling policy. It turns out that it is not only general enough to compute the solution to different instances of the problem at hand but also remarkably simple in the way it operates. We also show how some power allocation problems in signal processing and communications can be solved with the proposed algorithm.',\n",
       " 'Hierarchical uncertainty quantification can reduce the computational cost of stochastic circuit simulation by employing spectral methods at different levels. This paper presents an efficient framework to simulate hierarchically some challenging stochastic circuits/systems that include high-dimensional subsystems. Due to the high parameter dimensionality, it is challenging to both extract surrogate models at the low level of the design hierarchy and to handle them in the high-level simulation. In this paper, we develop an efficient ANOVA-based stochastic circuit/MEMS simulator to extract efficiently the surrogate models at the low level. In order to avoid the curse of dimensionality, we employ tensor-train decomposition at the high level to construct the basis functions and Gauss quadrature points. As a demonstration, we verify our algorithm on a stochastic oscillator with four MEMS capacitors and 184 random parameters. This challenging example is simulated efficiently by our simulator at the cost of only 10 minutes in MATLAB on a regular personal computer.',\n",
       " 'In this work, we focus on separable convex optimization problems with linear and box constraints and compute the solution in closed-form as a function of some Lagrange multipliers that can be easily computed in a finite number of iterations. This allows us to bridge the gap between a wide family of power allocation problems of practical interest in signal processing and communications and their efficient implementation in practice.',\n",
       " 'We generalize Wheeler-Feynman electrodynamics with a variational boundary-value problem with past and future boundary segments that can include velocity discontinuity points. Critical-point trajectories must satisfy the Euler-Lagrange equations of the action functional, which are neutral-differential delay equations of motion (the Wheeler-Feynman equations of motion). At velocity discontinuity points, critical-point orbits must satisfy the Weierstrass-Erdmann conditions of continuity of partial momenta and partial energies. We study a special class of boundary data having the shortest time-separation between boundary segments, for which case the Wheeler-Feynman equations reduce to a two-point boundary problem for an ordinary differential equation. For this simple case we prove that the extended variational problem has solutions with discontinuous velocities. We construct a numerical method to solve the Wheeler-Feynman equations together with the Weierstrass-Erdmann conditions and calculate some numerical orbits with discontinuous velocities.',\n",
       " 'Emitters of indistinguishable single photons are crucial for the growing field of quantum technologies. To realize scalability and increase the complexity of quantum optics technologies, multiple independent yet identical single photon emitters are also required. However typical solid-state single photon sources are inherently dissimilar, necessitating the use of electrical feedback or optical cavities to improve spectral overlap between distinct emitters. Here, we demonstrate bright silicon-vacancy (SiV-) centres in low-strain bulk diamond which intrinsically show spectral overlap of up to 91% and near transform-limited excitation linewidths. Our results have impact upon the application of single photon sources for quantum optics and cryptography, and the production of next generation fluorophores for bio-imaging.',\n",
       " 'Contributions of the Pierre Auger Collaboration to the 33rd International Cosmic Ray Conference, Rio de Janeiro, Brazil, July 2013',\n",
       " 'In this paper, the development of the dual mirror Small Size Telescopes (SST) for the Cherenkov Telescope Array (CTA) is reviewed. Up to 70 SST, with a primary mirror diameter of 4 m, will be produced and installed at the CTA southern site. These will allow investigation of the gamma-ray sky at the highest energies accessible to CTA, in the range from about 1 TeV to 300 TeV. The telescope presented in this contribution is characterized by two major innovations: the use of a dual mirror Schwarzschild-Couder configuration and of an innovative camera using as sensors either multi-anode photomultipliers (MAPM) or silicon photomultipliers (SiPM). The reduced plate-scale of the telescope, achieved with the dual-mirror optics, allows the camera to be compact (40 cm in diameter), and low-cost. The camera, which has about 2000 pixels of size 6x6 mm^2, covers a field of view of 10Â°. The dual mirror telescopes and their cameras are being developed by three consortia, ASTRI (Astrofisica con Specchi a Tecnologia Replicante Italiana, Italy/INAF), GATE (Gamma-ray Telescope Elements, France/Paris Observ.) and CHEC (Compact High Energy Camera, universities in UK, US and Japan) which are merging their efforts in order to finalize an end-to-end design that will be constructed for CTA. A number of prototype structures and cameras are being developed in order to investigate various alternative designs. In this contribution, these designs are presented, along with the technological solutions under study.',\n",
       " 'Compilation of CTA contributions to the proceedings of the 33rd International Cosmic Ray Conference (ICRC2013), which took place in 2-9 July, 2013, in Rio de Janeiro, Brazil',\n",
       " 'We present the first signature-based search for delayed photons using an exclusive photon plus missing transverse energy final state. Events are reconstructed in a data sample from the CDF II detector corresponding to $6.3 \\\\text{fb}^{-1}$ of integrated luminosity from $\\\\sqrt{s}=1.96$ TeV proton-antiproton collisions. Candidate events are selected if they contain a photon with an arrival time in the detector larger than expected from a promptly-produced photon. The mean number of events from standard model sources predicted by the data-driven background model based on the photon timing distribution is $286 \\\\pm 24$. A total of 322 events are observed. A $p$-value of 12% is obtained, showing consistency of the data with standard model predictions.',\n",
       " 'This White Paper, submitted to the recent ESA call for science themes to define its future large missions, advocates the need for a transformational leap in our understanding of two key questions in astrophysics: 1) How does ordinary matter assemble into the large scale structures that we see today? 2) How do black holes grow and shape the Universe? Hot gas in clusters, groups and the intergalactic medium dominates the baryonic content of the local Universe. To understand the astrophysical processes responsible for the formation and assembly of these large structures, it is necessary to measure their physical properties and evolution. This requires spatially resolved X-ray spectroscopy with a factor 10 increase in both telescope throughput and spatial resolving power compared to currently planned facilities. Feedback from supermassive black holes is an essential ingredient in this process and in most galaxy evolution models, but it is not well understood. X-ray observations can uniquely reveal the mechanisms launching winds close to black holes and determine the coupling of the energy and matter flows on larger scales. Due to the effects of feedback, a complete understanding of galaxy evolution requires knowledge of the obscured growth of supermassive black holes through cosmic time, out to the redshifts where the first galaxies form. X-ray emission is the most reliable way to reveal accreting black holes, but deep survey speed must improve by a factor ~100 over current facilities to perform a full census into the early Universe. The Advanced Telescope for High Energy Astrophysics (Athena+) mission provides the necessary performance (e.g. angular resolution, spectral resolution, survey grasp) to address these questions and revolutionize our understanding of the Hot and Energetic Universe. These capabilities will also provide a powerful observatory to be used in all areas of astrophysics.',\n",
       " 'Physical laws are believed to be invariant under the combined transformations of charge, parity and time reversal (CPT symmetry). This implies that an antimatter particle has exactly the same mass and absolute value of charge as its particle counterpart. Metastable antiprotonic helium ($\\\\bar{p}{\\\\rm He}^+$) is a three-body atom consisting of a normal helium nucleus, an electron in its ground state and an antiproton ($\\\\bar{p}$) occupying a Rydberg state with high principal and angular momentum quantum numbers, respectively $n$ and $\\\\ell$, such that $n\\\\sim\\\\ell\\\\sim 38$. These atoms are amenable to precision laser spectroscopy, the results of which can in principle be used to determine the antiproton-to-electron mass ratio and to constrain the equality between the antiproton and proton charges and masses. Here we report two-photon spectroscopy of antiprotonic helium, in which $\\\\bar{p}{\\\\rm ^3He^+}$ and $\\\\bar{p}{\\\\rm ^4He^+}$ isotopes are irradiated by two counter-propagating laser beams. This excites nonlinear, two-photon transitions of the antiproton of the type $(n,\\\\ell)\\\\rightarrow (n-2,\\\\ell-2)$ at deep-ultraviolet wavelengths ($Î»$=139.8, 193.0 and 197.0nm), which partly cancel the Doppler broadening of the laser resonance caused by the thermal motion of the atoms. The resulting narrow spectral lines allowed us to measure three transition frequencies with fractional precisions of 2.3-5 parts in $10^9$. By comparing the results with three-body quantum electrodynamics calculations, we derived an antiproton-to-electron mass ratio of 1,836.1526736(23), where the parenthetical error represents one standard deviation. This agrees with the proton-to-electron value known to a similar precision.',\n",
       " 'We construct a class of supersymmetric vacua of type IIB string theory describing systems of three- and seven-branes non-perturbatively completed by brane instantons. The vacua are specified by a set of holomorphic functions defined over a complex plane up to non-trivial U-duality monodromies around the brane locations. In the simplest setting, the solutions can be seen as a generalization of F-theory elliptic fibrations, where the torus fiber is replaced by a genus two Riemann surface with periods encoding the information on the axio-dilaton, the warp factor and the NS-NS and R-R fluxes.',\n",
       " 'We present diffraction-limited \\\\ks band and \\\\lprime adaptive optics images of the edge-on debris disk around the nearby F2 star HD 15115, obtained with a single 8.4 m primary mirror at the Large Binocular Telescope. At \\\\ks band the disk is detected at signal-to-noise per resolution element (SNRE) \\\\about 3-8 from \\\\about 1-2\\\\fasec 5 (45-113 AU) on the western side, and from \\\\about 1.2-2\\\\fasec 1 (63-90 AU) on the east. At \\\\lprime the disk is detected at SNRE \\\\about 2.5 from \\\\about 1-1\\\\fasec 45 (45-90 AU) on both sides, implying more symmetric disk structure at 3.8 \\\\microns . At both wavelengths the disk has a bow-like shape and is offset from the star to the north by a few AU. A surface brightness asymmetry exists between the two sides of the disk at \\\\ks band, but not at \\\\lprime . The surface brightness at \\\\ks band declines inside 1\\\\asec (\\\\about 45 AU), which may be indicative of a gap in the disk near 1\\\\asec. The \\\\ks - \\\\lprime disk color, after removal of the stellar color, is mostly grey for both sides of the disk. This suggests that scattered light is coming from large dust grains, with 3-10 \\\\microns -sized grains on the east side and 1-10 \\\\microns dust grains on the west. This may suggest that the west side is composed of smaller dust grains than the east side, which would support the interpretation that the disk is being dynamically affected by interactions with the local interstellar medium.',\n",
       " \"As the only directly imaged multiple planet system, HR 8799 provides a unique opportunity to study the physical properties of several planets in parallel. In this paper, we image all four of the HR 8799 planets at H-band and 3.3 microns with the new LBT adaptive optics system, PISCES, and LBTI/LMIRCam. Our images offer an unprecedented view of the system, allowing us to obtain H and 3.3$ micron photometry of the innermost planet (for the first time) and put strong upper-limits on the presence of a hypothetical fifth companion. We find that all four planets are unexpectedly bright at 3.3 microns compared to the equilibrium chemistry models used for field brown dwarfs, which predict that planets should be faint at 3.3 microns due to CH4 opacity. We attempt to model the planets with thick-cloudy, non-equilibrium chemistry atmospheres, but find that removing CH4 to fit the 3.3 micron photometry increases the predicted L' (3.8 microns) flux enough that it is inconsistent with observations. In an effort to fit the SED of the HR 8799 planets, we construct mixtures of cloudy atmospheres, which are intended to represent planets covered by clouds of varying opacity. In this scenario, regions with low opacity look hot and bright, while regions with high opacity look faint, similar to the patchy cloud structures on Jupiter and L/T transition brown-dwarfs. Our mixed cloud models reproduce all of the available data, but self-consistent models are still necessary to demonstrate their viability.\",\n",
       " 'From a time series whose data are embedded in heavy noise, we construct an Hilbert space operator (J-operator) whose discrete spectrum represents the signal while the essential spectrum located on the unit circle, is associated with the noise. Furthermore the essential spectrum, in the absence of signal, is built from roots of unity (\"clock\" distribution). These results are independent of the statistical properties of the noise that can be Gaussian, non-Gaussian, pink or even without second moment (Levy). The presence of the signal has for effect to break the clock angular distribution of the essential spectrum on the unit circle. A discontinuity, proportional to the intensity of the signal, appears in the angular distribution. The sensitivity of this method is definitely better than standard techniques. We build an example that supports our claims.',\n",
       " 'A large sub-mm survey with Herschel will enable many exciting science opportunities, especially in an era of wide-field optical and radio surveys and high resolution cosmic microwave background experiments. The Herschel-SPIRE Legacy Survey (HSLS), will lead to imaging data over 4000 sq. degrees at 250, 350, and 500 micron. Major Goals of HSLS are: (a) produce a catalog of 2.5 to 3 million galaxies down to 26, 27 and 33 mJy (50% completeness; 5 sigma confusion noise) at 250, 350 and 500 micron, respectively, in the southern hemisphere (3000 sq. degrees) and in an equatorial strip (1000 sq. degrees), areas which have extensive multi-wavelength coverage and are easily accessible from ALMA. Two thirds of the of the sources are expected to be at z > 1, one third at z > 2 and about a 1000 at z > 5. (b) Remove point source confusion in secondary anisotropy studies with Planck and ground-based CMB data. (c) Find at least 1200 strongly lensed bright sub-mm sources leading to a 2% test of general relativity. (d) Identify 200 proto-cluster regions at z of 2 and perform an unbiased study of the environmental dependence of star formation. (e) Perform an unbiased survey for star formation and dust at high Galactic latitude and make a census of debris disks and dust around AGB stars and white dwarfs.',\n",
       " 'We present Herschel/HIFI observations of the fundamental rotational transitions of ortho- and para-H$_2^{16}$O and H$_2^{18}$O in absorption towards Sagittarius~B2(M) and W31C. The ortho/para ratio in water in the foreground clouds on the line of sight towards these bright continuum sources is generally consistent with the statistical high-temperature ratio of 3, within the observational uncertainties. However, somewhat unexpectedly, we derive a low ortho/para ratio of $2.35 \\\\pm 0.35$, corresponding to a spin temperature of $\\\\sim$27~K, towards Sagittarius~B2(M) at velocities of the expanding molecular ring. Water molecules in this region appear to have formed with, or relaxed to, an ortho/para ratio close to the value corresponding to the local temperature of the gas and dust.',\n",
       " \"The Vela-D region, according to the nomenclature given by Murphy & May (1991), of the star forming complex known as the Vela Molecular Ridge (VMR), has been recently analyzed in details by Olmi et al. (2009), who studied the physical properties of 141 pre- and proto-stellar cold dust cores, detected by the ``Balloon-borne Large-Aperture Submillimeter Telescope'' (BLAST) during a much larger (55 sq. degree) Galactic Plane survey encompassing the whole VMR. This survey's primary goal was to identify the coldest, dense dust cores possibly associated with the earliest phases of star formation. In this work, the dynamical state of the Vela-D cores is analyzed. Comparison to dynamical masses of a sub-sample of the Vela-D cores estimated from the 13CO survey of Elia et al. (2007), is complicated by the fact that the 13CO linewidths are likely to trace the lower density intercore material, in addition to the dense gas associated with the compact cores observed by BLAST. In fact, the total internal pressure of these cores, if estimated using the 13CO linewidths, appears to be higher than the cloud ambient pressure. If this were the case, then self-gravity and surface pressure would be insufficient to bind these cores and an additional source of external confinement (e.g., magnetic field pressure) would be required. However, if one attempts to scale down the 13CO linewidths, according to the observations of high-density tracers in a small sample of sources, then most proto-stellar cores would result effectively gravitationally bound.\",\n",
       " 'The deposition of mechanical feedback from a supermassive black hole (SMBH) in an active galactic nucleus (AGN) into the surrounding galaxy occurs via broad-line winds which must carry mass and radial momentum as well as energy. The effect can be summarized by the dimensionless parameter $Î·=dot{M_outflow}/dot{M_accretion}= (2 Îµ_w c^2)/v_w^2$ where ($\\\\epslion_w \\\\equiv dot{E}_w/(dot{M_accretion} c^2)$) is the efficiency by which accreted matter is turned into wind energy in the disc surrounding the central SMBH. The outflowing mass and omentum are proportional to $Î·$, and many prior treatments have essentially assumed that $Î·=0$. We perform one- and two-dimensional simulations and find that the growth of the central SMBH is very sensitive to the inclusion of the mass and momentum driving but is insensitive to the assumed mechanical efficiency. For example in representative calculations, the omission of momentum and mass feedback leads to an hundred fold increase in the mass of the SMBH to over $10^{10} \\\\Msun$. When allowance is made for momentum driving, the final SMBH mass is much lower and the wind efficiencies which lead to the most observationally acceptable results are relatively low with $Îµ_w \\\\lesssim 10^{-4}$.',\n",
       " '  The Balloon-borne Large-Aperture Submillimeter Telescope (BLAST) carried out a 250, 350 and 500 micron survey of the galactic plane encompassing the Vela Molecular Ridge, with the primary goal of identifying the coldest dense cores possibly associated with the earliest stages of star formation. Here we present the results from observations of the Vela-D region, covering about 4 square degrees, in which we find 141 BLAST cores. We exploit existing data taken with the Spitzer MIPS, IRAC and SEST-SIMBA instruments to constrain their (single-temperature) spectral energy distributions, assuming a dust emissivity index beta = 2.0. This combination of data allows us to determine the temperature, luminosity and mass of each BLAST core, and also enables us to separate starless from proto-stellar sources. We also analyze the effects that the uncertainties on the derived physical parameters of the individual sources have on the overall physical properties of starless and proto-stellar cores, and we find that there appear to be a smooth transition from the pre- to the proto-stellar phase. In particular, for proto-stellar cores we find a correlation between the MIPS24 flux, associated with the central protostar, and the temperature of the dust envelope. We also find that the core mass function of the Vela-D cores has a slope consistent with other similar (sub)millimeter surveys.',\n",
       " '  We propose to apply to the detection of Gravitational Waves a new method developed for the spectral analysis of noisy time-series of damped oscillators.\\n  From the PadÃ© Approximations of the time-series Z-transform, a Jacobi Matrix (J-Matrix) is constructed. We show that the J-Matrix has bound states with eigenvalues strictly inside the unit circle. Each bound state can be identified with one precise damped oscillator. Beside these bound states, there is an essential spectrum sitting on the unit circle which represents the noise. In this picture, signal and noise are clearly separated and identified in the complex plane. Furthermore, we show that the J-transform enjoys the exceptional feature of lossless undersampling. We take advantage of the above properties of the J-transform to develop a procedure for the search of Gravitational Wave bursts in interferometric data series such as those of LIGO and VIRGO projects. Successful application of our procedure to simulated data having a poor signal to noise ratio, highlights the power of our method.',\n",
       " '  How did the universe evolve? The fine angular scale (l>1000) temperature and polarization anisotropies in the CMB are a Rosetta stone for understanding the evolution of the universe. Through detailed measurements one may address everything from the physics of the birth of the universe to the history of star formation and the process by which galaxies formed. One may in addition track the evolution of the dark energy and discover the net neutrino mass.\\n  We are at the dawn of a new era in which hundreds of square degrees of sky can be mapped with arcminute resolution and sensitivities measured in microKelvin. Acquiring these data requires the use of special purpose telescopes such as the Atacama Cosmology Telescope (ACT), located in Chile, and the South Pole Telescope (SPT). These new telescopes are outfitted with a new generation of custom mm-wave kilo-pixel arrays. Additional instruments are in the planning stages.',\n",
       " '  These are the findings of the Joint Dark Energy Mission (JDEM) Figure of Merit (FoM) Science Working Group (SWG), the FoMSWG. JDEM is a space mission planned by NASA and the DOE for launch in the 2016 time frame. The primary mission is to explore the nature of dark energy. In planning such a mission, it is necessary to have some idea of knowledge of dark energy in 2016, and a way to quantify the performance of the mission. In this paper we discuss these issues.',\n",
       " '  We summarize the utility of precise cosmic microwave background (CMB) polarization measurements as probes of the physics of inflation. We focus on the prospects for using CMB measurements to differentiate various inflationary mechanisms. In particular, a detection of primordial B-mode polarization would demonstrate that inflation occurred at a very high energy scale, and that the inflaton traversed a super-Planckian distance in field space. We explain how such a detection or constraint would illuminate aspects of physics at the Planck scale. Moreover, CMB measurements can constrain the scale-dependence and non-Gaussianity of the primordial fluctuations and limit the possibility of a significant isocurvature contribution. Each such limit provides crucial information on the underlying inflationary dynamics. Finally, we quantify these considerations by presenting forecasts for the sensitivities of a future satellite experiment to the inflationary parameters.',\n",
       " '  We propose a new method in the spectral analysis of noisy time-series data for damped oscillators. From the Jacobi three terms recursive relation for the denominators of the PadÃ© Approximations built on the well-known Z-transform of an infinite time-series, we build an Hilbert space operator, a J-Operator, where each bound state (inside the unit circle in the complex plane) is simply associated to one damped oscillator while the continuous spectrum of the J-Operator, which lies on the unit circle itself, is shown to represent the noise. Signal and noise are thus clearly separated in the complex plane. For a finite time series of length 2N, the J-operator is replaced by a finite order J-Matrix J_N, having N eigenvalues which are time reversal covariant. Different classes of input noise, such as blank (white and uniform), Gaussian and pink, are discussed in detail, the J-Matrix formalism allowing us to efficiently calculate hundreds of poles of the Z-transform. Evidence of a universal behaviour in the final statistical distribution of the associated poles and zeros of the Z-transform is shown. In particular the poles and zeros tend, when the length of the time series goes to infinity, to a uniform angular distribution on the unit circle. Therefore at finite order, the roots of unity in the complex plane appear to be noise attractors. We show that the Z-transform presents the exceptional feature of allowing lossless undersampling and how to make use of this property. A few basic examples are given to suggest the power of the proposed method.',\n",
       " '  We investigate the problem of backscattering off a time-dependent impurity in a one-dimensional electron gas. By combining the Schwinger-Keldysh method with an adiabatic approximation in order to deal with the corresponding out of equilibrium Dirac equation, we compute the total energy density (TED) of the system. We show how the free fermion TED is distorted by the backscattering amplitude and the geometry of the impurity.',\n",
       " '  We discuss the construction of optimized electronic filters using inverse scattering methods. We study a wide range of densities and temperatures, room temperature included. Discretization methods of the potential (including the self-consistent potential of the conduction electrons) are worked out that retain all its properties.',\n",
       " '  We study the tunneling between two quantum Hall systems, along a quasi one-dimensional interface. A detailed analysis relates microscopic parameters, characterizing the potential barrier, with the effective field theory model for the tunneling. It is shown that the phenomenon of fermion number fractionalization is expected to occur, either localized in conveniently modulated barriers or in the form of free excitations, once lattice effects are taken into account. This opens the experimental possibility of an observation of fractional charges with internal structure, close to the magnetic length scale. The coupling of the system to external gauge fields is performed, leading us to the exact quantization of the Hall conductivity at the interface. The field theory approach is well supported by a numerical diagonalization of the microscopic Hamiltonian.',\n",
       " \"Given one sample $X \\\\in \\\\{\\\\pm 1\\\\}^n$ from an Ising model $\\\\Pr[X=x]\\\\propto \\\\exp(x^\\\\top J x/2)$, whose interaction matrix satisfies $J:= \\\\sum_{i=1}^k Î²_i J_i$ for some known matrices $J_i$ and some unknown parameters $Î²_i$, we study whether $J$ can be estimated to high accuracy. Assuming that each node of the Ising model has bounded total interaction with the other nodes, i.e. $\\\\|J\\\\|_{\\\\infty} \\\\le O(1)$, we provide a computationally efficient estimator $\\\\hat{J}$ with the high probability guarantee $\\\\|\\\\hat{J} -J\\\\|_F \\\\le \\\\widetilde O(\\\\sqrt{k})$, where $\\\\|J\\\\|_F$ can be as high as $Î©(\\\\sqrt{n})$. Our guarantee is tight when the interaction strengths are sufficiently low. An example application of our result is in social networks, wherein nodes make binary choices, $x_1,\\\\ldots,x_n$, which may be influenced at varying strengths $Î²_i$ by different networks $J_i$ in which these nodes belong. By observing a single snapshot of the nodes' behaviors the goal is to learn the combined correlation structure.\\n  When $k=1$ and a single parameter is to be inferred, we further show $|\\\\hatÎ²_1 - Î²_1| \\\\le \\\\widetilde O(F(Î²_1J_1)^{-1/2})$, where $F(Î²_1J_1)$ is the log-partition function of the model. This was proved in prior work under additional assumptions. We generalize these results to any setting.\\n  While our guarantees aim both high and low temperature regimes, our proof relies on sparsifying the correlation network by conditioning on subsets of the variables, such that the unconditioned variables satisfy Dobrushin's condition, i.e. a high temperature condition which allows us to apply stronger concentration inequalities. We use this to prove concentration and anti-concentration properties of the Ising model, and we believe this sparsification result has applications beyond the scope of this paper as well.\",\n",
       " 'Gaussian processes provide a probabilistic framework for quantifying uncertainty of prediction and have been adopted in many applications in Statistics and Bayesian optimization. Unfortunately, they are hard to scale to large datasets as they necessitate inverting matrices whose size is linear in the number of observations. Moreover, they necessitate an a priori chosen functional form for their kernels with predetermined features. Our contribution is a framework that addresses both challenges. We use deep neural networks for automatic feature extraction, combined with explicit functional forms for the eigenspectrum of Gaussian processes with Gaussian kernels, to derive a Gaussian process inference and prediction framework whose complexity scales linearly in the number of observations and which accommodates automatic feature extraction. On a series of datasets, our method outperforms state of the art scalable Gaussian process approximations.',\n",
       " 'Spin glass models, such as the Sherrington-Kirkpatrick, Hopfield and Ising models, are all well-studied members of the exponential family of discrete distributions, and have been influential in a number of application domains where they are used to model correlation phenomena on networks. Conventionally these models have quadratic sufficient statistics and consequently capture correlations arising from pairwise interactions. In this work we study extensions of these to models with higher-order sufficient statistics, modeling behavior on a social network with peer-group effects. In particular, we model binary outcomes on a network as a higher-order spin glass, where the behavior of an individual depends on a linear function of their own vector of covariates and some polynomial function of the behavior of others, capturing peer-group effects. Using a {\\\\em single}, high-dimensional sample from such model our goal is to recover the coefficients of the linear function as well as the strength of the peer-group effects. The heart of our result is a novel approach for showing strong concavity of the log pseudo-likelihood of the model, implying statistical error rate of $\\\\sqrt{d/n}$ for the Maximum Pseudo-Likelihood Estimator (MPLE), where $d$ is the dimensionality of the covariate vectors and $n$ is the size of the network (number of nodes). Our model generalizes vanilla logistic regression as well as the peer-effect models studied in recent works, and our results extend these results to accommodate higher-order interactions.',\n",
       " 'GANs for time series data often use sliding windows or self-attention to capture underlying time dependencies. While these techniques have no clear theoretical justification, they are successful in significantly reducing the discriminator size, speeding up the training process, and improving the generation quality. In this paper, we provide both theoretical foundations and a practical framework of GANs for high-dimensional distributions with conditional independence structure captured by a Bayesian network, such as time series data. We prove that several probability divergences satisfy subadditivity properties with respect to the neighborhoods of the Bayes-net graph, providing an upper bound on the distance between two Bayes-nets by the sum of (local) distances between their marginals on every neighborhood of the graph. This leads to our proposed Subadditive GAN framework that uses a set of simple discriminators on the neighborhoods of the Bayes-net, rather than a giant discriminator on the entire network, providing significant statistical and computational benefits. We show that several probability distances including Jensen-Shannon, Total Variation, and Wasserstein, have subadditivity or generalized subadditivity. Moreover, we prove that Integral Probability Metrics (IPMs), which encompass commonly-used loss functions in GANs, also enjoy a notion of subadditivity under some mild conditions. Furthermore, we prove that nearly all f-divergences satisfy local subadditivity in which subadditivity holds when the distributions are relatively close. Our experiments on synthetic as well as real-world datasets verify the proposed theory and the benefits of subadditive GANs.',\n",
       " 'We identify the first static credible mechanism for multi-item additive auctions that achieves a constant factor of the optimal revenue. This is one instance of a more general framework for designing two-part tariff auctions, adapting the duality framework of Cai et al [CDW16]. Given a (not necessarily incentive compatible) auction format $A$ satisfying certain technical conditions, our framework augments the auction with a personalized entry fee for each bidder, which must be paid before the auction can be accessed. These entry fees depend only on the prior distribution of bidder types, and in particular are independent of realized bids. Our framework can be used with many common auction formats, such as simultaneous first-price, simultaneous second-price, and simultaneous all-pay auctions. If all-pay auctions are used, we prove that the resulting mechanism is credible in the sense that the auctioneer cannot benefit by deviating from the stated mechanism after observing agent bids. If second-price auctions are used, we obtain a truthful $O(1)$-approximate mechanism with fixed entry fees that are amenable to tuning via online learning techniques. Our results for first price and all-pay are the first revenue guarantees of non-truthful mechanisms in multi-dimensional environments; an open question in the literature [RST17].',\n",
       " 'In this paper we study the smooth convex-concave saddle point problem. Specifically, we analyze the last iterate convergence properties of the Extragradient (EG) algorithm. It is well known that the ergodic (averaged) iterates of EG converge at a rate of $O(1/T)$ (Nemirovski, 2004). In this paper, we show that the last iterate of EG converges at a rate of $O(1/\\\\sqrt{T})$. To the best of our knowledge, this is the first paper to provide a convergence rate guarantee for the last iterate of EG for the smooth convex-concave saddle point problem. Moreover, we show that this rate is tight by proving a lower bound of $Î©(1/\\\\sqrt{T})$ for the last iterate. This lower bound therefore shows a quadratic separation of the convergence rates of ergodic and last iterates in smooth convex-concave saddle point problems.',\n",
       " 'We study the sample complexity of learning revenue-optimal multi-item auctions. We obtain the first set of positive results that go beyond the standard but unrealistic setting of item-independence. In particular, we consider settings where bidders\\' valuations are drawn from correlated distributions that can be captured by Markov Random Fields or Bayesian Networks -- two of the most prominent graphical models. We establish parametrized sample complexity bounds for learning an up-to-$\\\\varepsilon$ optimal mechanism in both models, which scale polynomially in the size of the model, i.e. the number of items and bidders, and only exponential in the natural complexity measure of the model, namely either the largest in-degree (for Bayesian Networks) or the size of the largest hyper-edge (for Markov Random Fields).\\n  We obtain our learnability results through a novel and modular framework that involves first proving a robustness theorem. We show that, given only \"approximate distributions\" for bidder valuations, we can learn a mechanism whose revenue is nearly optimal simultaneously for all \"true distributions\" that are close to the ones we were given in Prokhorov distance. Thus, to learn a good mechanism, it suffices to learn approximate distributions. When item values are independent, learning in Prokhorov distance is immediate, hence our framework directly implies the main result of Gonczarowski and Weinberg \\\\cite{GonczarowskiW18}. When item values are sampled from more general graphical models, we combine our robustness theorem with novel sample complexity results for learning Markov Random Fields or Bayesian Networks in Prokhorov distance, which may be of independent interest. Finally, in the single-item case, our robustness result can be strengthened to hold under an even weaker distribution distance, the LÃ©vy distance.',\n",
       " 'Generative adversarial networks (GANs) are a widely used framework for learning generative models. Wasserstein GANs (WGANs), one of the most successful variants of GANs, require solving a minmax optimization problem to global optimality, but are in practice successfully trained using stochastic gradient descent-ascent. In this paper, we show that, when the generator is a one-layer network, stochastic gradient descent-ascent converges to a global solution with polynomial time and sample complexity.',\n",
       " \"Statistical learning theory has largely focused on learning and generalization given independent and identically distributed (i.i.d.) samples. Motivated by applications involving time-series data, there has been a growing literature on learning and generalization in settings where data is sampled from an ergodic process. This work has also developed complexity measures, which appropriately extend the notion of Rademacher complexity to bound the generalization error and learning rates of hypothesis classes in this setting. Rather than time-series data, our work is motivated by settings where data is sampled on a network or a spatial domain, and thus do not fit well within the framework of prior work. We provide learning and generalization bounds for data that are complexly dependent, yet their distribution satisfies the standard Dobrushin's condition. Indeed, we show that the standard complexity measures of Gaussian and Rademacher complexities and VC dimension are sufficient measures of complexity for the purposes of bounding the generalization error and learning rates of hypothesis classes in our setting. Moreover, our generalization bounds only degrade by constant factors compared to their i.i.d. analogs, and our learnability bounds degrade by log factors in the size of the training set.\",\n",
       " ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lib[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer, vocabulary = build_text_vectorizer(lib[0],\n",
    "                             use_tfidf=True,\n",
    "                             use_stemmer=False,\n",
    "                             max_features=5000)\n",
    "X = vectorizer(lib[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5356, 5000)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 : reconstruction error: 281.76855579300764\n",
      "iter 1 : reconstruction error: 100.07106578083967\n",
      "iter 2 : reconstruction error: 74.88950460132432\n",
      "iter 3 : reconstruction error: 72.42671194695552\n",
      "iter 4 : reconstruction error: 71.4710270321863\n",
      "FINAL reconstruction error: 71.13234822441217 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We'd like to see consistent results, so set the seed.\n",
    "np.random.seed(12345)\n",
    "\n",
    "# Find latent topics using our NMF model.\n",
    "factorizer = NMF(k=5, max_iters=5, alpha=0.5)\n",
    "W, H = factorizer.fit(X, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0\n",
      "--> n 1 2 p mass 3 4 10 5 Ïˆ mathrm e c 0 times j pm0 rm gev cross\n",
      "please label this topic: numbers\n",
      "\n",
      "topic 1\n",
      "--> 0 pm Ï€ b k cp decay decays overline rightarrow lhcb fb integrated proton 1 mathcal rm branching luminosity 7\n",
      "please label this topic: lol\n",
      "\n",
      "topic 2\n",
      "--> gravitational wave ligo galaxies data noise dark search matter sources optical frequency survey waves high advanced energy z hz detectors\n",
      "please label this topic: space\n",
      "\n",
      "topic 3\n",
      "--> model method learning methods objects task dune based models optimization neutrino performance propose physics matrix graph system approach design 3d\n",
      "please label this topic: modeling\n",
      "\n",
      "topic 4\n",
      "--> network n algorithms problem algorithm quantum networks show learning graph channel time coding data optimal models bounds model number log\n",
      "please label this topic: networking\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Label topics and analyze a few NYT articles.\n",
    "hand_labels = hand_label_topics(H, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x667.491 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the library with the CountVectorizer method\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "# Helper function\n",
    "def plot_10_most_common_words(count_data, count_vectorizer):\n",
    "    import matplotlib.pyplot as plt\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    total_counts = np.zeros(len(words))\n",
    "    for t in count_data:\n",
    "        total_counts+=t.toarray()[0]\n",
    "    \n",
    "    count_dict = (zip(words, total_counts))\n",
    "    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n",
    "    words = [w[0] for w in count_dict]\n",
    "    counts = [w[1] for w in count_dict]\n",
    "    x_pos = np.arange(len(words)) \n",
    "    \n",
    "    plt.figure(2, figsize=(15, 15/1.6180))\n",
    "    plt.subplot(title='10 most common words')\n",
    "    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "    sns.barplot(x_pos, counts, palette='husl')\n",
    "    plt.xticks(x_pos, words, rotation=90) \n",
    "    plt.xlabel('words')\n",
    "    plt.ylabel('counts')\n",
    "    plt.show()\n",
    "# Initialise the count vectorizer with the English stop words\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "# Fit and transform the processed titles\n",
    "count_data = count_vectorizer.fit_transform(lib[0])\n",
    "# Visualise the 10 most common words\n",
    "plot_10_most_common_words(count_data, count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "pm mass decay data mathrm using decays rm mathcal lhcb integrated measured 10 proton rightarrow\n",
      "\n",
      "Topic #1:\n",
      "problem algorithm time algorithms model graph number problems optimal results network paper linear log bounds\n",
      "\n",
      "Topic #2:\n",
      "galaxies galaxy star mass redshift sample games stellar formation game 10 gas high evolution cluster\n",
      "\n",
      "Topic #3:\n",
      "data quantum model based learning state performance using systems network models high demonstrate new approach\n",
      "\n",
      "Topic #4:\n",
      "data gravitational energy wave high 10 using search ray mass results stars signal frequency survey\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "# Load the LDA model from sk-learn\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    " \n",
    "# Helper function\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "# Tweak the two parameters below\n",
    "number_topics = 5\n",
    "number_words = 15\n",
    "# Create and fit the LDA model\n",
    "lda = LDA(n_components=number_topics, n_jobs=-1)\n",
    "lda.fit(count_data)\n",
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, count_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-4db6fb1a10cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# # if you want to execute visualization prep yourself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mLDAvis_prepared\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn_lda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLDAvis_data_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLDAvis_prepared\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyLDAvis/sklearn.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(lda_model, dtm, vectorizer, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m     94\u001b[0m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics)\u001b[0m\n\u001b[1;32m    396\u001b[0m    \u001b[0mterm_frequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m    \u001b[0mtopic_info\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0m_topic_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_proportion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m    \u001b[0mtoken_table\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0m_token_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m    \u001b[0mtopic_coordinates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_topic_coordinates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_proportion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36m_topic_info\u001b[0;34m(topic_term_dists, topic_proportion, term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m    top_terms = pd.concat(Parallel(n_jobs=n_jobs)(delayed(_find_relevance_chunks)(log_ttd, log_lift, R, ls) \\\n\u001b[0;32m--> 255\u001b[0;31m                                                  for ls in _job_chunks(lambda_seq, n_jobs)))\n\u001b[0m\u001b[1;32m    256\u001b[0m    \u001b[0mtopic_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_top_term_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_terms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdefault_term_info\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_dfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1017\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    906\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    552\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    553\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os \n",
    "from pyLDAvis import sklearn as sklearn_lda\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "LDAvis_data_filepath = os.path.join('./ldavis_prepared_'+str(number_topics))\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = sklearn_lda.prepare(lda, count_data, count_vectorizer)\n",
    "with open(LDAvis_data_filepath, 'w') as f:\n",
    "    pickle.dump(LDAvis_prepared, f)\n",
    "        \n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath) as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, './ldavis_prepared_'+ str(number_topics) +'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
